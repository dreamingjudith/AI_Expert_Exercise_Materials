{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 삼성 DS-KAIST AI Expert 프로그램\n",
    "- 강의: 신진우 교수님\n",
    "- 조교: 김재형 (jaehyungkim@kaist.ac.kr)\n",
    "\n",
    "본 실습은 https://github.com/DetectionTeamUCAS/FPN_Tensorflow 에 기반하여 작성되었습니다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection\n",
    "\n",
    "Image classification은 이미지 하나에 대해서 하나의 label을 예측하는 반면, object detection은 이미지 내의 모든 객체에 대해서 적합한 bounding box를 계산하고 (localization), 동시에 어떤 category에 속하는지 분류하는 (classification) 문제입니다. \n",
    "\n",
    "본 실습에서는 TensorFlow 프레임워크를 이용하여, object detection을 위한 neural network를 구현해보고, 이를 통해 테스트 이미지에 대한 object detection을 진행해볼 예정입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import tensorflow as tf\n",
    "if type(tf.contrib) != type(tf): tf.contrib._warning = None\n",
    "import tensorflow.contrib.slim as slim\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import argparse\n",
    "import time\n",
    "import cv2\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from libs.networks import resnet\n",
    "from libs.networks import mobilenet_v2\n",
    "from libs.box_utils import encode_and_decode\n",
    "from libs.box_utils import boxes_utils\n",
    "from libs.box_utils import anchor_utils\n",
    "from libs.configs import cfgs\n",
    "from libs.losses import losses\n",
    "from libs.box_utils import show_box_in_tensor\n",
    "from libs.box_utils import draw_box_in_img\n",
    "from libs.detection_oprations.proposal_opr import postprocess_rpn_proposals\n",
    "from libs.detection_oprations.anchor_target_layer_without_boxweight import anchor_target_layer\n",
    "from libs.detection_oprations.proposal_target_layer import proposal_target_layer\n",
    "\n",
    "from data.io.read_tfrecord import next_batch\n",
    "from data.io.image_preprocess import short_side_resize_for_inference_data\n",
    "\n",
    "from help_utils import tools\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = cfgs.GPU_GROUP\n",
    "from tensorflow.python.util import deprecation\n",
    "deprecation._PRINT_DEPRECATION_WARNINGS = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Faster R-CNN with FPN\n",
    "\n",
    "본 실습에서 저희는 앞서 강의에서 다루었던 two-stage detector 중 하나인 Faster R-CNN (Ren et al., 2015)를 구현해볼 것입니다. \n",
    "Faster R-CNN에서 임의의 이미지에 대한 object detection은 다음과 같이 진행되며, 저희는 이를 순서대로 직접 구현해볼 것입니다.\n",
    "\n",
    "1. 사전에 학습된 backbone network $f$ 를 이용하여 주어진 이미지 $x$에 대한 feature map $f(x)$를 얻습니다.\n",
    "   (저희는 이미지넷에서 사전학습된 ResNet-50을 사용합니다.)\n",
    "2. 앞서 얻은 이미지 feature map $f(x)$를 `Region Proposal Network (RPN)` 에 통과시킴으로써, 해당 이미지에 내에서 object가 있을 가능성이    높은 후보군인 `Region of Interests (RoIs)` 를 생성합니다.  \n",
    "3. 각각의 `RoI` 에 대한 classification 및 추가적인 bounding box 정교화(localization)를 위해, 주어진 `RoI` 에 해당하는 feature map      $f(x)$ 의 영역을 `RoI pooling` 을 통해 구합니다.\n",
    "4. 구해진 `RoI` feature map을 이용하여 classification 및 bounding box 좌표를 계산합니다. 이는 Fast R-CNN (Ross Girshick, 2015)에      해당하는 부분으로 생각할 수 있습니다.   \n",
    "\n",
    "<img src=\"https://www.researchgate.net/profile/Zhipeng_Deng/publication/324903264/figure/fig2/AS:640145124499471@1529633899620/The-architecture-of-Faster-R-CNN.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction with Backbone Network  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_base_network(input_img_batch, base_network_name, is_training=False):\n",
    "    if base_network_name.startswith('resnet_v1'):\n",
    "        return resnet.resnet_base(input_img_batch, scope_name=base_network_name, is_training=is_training)\n",
    "    else:\n",
    "        raise ValueError('Sry, we only support resnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Task 1] Region Proposal Network (RPN) \n",
    "\n",
    "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=http%3A%2F%2Fcfile5.uf.tistory.com%2Fimage%2F992805365B46F812147056\">\n",
    "\n",
    "아래에 주어진 함수를 적절히 활용하여 RPN을 구현해보세요.\n",
    "(단, 그림에서 256-d를 사용한 것과 달리 저희는 512-d를 사용하겠습니다.)\n",
    "- `slim.conv2d(x, p, [k,k], ...)`: For input x, output a feature with p channels using convolution filter of size k\n",
    "- `slim.softmax(x, ...)`: Get a probability from x using a softmax\n",
    "- `tf.reshape(x, [-1, b])`: Reshape a tensor x to be a tensor with shape [-1, b] => using it, make ith row mean ith RoI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RPN(feature_maps, num_anchors_per_location, is_training=False, reuse_flag=False):\n",
    "    '''\n",
    "    feature_maps: feature map of given image\n",
    "    '''\n",
    "    rpn_conv3x3 = slim.conv2d(\n",
    "                    ?, ?, [?, ?],\n",
    "                    trainable=is_training, weights_initializer=cfgs.INITIALIZER, padding=\"SAME\",\n",
    "                    activation_fn=tf.nn.relu,\n",
    "                    scope='rpn_conv/3x3',\n",
    "                    reuse=reuse_flag)\n",
    "    rpn_cls_score = slim.conv2d(?, ?, [?, ?], stride=?,\n",
    "                                            trainable=is_training, weights_initializer=cfgs.INITIALIZER,\n",
    "                                            activation_fn=None, padding=\"VALID\",\n",
    "                                            scope='rpn_cls_score',\n",
    "                                            reuse=reuse_flag)\n",
    "    rpn_box_pred = slim.conv2d(?, ?, [?, ?], stride=?,\n",
    "                                           trainable=is_training, weights_initializer=cfgs.BBOX_INITIALIZER,\n",
    "                                           activation_fn=None, padding=\"VALID\",\n",
    "                                           scope='rpn_bbox_pred',\n",
    "                                           reuse=reuse_flag)\n",
    "    rpn_box_pred = tf.reshape(?, [-1, ?])\n",
    "    rpn_cls_score = tf.reshape(?, [-1, ?])\n",
    "    rpn_cls_prob = slim.softmax(?, scope='rpn_cls_prob')\n",
    "    \n",
    "    return rpn_cls_prob, rpn_box_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Task 2] RoI Pooling  \n",
    "\n",
    "아래에 주어진 함수를 적절히 활용하여 다음 함수의 빈칸을 채우고, RoI Pooling 구현을 완성해주세요.\n",
    "- [`tf.image.crop_and_resize`](https://www.tensorflow.org/api_docs/python/tf/image/crop_and_resize): Using this function, resize each RoIs to be 14x14\n",
    "- `slim.max_pool2d(x, [k, k], stride=s)`: Max-pooling of feature x with kernel size k and stride s => Make 7x7 final features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roi_pooling(feature_maps, rois, img_shape, scope):\n",
    "    '''\n",
    "    feature_maps: feature map to be cropped\n",
    "    rois: shape is [-1, 4]. [x1, y1, x2, y2]\n",
    "    img_shape: 3 x h x w\n",
    "    '''\n",
    "    with tf.compat.v1.variable_scope('ROI_Warping_' + scope):\n",
    "        img_h, img_w = tf.cast(img_shape[1], tf.float32), tf.cast(img_shape[2], tf.float32)\n",
    "        N = tf.shape(rois)[0]\n",
    "        x1, y1, x2, y2 = tf.unstack(rois, axis=1) \n",
    "        \n",
    "        normalized_x1 = x1 / img_w\n",
    "        normalized_x2 = x2 / img_w\n",
    "        normalized_y1 = y1 / img_h\n",
    "        normalized_y2 = y2 / img_h\n",
    "\n",
    "        # normalized_rois: (0,1) scaled positions of rois \n",
    "        normalized_rois = tf.transpose(tf.stack([normalized_y1, normalized_x1, normalized_y2, normalized_x2]), name='get_normalized_rois')\n",
    "        normalized_rois = tf.stop_gradient(normalized_rois)\n",
    "\n",
    "        cropped_roi_features = tf.image.crop_and_resize(?, ?, box_ind=tf.zeros(shape=[N, ],\n",
    "                                                            dtype=tf.int32), crop_size=[?, ?], name='CROP_AND_RESIZE')\n",
    "        roi_features = slim.max_pool2d(?, [?, ?], stride=?)\n",
    "    return roi_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Task 3] Fast R-CNN \n",
    "\n",
    "아래에 주어진 함수를 적절히 활용하여 다음 함수의 빈칸을 채우고, Fast R-CNN을 구현해보세요.\n",
    "- `roi_pooling`: RoI pooling layer from [Task 2]\n",
    "- `resnet.resnet_head(inputs=x, ...)`: fully connected network for flattening a given input x (feature map)\n",
    "- `slim.fully_connected(x, num_outputs=k, ...)`: fully connected network (input:x, output:k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_fastrcnn(feature_map, rois, img_shape, level_name, base_network_name, is_training=False):\n",
    "    with tf.compat.v1.variable_scope('Fast-RCNN'):\n",
    "        # ROI Pooling\n",
    "        with tf.compat.v1.variable_scope('rois_pooling'):\n",
    "            pooled_features = roi_pooling(feature_maps=?, rois=?, img_shape=?, scope=level_name)\n",
    "                    \n",
    "        # inferecne rois in Fast-RCNN to obtain fc_flatten features\n",
    "        if base_network_name.startswith('resnet'):\n",
    "            fc_flatten = resnet.restnet_head(inputs=?, is_training=is_training, scope_name=base_network_name)\n",
    "        else:\n",
    "            raise NotImplementedError('only support resnet')\n",
    "\n",
    "        # cls and reg in Fast-RCNN\n",
    "        with slim.arg_scope([slim.fully_connected], weights_regularizer=slim.l2_regularizer(cfgs.WEIGHT_DECAY)):\n",
    "            cls_score = slim.fully_connected(?, num_outputs=?[1], weights_initializer=cfgs.INITIALIZER,\n",
    "                                             activation_fn=None, trainable=is_training, scope='cls_fc')\n",
    "            bbox_pred = slim.fully_connected(?, num_outputs=?[2], weights_initializer=cfgs.BBOX_INITIALIZER,\n",
    "                                             activation_fn=None, trainable=is_training, scope='reg_fc')\n",
    "                \n",
    "            cls_score = tf.reshape(cls_score, [-1, ?[1])\n",
    "            bbox_pred = tf.reshape(bbox_pred, [-1, ?[2])\n",
    "            cls_prob = slim.softmax(cls_score, 'cls_prob')\n",
    "\n",
    "            return bbox_pred, cls_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Faster R-CNN\n",
    "\n",
    "앞서 구현한 함수들을 이용하여, 아래와 같이 Faster R-CNN을 구현할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Faster_RCNN(object):\n",
    "    def __init__(self, base_network_name, layer_idx=4, is_training=False):\n",
    "        self.base_network_name = base_network_name\n",
    "        self.layer_idx = layer_idx\n",
    "        self.is_training = is_training\n",
    "        self.num_anchors_per_location = len(cfgs.ANCHOR_SCALES) * len(cfgs.ANCHOR_RATIOS)\n",
    "\n",
    "    def build_whole_detection_network(self, input_img_batch, gtboxes_batch):\n",
    "        img_shape = tf.shape(input_img_batch)\n",
    "\n",
    "        # 1. build base network\n",
    "        P_list = build_base_network(input_img_batch, self.base_network_name)  # Multi-scale features:[P2, P3, P4, P5, P6]\n",
    "\n",
    "        # 2. build rpn\n",
    "        with tf.compat.v1.variable_scope('build_rpn', regularizer=slim.l2_regularizer(cfgs.WEIGHT_DECAY)):\n",
    "            level_name, p = cfgs.LEVLES[self.layer_idx], P_list[self.layer_idx]    \n",
    "            rpn_cls_prob, rpn_box_pred = RPN(p, self.num_anchors_per_location)\n",
    "            \n",
    "        # 3. generate_anchors\n",
    "        p_h, p_w = tf.shape(p)[1], tf.shape(p)[2]\n",
    "        featuremap_height = tf.cast(p_h, tf.float32)\n",
    "        featuremap_width = tf.cast(p_w, tf.float32)\n",
    "        anchors = anchor_utils.make_anchors(base_anchor_size=cfgs.BASE_ANCHOR_SIZE_LIST[self.layer_idx],\n",
    "                                            anchor_scales=cfgs.ANCHOR_SCALES,\n",
    "                                            anchor_ratios=cfgs.ANCHOR_RATIOS,\n",
    "                                            featuremap_height=featuremap_height,\n",
    "                                            featuremap_width=featuremap_width,\n",
    "                                            stride=cfgs.ANCHOR_STRIDE_LIST[self.layer_idx],\n",
    "                                            name=\"make_anchors_for%s\" % level_name)\n",
    "        \n",
    "        # 4. postprocess rpn proposals. such as: decode, clip, NMS\n",
    "        with tf.compat.v1.variable_scope('postprocess_RPN'):\n",
    "            rois, roi_scores = postprocess_rpn_proposals(rpn_bbox_pred=rpn_box_pred,\n",
    "                                                         rpn_cls_prob=rpn_cls_prob,\n",
    "                                                         img_shape=img_shape,\n",
    "                                                         anchors=anchors,\n",
    "                                                         is_training=self.is_training)\n",
    "        \n",
    "        # 5. build Fast-RCNN\n",
    "        bbox_pred, cls_prob = build_fastrcnn(feature_map=p, rois=rois, img_shape=img_shape, \n",
    "                                             level_name=level_name, base_network_name=self.base_network_name)\n",
    "        \n",
    "        #  6. postprocess_fastrcnn\n",
    "        final_boxes, final_scores, final_category = self.postprocess_fastrcnn(rois=rois, \n",
    "                                                                              bbox_ppred=bbox_pred, \n",
    "                                                                              scores=cls_prob, \n",
    "                                                                              img_shape=img_shape)\n",
    "        \n",
    "        return final_boxes, final_scores, final_category\n",
    "    \n",
    "    ######################################## Utility functions (do not disturb!) ########################################\n",
    "    \n",
    "    ## Goal: post-processing after inference  \n",
    "    def postprocess_fastrcnn(self, rois, bbox_ppred, scores, img_shape):\n",
    "        '''\n",
    "        rois:[-1, 4], bbox_ppred: [-1, (cfgs.Class_num+1) * 4], scores: [-1, cfgs.Class_num + 1]\n",
    "        '''\n",
    "        with tf.name_scope('postprocess_fastrcnn'):\n",
    "            rois = tf.stop_gradient(rois)\n",
    "            scores = tf.stop_gradient(scores)\n",
    "            bbox_ppred = tf.reshape(bbox_ppred, [-1, cfgs.CLASS_NUM + 1, 4])\n",
    "            bbox_ppred = tf.stop_gradient(bbox_ppred)\n",
    "\n",
    "            bbox_pred_list = tf.unstack(bbox_ppred, axis=1)\n",
    "            score_list = tf.unstack(scores, axis=1)\n",
    "\n",
    "            allclasses_boxes = []\n",
    "            allclasses_scores = []\n",
    "            categories = []\n",
    "            \n",
    "            for i in range(1, cfgs.CLASS_NUM+1):\n",
    "\n",
    "                # 1. decode boxes in each class\n",
    "                tmp_encoded_box = bbox_pred_list[i]\n",
    "                tmp_score = score_list[i]\n",
    "                tmp_decoded_boxes = encode_and_decode.decode_boxes(encoded_boxes=tmp_encoded_box,\n",
    "                                                                   reference_boxes=rois,\n",
    "                                                                   scale_factors=cfgs.ROI_SCALE_FACTORS)\n",
    "                \n",
    "                # 2. clip to img boundaries\n",
    "                tmp_decoded_boxes = boxes_utils.clip_boxes_to_img_boundaries(decode_boxes=tmp_decoded_boxes,\n",
    "                                                                             img_shape=img_shape)\n",
    "\n",
    "                # 3. NMS\n",
    "                keep = tf.image.non_max_suppression(\n",
    "                    boxes=tmp_decoded_boxes,\n",
    "                    scores=tmp_score,\n",
    "                    max_output_size=cfgs.FAST_RCNN_NMS_MAX_BOXES_PER_CLASS,\n",
    "                    iou_threshold=cfgs.FAST_RCNN_NMS_IOU_THRESHOLD)\n",
    "\n",
    "                perclass_boxes = tf.gather(tmp_decoded_boxes, keep)\n",
    "                perclass_scores = tf.gather(tmp_score, keep)\n",
    "\n",
    "                allclasses_boxes.append(perclass_boxes)\n",
    "                allclasses_scores.append(perclass_scores)\n",
    "                categories.append(tf.ones_like(perclass_scores) * i)\n",
    "\n",
    "            final_boxes = tf.concat(allclasses_boxes, axis=0)\n",
    "            final_scores = tf.concat(allclasses_scores, axis=0)\n",
    "            final_category = tf.concat(categories, axis=0)\n",
    "\n",
    "            if self.is_training:\n",
    "                '''\n",
    "                For a visualization in the tensorboard\n",
    "                '''\n",
    "                kept_indices = tf.reshape(tf.where(tf.greater_equal(final_scores, cfgs.SHOW_SCORE_THRSHOLD)), [-1])\n",
    "\n",
    "                final_boxes = tf.gather(final_boxes, kept_indices)\n",
    "                final_scores = tf.gather(final_scores, kept_indices)\n",
    "                final_category = tf.gather(final_category, kept_indices)\n",
    "\n",
    "        return final_boxes, final_scores, final_category\n",
    "\n",
    "    # Goal: load pre-trained parameters  \n",
    "    def get_restorer(self):\n",
    "        checkpoint_path = tf.train.latest_checkpoint(os.path.join(cfgs.TRAINED_CKPT, cfgs.VERSION))\n",
    "\n",
    "        if checkpoint_path != None:\n",
    "            restorer = tf.compat.v1.train.Saver()\n",
    "            print(\"model restore from :\", checkpoint_path)\n",
    "        else:\n",
    "            ValueError('Sry, we cannot restore the model from a given path')\n",
    "        return restorer, checkpoint_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo\n",
    "\n",
    "이제, 학습된 모델을 이용하여 실제 테스트 이미지에 대한 객체 검출을 진행해볼 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect(det_net, inference_save_path, real_test_imgname_list):\n",
    "    # Preprocess img\n",
    "    img_plac = tf.placeholder(dtype=tf.uint8, shape=[None, None, 3])\n",
    "    img_batch = tf.cast(img_plac, tf.float32)\n",
    "    img_batch = short_side_resize_for_inference_data(img_tensor=img_batch,\n",
    "                                                     target_shortside_len=cfgs.IMG_SHORT_SIDE_LEN,\n",
    "                                                     length_limitation=cfgs.IMG_MAX_LENGTH)\n",
    "    img_batch = img_batch - tf.constant(cfgs.PIXEL_MEAN)\n",
    "    img_batch = tf.expand_dims(img_batch, axis=0)\n",
    "\n",
    "    # Object detection\n",
    "    detection_boxes, detection_scores, detection_category = det_net.build_whole_detection_network(\n",
    "        input_img_batch=img_batch, gtboxes_batch=None)\n",
    "\n",
    "    init_op = tf.group(\n",
    "        tf.global_variables_initializer(),\n",
    "        tf.local_variables_initializer()\n",
    "    )\n",
    "\n",
    "    # Load pre-trained parameters\n",
    "    restorer, restore_ckpt = det_net.get_restorer()\n",
    "\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "\n",
    "    with tf.Session(config=config) as sess:\n",
    "        sess.run(init_op)\n",
    "        if not restorer is None:\n",
    "            restorer.restore(sess, restore_ckpt)\n",
    "            print('restore model')\n",
    "\n",
    "        for i, a_img_name in enumerate(real_test_imgname_list):\n",
    "            raw_img = cv2.imread(a_img_name)\n",
    "            start = time.time()\n",
    "            resized_img, detected_boxes, detected_scores, detected_categories = \\\n",
    "                sess.run(\n",
    "                    [img_batch, detection_boxes, detection_scores, detection_category],\n",
    "                    feed_dict={img_plac: raw_img[:, :, ::-1]}\n",
    "                )\n",
    "            end = time.time()\n",
    "            \n",
    "            show_indices = detected_scores >= cfgs.SHOW_SCORE_THRSHOLD\n",
    "            show_scores = detected_scores[show_indices]\n",
    "            show_boxes = detected_boxes[show_indices]\n",
    "            show_categories = detected_categories[show_indices]\n",
    "            final_detections = draw_box_in_img.draw_boxes_with_label_and_scores(np.squeeze(resized_img, 0),\n",
    "                                                                                boxes=show_boxes,\n",
    "                                                                                labels=show_categories,\n",
    "                                                                                scores=show_scores)\n",
    "            nake_name = a_img_name.split('/')[-1]\n",
    "            cv2.imwrite(inference_save_path + '/' + nake_name, final_detections[:, :, ::-1])\n",
    "            tools.view_bar('{} image cost {}s'.format(a_img_name, (end - start)), i + 1, len(real_test_imgname_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(test_dir, inference_save_path):\n",
    "    test_imgname_list = [os.path.join(test_dir, img_name) for img_name in os.listdir(test_dir)\n",
    "                                                          if img_name.endswith(('.jpg', '.png', '.jpeg', '.tif', '.tiff'))]\n",
    "    assert len(test_imgname_list) != 0, 'test_dir has no imgs there.' \\\n",
    "                                        ' Note that, we only support img format of (.jpg, .png, and .tiff) '\n",
    "\n",
    "    faster_rcnn = Faster_RCNN(base_network_name=cfgs.NET_NAME)\n",
    "\n",
    "    detect(det_net=faster_rcnn, inference_save_path=inference_save_path, real_test_imgname_list=test_imgname_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "data_dir = './demos'\n",
    "save_dir = './inference_results'\n",
    "inference(data_dir, save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection 결과 확인\n",
    "각 테스트 이미지에 대해 생성된 bounding box와 이에 대한 classification 결과를 직접 확인해보세요.\n",
    "- 이미지: '01~13.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "Image.open('./inference_results/07.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Task 4] Object Detection for Small Objective\n",
    "\n",
    "앞서 구현된 Faster R-CNN을 이용한 object detection의 경우 크기가 큰 물체들은 잘 검출되는 반면, 상대적으로 크기가 작은 물체들의 검출률이 상대적으로 떨어진다는 점을 위의 이미지들을 통해서 직접 확인하실 수 있으셨을 겁니다. 이는, 저희가 앞서 사용한 RPN이 현재 크기가 작은 물체들에 대해 RoIs 생성을 제대로 하지 못하기 때문입니다. 이를 해결하기 위해, 위의 `Faster_RCNN` 코드를 다시 한번 확인하시고, 간단한 수정을 통해 저희의 Faster R-CNN이 크기가 작은 물체들을 검출해낼 수 있도록 만들어주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "data_dir = './demos'\n",
    "save_dir = './inference_results_small'\n",
    "inference(data_dir, save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.open('./inference_results_small/07.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Task 5] Feature Pyramid Network (FPN)\n",
    "\n",
    "low level feature에서 RoIs를 추출함으로써 상대적으로 작은 물체까지 검출할 수 있음을 확인해보았습니다. 하지만, 이와 동시에, 큰 물체에 대한 검출률은 감소하였음을 확인하실 수 있으셨을텐데요, 이를 동시에 개선할 수 있는 방법이 앞서 강의 시간에서도 다루었던 FPN (Lin et al., 2016) 입니다. FPN은 multi-scale feature에서 동시에 RoIs를 생성하고, fast R-CNN을 통한 inference시에도 이를 활용함으로써 다양한 스케일의 물체 검출을 가능케 했는데요, 아래의 주어진 함수를 적절히 활용하여 FPN의 구현을 마무리해주세요. \n",
    "\n",
    "(1) self.build_fastrcnn 내의 pooled_features 완성하기\n",
    "\n",
    "(2) build_whole_detection_network 내의 RPN 부분 완성하기\n",
    "\n",
    "- `tf.concat(list, axis=d)` : list내의 tensor들을 모아 d axis에 대한 concatenation 실행 \n",
    "- `list.append(x)` : element x를 list에 삽입\n",
    "\n",
    "<img src=\"https://www.researchgate.net/publication/338363188/figure/fig1/AS:843281365737472@1578065353313/Framework-of-our-object-detection-in-remote-sensing-images-ODRSIs-method-a-feature.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FPN(object):\n",
    "    def __init__(self, base_network_name, is_training=False):\n",
    "        self.base_network_name = base_network_name\n",
    "        self.is_training = is_training\n",
    "        self.num_anchors_per_location = len(cfgs.ANCHOR_SCALES) * len(cfgs.ANCHOR_RATIOS)\n",
    "\n",
    "    def build_fastrcnn(self, P_list, rois_list, img_shape):\n",
    "        with tf.compat.v1.variable_scope('Fast-RCNN'):\n",
    "            # ROI Pooling\n",
    "            with tf.compat.v1.variable_scope('rois_pooling'):\n",
    "                pooled_features_list = []\n",
    "                for level_name, p, rois in zip(cfgs.LEVLES, P_list, rois_list):  \n",
    "                    pooled_feature = ?\n",
    "                    ?.append(?)\n",
    "\n",
    "                pooled_features = ? # [minibatch_size, H, W, C]\n",
    "\n",
    "            # inferecne rois in Fast-RCNN to obtain fc_flatten features\n",
    "            if self.base_network_name.startswith('resnet'):\n",
    "                fc_flatten = resnet.restnet_head(inputs=pooled_features, is_training=self.is_training, scope_name=self.base_network_name)\n",
    "            else:\n",
    "                raise NotImplementedError('only support resnet')\n",
    "\n",
    "            # cls and reg in Fast-RCNN\n",
    "            with slim.arg_scope([slim.fully_connected], weights_regularizer=slim.l2_regularizer(cfgs.WEIGHT_DECAY)):\n",
    "                cls_score = slim.fully_connected(fc_flatten, num_outputs=cfgs.CLASS_NUM+1, weights_initializer=cfgs.INITIALIZER,\n",
    "                                                 activation_fn=None, trainable=self.is_training, scope='cls_fc')\n",
    "                bbox_pred = slim.fully_connected(fc_flatten, num_outputs=(cfgs.CLASS_NUM+1)*4, weights_initializer=cfgs.BBOX_INITIALIZER,\n",
    "                                                 activation_fn=None, trainable=self.is_training, scope='reg_fc')\n",
    "                \n",
    "                cls_score = tf.reshape(cls_score, [-1, cfgs.CLASS_NUM+1])\n",
    "                bbox_pred = tf.reshape(bbox_pred, [-1, 4*(cfgs.CLASS_NUM+1)])\n",
    "                cls_prob = slim.softmax(cls_score, 'cls_prob')\n",
    "\n",
    "                return bbox_pred, cls_prob\n",
    "\n",
    "    def build_whole_detection_network(self, input_img_batch, gtboxes_batch):\n",
    "        img_shape = tf.shape(input_img_batch)\n",
    "\n",
    "        # 1. build base network\n",
    "        P_list = build_base_network(input_img_batch, self.base_network_name)\n",
    "\n",
    "        # 2. build rpn\n",
    "        with tf.compat.v1.variable_scope('build_rpn', regularizer=slim.l2_regularizer(cfgs.WEIGHT_DECAY)):\n",
    "            fpn_cls_prob =[]\n",
    "            fpn_box_pred = []\n",
    "            for level_name, p in zip(cfgs.LEVLES, P_list):\n",
    "                reuse_flag = None if level_name==cfgs.LEVLES[0] else True\n",
    "                \n",
    "                rpn_cls_prob, rpn_box_pred = RPN(p, self.num_anchors_per_location, self.is_training, reuse_flag)\n",
    "\n",
    "                ?.append(?)\n",
    "                ?.append(?)\n",
    "\n",
    "            fpn_cls_prob = tf.concat(?, axis=?, name='fpn_cls_prob')\n",
    "            fpn_box_pred = tf.concat(?, axis=?, name='fpn_box_pred')\n",
    "\n",
    "        # 3. generate_anchors\n",
    "        all_anchors = []\n",
    "        for i in range(len(cfgs.LEVLES)):\n",
    "            level_name, p = cfgs.LEVLES[i], P_list[i]\n",
    "\n",
    "            p_h, p_w = tf.shape(p)[1], tf.shape(p)[2]\n",
    "            featuremap_height = tf.cast(p_h, tf.float32)\n",
    "            featuremap_width = tf.cast(p_w, tf.float32)\n",
    "            anchors = anchor_utils.make_anchors(base_anchor_size=cfgs.BASE_ANCHOR_SIZE_LIST[i],\n",
    "                                                anchor_scales=cfgs.ANCHOR_SCALES,\n",
    "                                                anchor_ratios=cfgs.ANCHOR_RATIOS,\n",
    "                                                featuremap_height=featuremap_height,\n",
    "                                                featuremap_width=featuremap_width,\n",
    "                                                stride=cfgs.ANCHOR_STRIDE_LIST[i],\n",
    "                                                name=\"make_anchors_for%s\" % level_name)\n",
    "            all_anchors.append(anchors)\n",
    "        all_anchors = tf.concat(all_anchors, axis=0, name='all_anchors_of_FPN')\n",
    "\n",
    "        # 4. postprocess rpn proposals. such as: decode, clip, NMS\n",
    "        with tf.compat.v1.variable_scope('postprocess_FPN'):\n",
    "            rois, roi_scores = postprocess_rpn_proposals(rpn_bbox_pred=fpn_box_pred,\n",
    "                                                         rpn_cls_prob=fpn_cls_prob,\n",
    "                                                         img_shape=img_shape,\n",
    "                                                         anchors=all_anchors,\n",
    "                                                         is_training=self.is_training)\n",
    "        \n",
    "        rois_list = self.assign_levels(all_rois=rois)  # rois_list: [P2_rois, P3_rois, P4_rois, P5_rois]\n",
    "\n",
    "        # 5. build Fast-RCNN\n",
    "        bbox_pred, cls_prob = self.build_fastrcnn(P_list=P_list, rois_list=rois_list, img_shape=img_shape)\n",
    "        rois = tf.concat(rois_list, axis=0, name='concat_rois')\n",
    "        \n",
    "        #  6. postprocess_fastrcnn\n",
    "        final_boxes, final_scores, final_category = self.postprocess_fastrcnn(rois=rois, \n",
    "                                                                              bbox_ppred=bbox_pred, \n",
    "                                                                              scores=cls_prob, \n",
    "                                                                              img_shape=img_shape)\n",
    "        \n",
    "        return final_boxes, final_scores, final_category\n",
    "    \n",
    "    \n",
    "    ######################################## Utility functions (do not disturb) ########################################\n",
    "    \n",
    "    ## Goal: post-processing after inference  \n",
    "    def postprocess_fastrcnn(self, rois, bbox_ppred, scores, img_shape):\n",
    "        '''\n",
    "        rois:[-1, 4], bbox_ppred: [-1, (cfgs.Class_num+1) * 4], scores: [-1, cfgs.Class_num + 1]\n",
    "        '''\n",
    "        with tf.name_scope('postprocess_fastrcnn'):\n",
    "            rois = tf.stop_gradient(rois)\n",
    "            scores = tf.stop_gradient(scores)\n",
    "            bbox_ppred = tf.reshape(bbox_ppred, [-1, cfgs.CLASS_NUM + 1, 4])\n",
    "            bbox_ppred = tf.stop_gradient(bbox_ppred)\n",
    "\n",
    "            bbox_pred_list = tf.unstack(bbox_ppred, axis=1)\n",
    "            score_list = tf.unstack(scores, axis=1)\n",
    "\n",
    "            allclasses_boxes = []\n",
    "            allclasses_scores = []\n",
    "            categories = []\n",
    "            \n",
    "            for i in range(1, cfgs.CLASS_NUM+1):\n",
    "\n",
    "                # 1. decode boxes in each class\n",
    "                tmp_encoded_box = bbox_pred_list[i]\n",
    "                tmp_score = score_list[i]\n",
    "                tmp_decoded_boxes = encode_and_decode.decode_boxes(encoded_boxes=tmp_encoded_box,\n",
    "                                                                   reference_boxes=rois,\n",
    "                                                                   scale_factors=cfgs.ROI_SCALE_FACTORS)\n",
    "                \n",
    "                # 2. clip to img boundaries\n",
    "                tmp_decoded_boxes = boxes_utils.clip_boxes_to_img_boundaries(decode_boxes=tmp_decoded_boxes,\n",
    "                                                                             img_shape=img_shape)\n",
    "\n",
    "                # 3. NMS\n",
    "                keep = tf.image.non_max_suppression(\n",
    "                    boxes=tmp_decoded_boxes,\n",
    "                    scores=tmp_score,\n",
    "                    max_output_size=cfgs.FAST_RCNN_NMS_MAX_BOXES_PER_CLASS,\n",
    "                    iou_threshold=cfgs.FAST_RCNN_NMS_IOU_THRESHOLD)\n",
    "\n",
    "                perclass_boxes = tf.gather(tmp_decoded_boxes, keep)\n",
    "                perclass_scores = tf.gather(tmp_score, keep)\n",
    "\n",
    "                allclasses_boxes.append(perclass_boxes)\n",
    "                allclasses_scores.append(perclass_scores)\n",
    "                categories.append(tf.ones_like(perclass_scores) * i)\n",
    "\n",
    "            final_boxes = tf.concat(allclasses_boxes, axis=0)\n",
    "            final_scores = tf.concat(allclasses_scores, axis=0)\n",
    "            final_category = tf.concat(categories, axis=0)\n",
    "\n",
    "            if self.is_training:\n",
    "                '''\n",
    "                For a visualization in the tensorboard\n",
    "                '''\n",
    "                kept_indices = tf.reshape(tf.where(tf.greater_equal(final_scores, cfgs.SHOW_SCORE_THRSHOLD)), [-1])\n",
    "\n",
    "                final_boxes = tf.gather(final_boxes, kept_indices)\n",
    "                final_scores = tf.gather(final_scores, kept_indices)\n",
    "                final_category = tf.gather(final_category, kept_indices)\n",
    "\n",
    "        return final_boxes, final_scores, final_category\n",
    "    \n",
    "    ## Goal: assign a proper level for each RoI\n",
    "    def assign_levels(self, all_rois, labels=None, bbox_targets=None):\n",
    "        \n",
    "        with tf.name_scope('assign_levels'):\n",
    "            xmin, ymin, xmax, ymax = tf.unstack(all_rois, axis=1)\n",
    "\n",
    "            h = tf.maximum(0., ymax - ymin)\n",
    "            w = tf.maximum(0., xmax - xmin)\n",
    "\n",
    "            levels = tf.floor(4. + tf.math.log(tf.sqrt(w * h + 1e-8) / 224.0) / tf.math.log(2.))  # 4 + log_2(***)\n",
    "            # use floor instead of round\n",
    "\n",
    "            min_level = int(cfgs.LEVLES[0][-1])\n",
    "            max_level = min(5, int(cfgs.LEVLES[-1][-1]))\n",
    "            levels = tf.maximum(levels, tf.ones_like(levels) * min_level)  # level minimum is 2\n",
    "            levels = tf.minimum(levels, tf.ones_like(levels) * max_level)  # level maximum is 5\n",
    "            levels = tf.stop_gradient(tf.reshape(levels, [-1]))\n",
    "\n",
    "            def get_rois(levels, level_i, rois, labels, bbox_targets):\n",
    "\n",
    "                level_i_indices = tf.reshape(tf.where(tf.equal(levels, level_i)), [-1])\n",
    "                tf.compat.v1.summary.scalar('LEVEL/LEVEL_%d_rois_NUM'%level_i, tf.shape(level_i_indices)[0])\n",
    "                level_i_rois = tf.gather(rois, level_i_indices)\n",
    "\n",
    "                if self.is_training:\n",
    "                    level_i_rois = tf.stop_gradient(level_i_rois)\n",
    "                    level_i_labels = tf.gather(labels, level_i_indices)\n",
    "                    level_i_targets = tf.gather(bbox_targets, level_i_indices)\n",
    "                    \n",
    "                    return level_i_rois, level_i_labels, level_i_targets\n",
    "                else:\n",
    "                    return level_i_rois, None, None\n",
    "\n",
    "            rois_list = []\n",
    "            labels_list = []\n",
    "            targets_list = []\n",
    "            \n",
    "            for i in range(min_level, max_level+1):\n",
    "                P_i_rois, P_i_labels, P_i_targets = get_rois(levels, level_i=i, rois=all_rois,\n",
    "                                                             labels=labels,\n",
    "                                                             bbox_targets=bbox_targets)\n",
    "                rois_list.append(P_i_rois)\n",
    "                labels_list.append(P_i_labels)\n",
    "                targets_list.append(P_i_targets)\n",
    "\n",
    "            return rois_list  # [P2_rois, P3_rois, P4_rois, P5_rois] Note: P6 do not assign rois\n",
    "\n",
    "    ## Goal: load pre-trained parameters  \n",
    "    def get_restorer(self):\n",
    "        checkpoint_path = tf.train.latest_checkpoint(os.path.join(cfgs.TRAINED_CKPT, cfgs.VERSION))\n",
    "\n",
    "        if checkpoint_path != None:\n",
    "            restorer = tf.compat.v1.train.Saver()\n",
    "            print(\"model restore from :\", checkpoint_path)\n",
    "        else:\n",
    "            ValueError('Sry, we cannot restore the model from a given path')\n",
    "        return restorer, checkpoint_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_fpn(test_dir, inference_save_path):\n",
    "    test_imgname_list = [os.path.join(test_dir, img_name) for img_name in os.listdir(test_dir)\n",
    "                                                          if img_name.endswith(('.jpg', '.png', '.jpeg', '.tif', '.tiff'))]\n",
    "    assert len(test_imgname_list) != 0, 'test_dir has no imgs there.' \\\n",
    "                                        ' Note that, we only support img format of (.jpg, .png, and .tiff) '\n",
    "\n",
    "    fpn = FPN(base_network_name=cfgs.NET_NAME)\n",
    "    detect(det_net=fpn, inference_save_path=inference_save_path, real_test_imgname_list=test_imgname_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "data_dir = './demos'\n",
    "save_dir = './inference_results_all'\n",
    "inference_fpn(data_dir, save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.open('./inference_results_all/07.jpg')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5.6 64-bit ('0730_detection': conda)",
   "language": "python",
   "name": "python_defaultSpec_1596090532527"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}