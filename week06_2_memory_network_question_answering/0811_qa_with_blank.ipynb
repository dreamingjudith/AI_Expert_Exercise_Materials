{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"BERT model for Question Answering (span extraction).\n",
    "    This module is composed of the BERT model with a linear layer on top of\n",
    "    the sequence output that computes start_logits and end_logits\n",
    "\n",
    "    Params:\n",
    "        `config`: a BertConfig class instance with the configuration to build a new model.\n",
    "\n",
    "    Inputs:\n",
    "        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n",
    "            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n",
    "            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n",
    "        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n",
    "            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n",
    "            a `sentence B` token (see BERT paper for more details).\n",
    "        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n",
    "            selected in [0, 1]. It's a mask to be used if the input sequence length is smaller than the max\n",
    "            input sequence length in the current batch. It's the mask that we typically use for attention when\n",
    "            a batch has varying length sentences.\n",
    "        `start_positions`: position of the first token for the labeled span: torch.LongTensor of shape [batch_size].\n",
    "            Positions are clamped to the length of the sequence and position outside of the sequence are not taken\n",
    "            into account for computing the loss.\n",
    "        `end_positions`: position of the last token for the labeled span: torch.LongTensor of shape [batch_size].\n",
    "            Positions are clamped to the length of the sequence and position outside of the sequence are not taken\n",
    "            into account for computing the loss.\n",
    "\n",
    "    Outputs:\n",
    "        if `start_positions` and `end_positions` are not `None`:\n",
    "            Outputs the total_loss which is the sum of the CrossEntropy loss for the start and end token positions.\n",
    "        if `start_positions` or `end_positions` is `None`:\n",
    "            Outputs a tuple of start_logits, end_logits which are the logits respectively for the start and end\n",
    "            position tokens of shape [batch_size, sequence_length].\n",
    "\n",
    "    Example usage:\n",
    "    ```python\n",
    "    # Already been converted into WordPiece token ids\n",
    "    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n",
    "    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n",
    "    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n",
    "\n",
    "    config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n",
    "        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n",
    "\n",
    "    model = BertForQuestionAnswering(config)\n",
    "    start_logits, end_logits = model(input_ids, token_type_ids, input_mask)\n",
    "    ```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import collections\n",
    "import logging\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "from bert_utils.bert_utils import *\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "from pytorch_pretrained_bert.tokenization import whitespace_tokenize, BasicTokenizer, BertTokenizer\n",
    "from pytorch_pretrained_bert.modeling import BertForQuestionAnswering\n",
    "from pytorch_pretrained_bert.optimization import BertAdam\n",
    "from pytorch_pretrained_bert.file_utils import PYTORCH_PRETRAINED_BERT_CACHE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {'language':'eng', 'model_config':123}\n",
    "\n",
    "## logger settings\n",
    "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
    "                    level = logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "if 'eng' in config['language']:\n",
    "    train_file = './datasets/squad_v1.1/train-v1.1.json'\n",
    "    dev_file = './datasets/squad_v1.1/dev-v1.1.json'\n",
    "elif 'kor' in config['language']:\n",
    "    train_file = './datasets/korquad_v1/KorQuAD_v1.0_train.json'\n",
    "    dev_file = './datasets/korquad_v1/KorQuAD_v1.0_dev.json'    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json(file_name):\n",
    "    print(\"reading {}\".format(file_name))\n",
    "    with open(file_name, \"r\", encoding='utf-8') as reader:\n",
    "        # >> type(input_data) -> list\n",
    "        input_data = json.load(reader)[\"data\"]\n",
    "    print(\"success to read {}\".format(file_name))\n",
    "    return input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "reading ./datasets/squad_v1.1/train-v1.1.json\nsuccess to read ./datasets/squad_v1.1/train-v1.1.json\n"
    }
   ],
   "source": [
    "input_data = read_json(train_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "explore the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(list, dict, dict_keys(['title', 'paragraphs']), list)"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "type(input_data), type(input_data[0]), input_data[0].keys(), type(input_data[0]['paragraphs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'context': 'Following the disbandment of Destiny\\'s Child in June 2005, she released her second solo album, B\\'Day (2006), which contained hits \"Déjà Vu\", \"Irreplaceable\", and \"Beautiful Liar\". Beyoncé also ventured into acting, with a Golden Globe-nominated performance in Dreamgirls (2006), and starring roles in The Pink Panther (2006) and Obsessed (2009). Her marriage to rapper Jay Z and portrayal of Etta James in Cadillac Records (2008) influenced her third album, I Am... Sasha Fierce (2008), which saw the birth of her alter-ego Sasha Fierce and earned a record-setting six Grammy Awards in 2010, including Song of the Year for \"Single Ladies (Put a Ring on It)\". Beyoncé took a hiatus from music in 2010 and took over management of her career; her fourth album 4 (2011) was subsequently mellower in tone, exploring 1970s funk, 1980s pop, and 1990s soul. Her critically acclaimed fifth studio album, Beyoncé (2013), was distinguished from previous releases by its experimental production and exploration of darker themes.',\n 'qas': [{'answers': [{'answer_start': 207, 'text': 'acting'}],\n   'question': 'After her second solo album, what other entertainment venture did Beyonce explore?',\n   'id': '56be86cf3aeaaa14008c9076'},\n  {'answers': [{'answer_start': 369, 'text': 'Jay Z'}],\n   'question': 'Which artist did Beyonce marry?',\n   'id': '56be86cf3aeaaa14008c9078'},\n  {'answers': [{'answer_start': 565, 'text': 'six'}],\n   'question': 'To set the record for Grammys, how many did Beyonce win?',\n   'id': '56be86cf3aeaaa14008c9079'},\n  {'answers': [{'answer_start': 260, 'text': 'Dreamgirls'}],\n   'question': 'For what movie did Beyonce receive  her first Golden Globe nomination?',\n   'id': '56bf6e823aeaaa14008c9627'},\n  {'answers': [{'answer_start': 586, 'text': '2010'}],\n   'question': 'When did Beyonce take a hiatus in her career and take control of her management?',\n   'id': '56bf6e823aeaaa14008c9629'},\n  {'answers': [{'answer_start': 180, 'text': 'Beyoncé'}],\n   'question': 'Which album was darker in tone from her previous work?',\n   'id': '56bf6e823aeaaa14008c962a'},\n  {'answers': [{'answer_start': 406, 'text': 'Cadillac Records'}],\n   'question': 'After what movie portraying Etta James, did Beyonce create Sasha Fierce?',\n   'id': '56bf6e823aeaaa14008c962b'},\n  {'answers': [{'answer_start': 48, 'text': 'June 2005'}],\n   'question': \"When did Destiny's Child end their group act?\",\n   'id': '56d43da72ccc5a1400d830bd'},\n  {'answers': [{'answer_start': 95, 'text': \"B'Day\"}],\n   'question': \"What was the name of Beyoncé's second solo album?\",\n   'id': '56d43da72ccc5a1400d830be'},\n  {'answers': [{'answer_start': 260, 'text': 'Dreamgirls'}],\n   'question': \"What was Beyoncé's first acting job, in 2006?\",\n   'id': '56d43da72ccc5a1400d830bf'},\n  {'answers': [{'answer_start': 369, 'text': 'Jay Z'}],\n   'question': 'Who is Beyoncé married to?',\n   'id': '56d43da72ccc5a1400d830c0'},\n  {'answers': [{'answer_start': 466, 'text': 'Sasha Fierce'}],\n   'question': \"What is the name of Beyoncé's alter-ego?\",\n   'id': '56d43da72ccc5a1400d830c1'}]}"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "input_data[1]['paragraphs'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## parse datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SquadExample(object):\n",
    "    \"\"\"A single training/test example for the Squad dataset.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 qas_id,\n",
    "                 question_text,\n",
    "                 doc_tokens,\n",
    "                 orig_answer_text=None,\n",
    "                 start_position=None,\n",
    "                 end_position=None):\n",
    "        self.qas_id = qas_id\n",
    "        self.question_text = question_text\n",
    "        self.doc_tokens = doc_tokens\n",
    "        self.orig_answer_text = orig_answer_text\n",
    "        self.start_position = start_position\n",
    "        self.end_position = end_position\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.__repr__()\n",
    "\n",
    "    def __repr__(self):\n",
    "        s = \"\"\n",
    "        s += \"qas_id: %s\" % (self.qas_id)\n",
    "        s += \"\\n\\n, question_text: %s\" % (\n",
    "            self.question_text)\n",
    "        s += \"\\n\\n, orig_answer_text: %s\" % (\n",
    "            self.orig_answer_text)\n",
    "        s += \"\\n\\n, doc_tokens: [%s]\" % (\" \".join(self.doc_tokens))\n",
    "        if self.start_position:\n",
    "            s += \"\\n\\n, start_position: %d\" % (self.start_position)\n",
    "        if self.start_position:\n",
    "            s += \"\\n\\n, end_position: %d\" % (self.end_position)\n",
    "        return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 실습 자료"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_whitespace(c):\n",
    "    if c == \" \" or c == \"\\t\" or c == \"\\r\" or c == \"\\n\" or ord(c) == 0x202F:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def parse_json_squad(input_data, is_train):\n",
    "    \"\"\"Read a SQuAD json file into a list of SquadExample.\"\"\"\n",
    "    examples = list()\n",
    "    for data_entry in input_data:\n",
    "        for paragraph in data_entry['paragraphs']:\n",
    "            paragraph_text = paragraph[\"context\"]\n",
    "            doc_tokens = []\n",
    "            char_to_word_offset = []\n",
    "            prev_is_whitespace = True\n",
    "\n",
    "            # Q1. doc_tokens에 whitespace(c)를 가지고 context를 토큰화하는 코드를 작성하세요. --> 시험엔 안 나옴\n",
    "            ###################################################################################################\n",
    "            for char in paragraph_text:\n",
    "                if is_whitespace(char):\n",
    "                    prev_is_whitespace = True\n",
    "                else:\n",
    "                    if prev_is_whitespace:\n",
    "                        doc_tokens.append(char)  # 리스트에 신규 항목 추가\n",
    "                    else:\n",
    "                        doc_tokens[-1] += char  # 기존에 있는 항목 뒤에 문자 추가\n",
    "                    prev_is_whitespace = False\n",
    "                char_to_word_offset.append(len(doc_tokens) - 1)  # Which word is the character in?; char 하나씩 추가되어 가면서 offset 정보 추가\n",
    "            ###################################################################################################\n",
    "    \n",
    "            for qa in paragraph[\"qas\"]:\n",
    "                \"\"\"\n",
    "                {'answers': [{'answer_start', 'text'}], 'question', 'id'}\n",
    "                \"\"\"\n",
    "                qas_id = qa[\"id\"]\n",
    "                question_text = qa[\"question\"]\n",
    "                start_position = None\n",
    "                end_position = None\n",
    "                orig_answer_text = None\n",
    "\n",
    "                if is_train:\n",
    "                    if len(qa[\"answers\"]) != 1:\n",
    "                        raise ValueError(\n",
    "                            \"For training, each question should have exactly 1 answer.\")\n",
    "                    \n",
    "                    # Q2. Line 34의 변수를 참고하여 Line 70: SquadExample의 instance를 만들기 위한 파라미터를 채우세요.  \n",
    "                    ###################################################################################################\n",
    "                    qas_id = qa[\"id\"]     # fill the black -> assign None\n",
    "                    question_text = qa[\"question\"] # fill the black # index of word\n",
    "                    answer = qa[\"answers\"][0]\n",
    "                    orig_answer_text = answer[\"text\"]\n",
    "                    answer_offset = answer[\"answer_start\"]\n",
    "                    answer_length = len(orig_answer_text)\n",
    "                    start_position = char_to_word_offset[answer_offset] # index of word\n",
    "                    end_position = char_to_word_offset[answer_offset + answer_length - 1] # index of word \n",
    "                    ###################################################################################################\n",
    "\n",
    "                    # CODE FOR Handling exceptions \n",
    "                    # Only add answers where the text can be exactly recovered from the\n",
    "                    # document. If this CAN'T happen it's likely due to weird Unicode\n",
    "                    # stuff so we will just skip the example.\n",
    "                    #\n",
    "                    # Note that this means for training mode, every example is NOT\n",
    "                    # guaranteed to be preserved.\n",
    "                    actual_text = \" \".join(doc_tokens[start_position:(end_position + 1)])\n",
    "                    cleaned_answer_text = \" \".join(whitespace_tokenize(\n",
    "                        orig_answer_text))  # segment words from the sentense including the white space\n",
    "                    if actual_text.find(cleaned_answer_text) == -1:\n",
    "                        logger.warning(\"Could not find answer: '%s' vs. '%s'\",\n",
    "                                       actual_text, cleaned_answer_text)\n",
    "                        continue\n",
    "\n",
    "                example = SquadExample(\n",
    "                    qas_id=qas_id,\n",
    "                    question_text=question_text,\n",
    "                    doc_tokens=doc_tokens,  # a set of tokens(words) in the\n",
    "                    orig_answer_text=orig_answer_text,\n",
    "                    start_position=start_position,\n",
    "                    end_position=end_position)\n",
    "                examples.append(example)\n",
    "    print(\"success to convert input data into a set of {} examples\".format(len(examples)))\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "success to convert input data into a set of 87599 examples\n"
    }
   ],
   "source": [
    "train_examples = parse_json_squad(input_data, True)\n",
    "## len of examples 87599"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "qas_id: 5733be284776f41900661182\n\n, question_text: To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\n\n, orig_answer_text: Saint Bernadette Soubirous\n\n, doc_tokens: [Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.]\n\n, start_position: 90\n\n, end_position: 92\n"
    }
   ],
   "source": [
    "print(train_examples[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 unique_id,\n",
    "                 example_index,\n",
    "                 doc_span_index,\n",
    "                 tokens,\n",
    "                 token_to_orig_map,\n",
    "                 token_is_max_context,\n",
    "                 input_ids,\n",
    "                 input_mask,\n",
    "                 segment_ids,\n",
    "                 start_position=None,\n",
    "                 end_position=None):\n",
    "        \n",
    "        self.unique_id = unique_id\n",
    "        self.example_index = example_index\n",
    "        self.doc_span_index = doc_span_index\n",
    "        self.tokens = tokens\n",
    "        self.token_to_orig_map = token_to_orig_map\n",
    "        self.token_is_max_context = token_is_max_context\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.start_position = start_position\n",
    "        self.end_position = end_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_examples_to_features(examples, tokenizer, max_seq_length,\n",
    "                                 doc_stride, max_query_length, is_training):\n",
    "    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
    "\n",
    "    unique_id = 1000000000\n",
    "\n",
    "    features = []\n",
    "    \n",
    "    for (example_index, example) in enumerate(examples):\n",
    "        \n",
    "        # Q3. Pretrained model을 학습하는데 사용한 Tokenizer를 사용하여 question과 context의 token을 sub-token으로 토큰화해주세요.  \n",
    "        ###################################################################################################\n",
    "        query_tokens = tokenizer.tokenize(example.question_text)\n",
    "        if len(query_tokens) > max_query_length:\n",
    "            query_tokens = query_tokens[0:max_query_length]\n",
    "        tok_to_orig_index = []\n",
    "        orig_to_tok_index = []\n",
    "        all_doc_tokens = [] \n",
    "        for (i, token) in enumerate(example.doc_tokens):  # token == word\n",
    "            orig_to_tok_index.append(len(all_doc_tokens))\n",
    "            sub_tokens = tokenizer.tokenize(token)\n",
    "            for sub_token in sub_tokens:  # sub_token == wordpiece\n",
    "                tok_to_orig_index.append(i)                  \n",
    "                all_doc_tokens.append(sub_token)\n",
    "        ## fill black with the code for appending some values to the above lists\n",
    "        ###################################################################################################\n",
    "                            \n",
    "        # Q4. sub-tokens에 맞추어 span을 업데이트 해주세요.\n",
    "        ###################################################################################################\n",
    "        # fill the code for updating the span of token (tok_start_position, tok_end_position)\n",
    "        tok_start_position = None\n",
    "        tok_end_position = None\n",
    "        if is_training:\n",
    "            tok_start_position = orig_to_tok_index[example.start_position]\n",
    "            if example.end_position < len(example.doc_tokens) - 1:\n",
    "                tok_end_position = orig_to_tok_index[example.end_position + 1] - 1\n",
    "            else:\n",
    "                tok_end_position = len(all_doc_tokens) - 1\n",
    "            (tok_start_position, tok_end_position) = improve_answer_span(\n",
    "                all_doc_tokens, tok_start_position, tok_end_position, tokenizer,\n",
    "                example.orig_answer_text)\n",
    "            \n",
    "        ####################################################################################################\n",
    "        \n",
    "        # The -3 accounts for [CLS], [SEP] and [SEP]  --> 첫 번째 시험문제\n",
    "        max_tokens_for_doc = max_seq_length - len(query_tokens) - 3\n",
    "\n",
    "        # doc_tokens가 설정한 max length를 넘는다면, 몇개의 DocSpan으로 쪼개야 합니다.\n",
    "        # We can have documents that are longer than the maximum sequence length.\n",
    "        # To deal with this we do a sliding window approach, where we take chunks\n",
    "        # of the up to our max length with a stride of `doc_stride`.\n",
    "        _DocSpan = collections.namedtuple(  # pylint: disable=invalid-name\n",
    "            \"DocSpan\", [\"start\", \"length\"])\n",
    "        doc_spans = []\n",
    "        start_offset = 0\n",
    "        while start_offset < len(all_doc_tokens):\n",
    "            length = len(all_doc_tokens) - start_offset\n",
    "            if length > max_tokens_for_doc:\n",
    "                length = max_tokens_for_doc\n",
    "            doc_spans.append(_DocSpan(start=start_offset, length=length))\n",
    "            if start_offset + length == len(all_doc_tokens):\n",
    "                break\n",
    "            start_offset += min(length, doc_stride)\n",
    "            \n",
    "        for (doc_span_index, doc_span) in enumerate(doc_spans):\n",
    "            tokens = [] # input data\n",
    "            segment_ids = [] # segment data\n",
    "            token_to_orig_map = {}\n",
    "            token_is_max_context = {}\n",
    "\n",
    "            # Q5. query를 pretrained BERT의 입력값(features) 형식에 따라 바꿔주세요.\n",
    "              # tokens -> [CLS] question [SEP] context [SEP]\n",
    "              # segment_ids -> 00000000000000000000 1111111111111\n",
    "            ###################################################################################################            \n",
    "            tokens.append(\"[CLS]\")\n",
    "            segment_ids.append(0)\n",
    "            for token in query_tokens:\n",
    "                ## fill the blank ##        \n",
    "                tokens.append(token)\n",
    "                segment_ids.append(0)\n",
    "\n",
    "            tokens.append(\"[SEP]\")\n",
    "            segment_ids.append(0)\n",
    "            ###################################################################################################\n",
    "\n",
    "                \n",
    "            # Q6. context를 pretrained BERT의 입력값(features) 형식에 따라 바꿔주세요.\n",
    "            ###################################################################################################\n",
    "            for i in range(doc_span.length):\n",
    "                split_token_index = doc_span.start + i\n",
    "                token_to_orig_map[len(tokens)] = tok_to_orig_index[split_token_index]\n",
    "                is_max_context = check_is_max_context(doc_spans, doc_span_index,\n",
    "                                                       split_token_index)\n",
    "                token_is_max_context[len(tokens)] = is_max_context\n",
    "                tokens.append(all_doc_tokens[split_token_index])\n",
    "                segment_ids.append(1) # segment ids 12 means the context\n",
    "            tokens.append(\"[SEP]\")\n",
    "            segment_ids.append(1)\n",
    "            ###################################################################################################\n",
    "            \n",
    "            # convert into the index of emmeding matrix\n",
    "            input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "            \n",
    "            # Q7. pretrained BERT의 입력값 크기에 맞게 zero-padding 해주세요. --> 시험 문제\n",
    "            ###################################################################################################\n",
    "            # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "            # tokens are attended to.\n",
    "            input_mask = [1] * len(input_ids)\n",
    "            while len(input_ids) < max_seq_length:\n",
    "                input_ids.append(0)\n",
    "                input_mask.append(0)\n",
    "                segment_ids.append(0)\n",
    "            ###################################################################################################\n",
    "            assert len(input_ids) == max_seq_length\n",
    "            assert len(input_mask) == max_seq_length\n",
    "            assert len(segment_ids) == max_seq_length\n",
    "            \n",
    "            start_position = None\n",
    "            end_position = None\n",
    "            if is_training:\n",
    "                # For training, if our document chunk does not contain an annotation\n",
    "                # we throw it out, since there is nothing to predict. --> 현재 DocSpan에 정답이 없으면 학습 때 안 보고 스킵한다\n",
    "                doc_start = doc_span.start\n",
    "                doc_end = doc_span.start + doc_span.length - 1\n",
    "                if (example.start_position < doc_start or\n",
    "                        example.end_position < doc_start or\n",
    "                        example.start_position > doc_end or example.end_position > doc_end):\n",
    "                    continue # -> next to the DocSpan \n",
    "\n",
    "                doc_offset = len(query_tokens) + 2\n",
    "                start_position = tok_start_position - doc_start + doc_offset\n",
    "                end_position = tok_end_position - doc_start + doc_offset\n",
    "\n",
    "            if example_index < 20:\n",
    "                logger.info(\"*** Example ***\")\n",
    "                logger.info(\"unique_id: %s\" % (unique_id))\n",
    "                logger.info(\"example_index: %s\" % (example_index))\n",
    "                logger.info(\"doc_span_index: %s\" % (doc_span_index))\n",
    "                logger.info(\"tokens: %s\" % \" \".join(tokens))\n",
    "                logger.info(\"token_to_orig_map: %s\" % \" \".join([\n",
    "                    \"%d:%d\" % (x, y) for (x, y) in token_to_orig_map.items()]))\n",
    "                logger.info(\"token_is_max_context: %s\" % \" \".join([\n",
    "                    \"%d:%s\" % (x, y) for (x, y) in token_is_max_context.items()\n",
    "                ]))\n",
    "                logger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
    "                logger.info(\n",
    "                    \"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
    "                logger.info(\n",
    "                    \"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
    "                if is_training:\n",
    "                    answer_text = \" \".join(tokens[start_position:(end_position + 1)])\n",
    "                    logger.info(\"start_position: %d\" % (start_position))\n",
    "                    logger.info(\"end_position: %d\" % (end_position))\n",
    "                    logger.info(\n",
    "                        \"answer: %s\" % (answer_text))\n",
    "                    \n",
    "            features.append(\n",
    "                InputFeatures(\n",
    "                    unique_id=unique_id,\n",
    "                    example_index=example_index,\n",
    "                    doc_span_index=doc_span_index,\n",
    "                    tokens=tokens,\n",
    "                    token_to_orig_map=token_to_orig_map,\n",
    "                    token_is_max_context=token_is_max_context,\n",
    "                    input_ids=input_ids,\n",
    "                    input_mask=input_mask,\n",
    "                    segment_ids=segment_ids,\n",
    "                    start_position=start_position,\n",
    "                    end_position=end_position))\n",
    "            unique_id += 1\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap the train_features in a `Dataloader`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "08/12/2020 13:01:27 - WARNING - pytorch_pretrained_bert.tokenization -   The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n08/12/2020 13:01:28 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-vocab.txt from cache at /home/com10/.pytorch_pretrained_bert/96435fa287fbf7e469185f1062386e05a075cadbf6838b74da22bf64b080bc32.99bcd55fc66f4f3360bc49ba472b940b8dcf223ea6a345deb969d607ca900729\n"
    }
   ],
   "source": [
    "config='bert-base-multilingual-cased'\n",
    "tokenizer = BertTokenizer.from_pretrained(config)\n",
    "max_seq_length=128\n",
    "doc_stride=128\n",
    "max_query_length=64\n",
    "cached_train_features_file = train_file+'_{0}_{1}_{2}'.format(config, str(max_seq_length), str(doc_stride))\n",
    "\n",
    "train_batch_size=15\n",
    "predict_batch_size=15\n",
    "num_train_epochs=2\n",
    "gradient_accumulation_steps=1\n",
    "warmup_proportion=0.1\n",
    "learning_rate=5e-5\n",
    "num_train_steps = int(num_train_epochs * len(train_examples) / train_batch_size) #  /gradient_accumulation_steps *num_train_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    with open(cached_train_features_file, \"rb\") as reader:\n",
    "        train_features = pickle.load(reader)\n",
    "except:\n",
    "    train_features = convert_examples_to_features(\n",
    "        examples=train_examples,\n",
    "        tokenizer=tokenizer,\n",
    "        max_seq_length=max_seq_length,\n",
    "        doc_stride=doc_stride,\n",
    "        max_query_length=max_query_length,\n",
    "        is_training=True)\n",
    "    print('finish extracting the features from the examples')\n",
    "    logger.info(\"  Saving train features into cached file %s\", cached_train_features_file)\n",
    "    with open(cached_train_features_file, \"wb\") as writer:\n",
    "        pickle.dump(train_features, writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "08/12/2020 13:01:31 - INFO - __main__ -   ***** Running training *****\n08/12/2020 13:01:31 - INFO - __main__ -     Num orig examples = 87599\n08/12/2020 13:01:31 - INFO - __main__ -     Num split examples = 87056\n08/12/2020 13:01:31 - INFO - __main__ -     Batch size = 15\n08/12/2020 13:01:31 - INFO - __main__ -     Num steps = 11679\n"
    }
   ],
   "source": [
    "logger.info(\"***** Running training *****\")\n",
    "logger.info(\"  Num orig examples = %d\", len(train_examples))\n",
    "logger.info(\"  Num split examples = %d\", len(train_features))\n",
    "logger.info(\"  Batch size = %d\", train_batch_size)\n",
    "logger.info(\"  Num steps = %d\", num_train_steps)\n",
    "\n",
    "\n",
    "# Q8. 데이터로더에 데이터를 로드하기 위해 tensor로 data type을 torch.Tensor로 바꿔주세요.\n",
    "###################################################################################################\n",
    "# fill the blank\n",
    "all_input_ids = torch.tensor([f.input_ids for f in train_features], dtype=torch.long) ## must be long type\n",
    "all_input_mask = torch.tensor([f.input_mask for f in train_features], dtype=torch.long)\n",
    "all_segment_ids = torch.tensor([f.segment_ids for f in train_features], dtype=torch.long)\n",
    "all_start_positions = torch.tensor([f.start_position for f in train_features], dtype=torch.long)\n",
    "all_end_positions = torch.tensor([f.end_position for f in train_features], dtype=torch.long)\n",
    "###################################################################################################\n",
    "\n",
    "train_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids,all_start_positions, all_end_positions)                           \n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=train_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "08/12/2020 13:01:33 - INFO - pytorch_pretrained_bert.modeling -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased.tar.gz from cache at /home/com10/.pytorch_pretrained_bert/731c19ddf94e294e00ec1ba9a930c69cc2a0fd489b25d3d691373fae4c0986bd.4e367b0d0155d801930846bb6ed98f8a7c23e0ded37888b29caa37009a40c7b9\n08/12/2020 13:01:33 - INFO - pytorch_pretrained_bert.modeling -   extracting archive file /home/com10/.pytorch_pretrained_bert/731c19ddf94e294e00ec1ba9a930c69cc2a0fd489b25d3d691373fae4c0986bd.4e367b0d0155d801930846bb6ed98f8a7c23e0ded37888b29caa37009a40c7b9 to temp dir /tmp/tmpob79dc7x\n08/12/2020 13:01:36 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n  \"attention_probs_dropout_prob\": 0.1,\n  \"directionality\": \"bidi\",\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"max_position_embeddings\": 512,\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pooler_fc_size\": 768,\n  \"pooler_num_attention_heads\": 12,\n  \"pooler_num_fc_layers\": 3,\n  \"pooler_size_per_head\": 128,\n  \"pooler_type\": \"first_token_transform\",\n  \"type_vocab_size\": 2,\n  \"vocab_size\": 119547\n}\n\n08/12/2020 13:01:39 - INFO - pytorch_pretrained_bert.modeling -   Weights of BertForQuestionAnswering not initialized from pretrained model: ['qa_outputs.weight', 'qa_outputs.bias']\n08/12/2020 13:01:39 - INFO - pytorch_pretrained_bert.modeling -   Weights from pretrained model not used in BertForQuestionAnswering: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "BertForQuestionAnswering(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): BertLayerNorm()\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (1): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (2): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (3): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (4): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (5): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (6): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (7): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (8): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (9): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (10): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (11): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n)"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "model = BertForQuestionAnswering.from_pretrained(config)\n",
    "local_rank = -1\n",
    "gpu_num=0\n",
    "device = torch.device(f\"cuda:{gpu_num}\")\n",
    "t_total = num_train_steps\n",
    "model.to(device)  # [2] TITAN Xp         | 43'C,   0 % |  1235 / 12196 MB | gyuhyeon(1223M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_total = num_train_steps\n",
    "param_optimizer = list(model.named_parameters())\n",
    "\n",
    "# hack to remove pooler, which is not used\n",
    "# thus it produce None grad that break apex\n",
    "param_optimizer = [n for n in param_optimizer if 'pooler' not in n[0]]  # remove the first class label('pooler')\n",
    "\n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "\n",
    "t_total = num_train_steps\n",
    "\n",
    "# find BertAdam in the https://huggingface.co/transformers/migration.html\n",
    "optimizer = BertAdam(optimizer_grouped_parameters,\n",
    "                     lr=learning_rate,\n",
    "                     warmup=warmup_proportion,\n",
    "                     t_total=t_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": ",  3.30it/s]\u001b[A\nIteration:  95%|█████████▍| 5503/5804 [27:47<01:31,  3.30it/s]\u001b[A\nIteration:  95%|█████████▍| 5504/5804 [27:47<01:30,  3.30it/s]\u001b[A\nIteration:  95%|█████████▍| 5505/5804 [27:47<01:30,  3.30it/s]\u001b[A\nIteration:  95%|█████████▍| 5506/5804 [27:48<01:30,  3.30it/s]\u001b[A\nIteration:  95%|█████████▍| 5507/5804 [27:48<01:30,  3.30it/s]\u001b[A\nIteration:  95%|█████████▍| 5508/5804 [27:48<01:29,  3.29it/s]\u001b[A\nIteration:  95%|█████████▍| 5509/5804 [27:49<01:29,  3.29it/s]\u001b[A\nIteration:  95%|█████████▍| 5510/5804 [27:49<01:29,  3.30it/s]\u001b[A\nIteration:  95%|█████████▍| 5511/5804 [27:49<01:28,  3.30it/s]\u001b[A\nIteration:  95%|█████████▍| 5512/5804 [27:50<01:28,  3.30it/s]\u001b[A\nIteration:  95%|█████████▍| 5513/5804 [27:50<01:27,  3.31it/s]\u001b[A\nIteration:  95%|█████████▌| 5514/5804 [27:50<01:27,  3.31it/s]\u001b[A\nIteration:  95%|█████████▌| 5515/5804 [27:51<01:27,  3.31it/s]\u001b[A\nIteration:  95%|█████████▌| 5516/5804 [27:51<01:27,  3.31it/s]\u001b[A\nIteration:  95%|█████████▌| 5517/5804 [27:51<01:26,  3.31it/s]\u001b[A\nIteration:  95%|█████████▌| 5518/5804 [27:51<01:26,  3.31it/s]\u001b[A\nIteration:  95%|█████████▌| 5519/5804 [27:52<01:26,  3.31it/s]\u001b[A\nIteration:  95%|█████████▌| 5520/5804 [27:52<01:25,  3.31it/s]\u001b[A\nIteration:  95%|█████████▌| 5521/5804 [27:52<01:25,  3.31it/s]\u001b[A\nIteration:  95%|█████████▌| 5522/5804 [27:53<01:25,  3.31it/s]\u001b[A\nIteration:  95%|█████████▌| 5523/5804 [27:53<01:24,  3.31it/s]\u001b[A\nIteration:  95%|█████████▌| 5524/5804 [27:53<01:24,  3.31it/s]\u001b[A\nIteration:  95%|█████████▌| 5525/5804 [27:54<01:24,  3.31it/s]\u001b[A\nIteration:  95%|█████████▌| 5526/5804 [27:54<01:24,  3.30it/s]\u001b[A\nIteration:  95%|█████████▌| 5527/5804 [27:54<01:23,  3.30it/s]\u001b[A\nIteration:  95%|█████████▌| 5528/5804 [27:54<01:23,  3.30it/s]\u001b[A\nIteration:  95%|█████████▌| 5529/5804 [27:55<01:23,  3.30it/s]\u001b[A\nIteration:  95%|█████████▌| 5530/5804 [27:55<01:22,  3.30it/s]\u001b[A\nIteration:  95%|█████████▌| 5531/5804 [27:55<01:22,  3.30it/s]\u001b[A\nIteration:  95%|█████████▌| 5532/5804 [27:56<01:22,  3.31it/s]\u001b[A\nIteration:  95%|█████████▌| 5533/5804 [27:56<01:21,  3.31it/s]\u001b[A\nIteration:  95%|█████████▌| 5534/5804 [27:56<01:21,  3.31it/s]\u001b[A\nIteration:  95%|█████████▌| 5535/5804 [27:57<01:21,  3.31it/s]\u001b[A\nIteration:  95%|█████████▌| 5536/5804 [27:57<01:21,  3.31it/s]\u001b[A\nIteration:  95%|█████████▌| 5537/5804 [27:57<01:20,  3.30it/s]\u001b[A\nIteration:  95%|█████████▌| 5538/5804 [27:57<01:20,  3.30it/s]\u001b[A\nIteration:  95%|█████████▌| 5539/5804 [27:58<01:20,  3.30it/s]\u001b[A\nIteration:  95%|█████████▌| 5540/5804 [27:58<01:19,  3.30it/s]\u001b[A\nIteration:  95%|█████████▌| 5541/5804 [27:58<01:19,  3.30it/s]\u001b[A\nIteration:  95%|█████████▌| 5542/5804 [27:59<01:19,  3.30it/s]\u001b[A\nIteration:  96%|█████████▌| 5543/5804 [27:59<01:18,  3.30it/s]\u001b[A\nIteration:  96%|█████████▌| 5544/5804 [27:59<01:18,  3.31it/s]\u001b[A\nIteration:  96%|█████████▌| 5545/5804 [28:00<01:18,  3.30it/s]\u001b[A\nIteration:  96%|█████████▌| 5546/5804 [28:00<01:18,  3.30it/s]\u001b[A\nIteration:  96%|█████████▌| 5547/5804 [28:00<01:17,  3.31it/s]\u001b[A\nIteration:  96%|█████████▌| 5548/5804 [28:01<01:17,  3.30it/s]\u001b[A\nIteration:  96%|█████████▌| 5549/5804 [28:01<01:17,  3.30it/s]\u001b[A\nIteration:  96%|█████████▌| 5550/5804 [28:01<01:16,  3.30it/s]\u001b[A\nIteration:  96%|█████████▌| 5551/5804 [28:01<01:16,  3.30it/s]\u001b[A\nIteration:  96%|█████████▌| 5552/5804 [28:02<01:16,  3.30it/s]\u001b[A\nIteration:  96%|█████████▌| 5553/5804 [28:02<01:16,  3.30it/s]\u001b[A\nIteration:  96%|█████████▌| 5554/5804 [28:02<01:15,  3.30it/s]\u001b[A\nIteration:  96%|█████████▌| 5555/5804 [28:03<01:15,  3.30it/s]\u001b[A\nIteration:  96%|█████████▌| 5556/5804 [28:03<01:15,  3.30it/s]\u001b[A\nIteration:  96%|█████████▌| 5557/5804 [28:03<01:14,  3.30it/s]\u001b[A\nIteration:  96%|█████████▌| 5558/5804 [28:04<01:14,  3.30it/s]\u001b[A\nIteration:  96%|█████████▌| 5559/5804 [28:04<01:14,  3.30it/s]\u001b[A\nIteration:  96%|█████████▌| 5560/5804 [28:04<01:13,  3.30it/s]\u001b[A\nIteration:  96%|█████████▌| 5561/5804 [28:04<01:13,  3.30it/s]\u001b[A\nIteration:  96%|█████████▌| 5562/5804 [28:05<01:13,  3.30it/s]\u001b[A\nIteration:  96%|█████████▌| 5563/5804 [28:05<01:12,  3.30it/s]\u001b[A\nIteration:  96%|█████████▌| 5564/5804 [28:05<01:12,  3.30it/s]\u001b[A\nIteration:  96%|█████████▌| 5565/5804 [28:06<01:12,  3.30it/s]\u001b[A\nIteration:  96%|█████████▌| 5566/5804 [28:06<01:12,  3.30it/s]\u001b[A\nIteration:  96%|█████████▌| 5567/5804 [28:06<01:11,  3.30it/s]\u001b[A\nIteration:  96%|█████████▌| 5568/5804 [28:07<01:11,  3.30it/s]\u001b[A\nIteration:  96%|█████████▌| 5569/5804 [28:07<01:11,  3.29it/s]\u001b[A\nIteration:  96%|█████████▌| 5570/5804 [28:07<01:11,  3.29it/s]\u001b[A\nIteration:  96%|█████████▌| 5571/5804 [28:07<01:10,  3.29it/s]\u001b[A\nIteration:  96%|█████████▌| 5572/5804 [28:08<01:10,  3.29it/s]\u001b[A\nIteration:  96%|█████████▌| 5573/5804 [28:08<01:10,  3.29it/s]\u001b[A\nIteration:  96%|█████████▌| 5574/5804 [28:08<01:09,  3.29it/s]\u001b[A\nIteration:  96%|█████████▌| 5575/5804 [28:09<01:09,  3.29it/s]\u001b[A\nIteration:  96%|█████████▌| 5576/5804 [28:09<01:09,  3.29it/s]\u001b[A\nIteration:  96%|█████████▌| 5577/5804 [28:09<01:09,  3.29it/s]\u001b[A\nIteration:  96%|█████████▌| 5578/5804 [28:10<01:08,  3.29it/s]\u001b[A\nIteration:  96%|█████████▌| 5579/5804 [28:10<01:08,  3.29it/s]\u001b[A\nIteration:  96%|█████████▌| 5580/5804 [28:10<01:08,  3.29it/s]\u001b[A\nIteration:  96%|█████████▌| 5581/5804 [28:11<01:07,  3.29it/s]\u001b[A\nIteration:  96%|█████████▌| 5582/5804 [28:11<01:07,  3.29it/s]\u001b[A\nIteration:  96%|█████████▌| 5583/5804 [28:11<01:07,  3.29it/s]\u001b[A\nIteration:  96%|█████████▌| 5584/5804 [28:11<01:06,  3.30it/s]\u001b[A\nIteration:  96%|█████████▌| 5585/5804 [28:12<01:06,  3.30it/s]\u001b[A\nIteration:  96%|█████████▌| 5586/5804 [28:12<01:06,  3.30it/s]\u001b[A\nIteration:  96%|█████████▋| 5587/5804 [28:12<01:05,  3.30it/s]\u001b[A\nIteration:  96%|█████████▋| 5588/5804 [28:13<01:05,  3.30it/s]\u001b[A\nIteration:  96%|█████████▋| 5589/5804 [28:13<01:05,  3.30it/s]\u001b[A\nIteration:  96%|█████████▋| 5590/5804 [28:13<01:04,  3.31it/s]\u001b[A\nIteration:  96%|█████████▋| 5591/5804 [28:14<01:04,  3.31it/s]\u001b[A\nIteration:  96%|█████████▋| 5592/5804 [28:14<01:04,  3.31it/s]\u001b[A\nIteration:  96%|█████████▋| 5593/5804 [28:14<01:03,  3.31it/s]\u001b[A\nIteration:  96%|█████████▋| 5594/5804 [28:14<01:03,  3.31it/s]\u001b[A\nIteration:  96%|█████████▋| 5595/5804 [28:15<01:03,  3.31it/s]\u001b[A\nIteration:  96%|█████████▋| 5596/5804 [28:15<01:02,  3.31it/s]\u001b[A\nIteration:  96%|█████████▋| 5597/5804 [28:15<01:02,  3.31it/s]\u001b[A\nIteration:  96%|█████████▋| 5598/5804 [28:16<01:02,  3.31it/s]\u001b[A\nIteration:  96%|█████████▋| 5599/5804 [28:16<01:01,  3.31it/s]\u001b[A\nIteration:  96%|█████████▋| 5600/5804 [28:16<01:01,  3.31it/s]\u001b[A\nIteration:  97%|█████████▋| 5601/5804 [28:17<01:01,  3.31it/s]\u001b[A\nIteration:  97%|█████████▋| 5602/5804 [28:17<01:01,  3.31it/s]\u001b[A\nIteration:  97%|█████████▋| 5603/5804 [28:17<01:00,  3.31it/s]\u001b[A\nIteration:  97%|█████████▋| 5604/5804 [28:17<01:00,  3.31it/s]\u001b[A\nIteration:  97%|█████████▋| 5605/5804 [28:18<01:00,  3.31it/s]\u001b[A\nIteration:  97%|█████████▋| 5606/5804 [28:18<00:59,  3.31it/s]\u001b[A\nIteration:  97%|█████████▋| 5607/5804 [28:18<00:59,  3.30it/s]\u001b[A\nIteration:  97%|█████████▋| 5608/5804 [28:19<00:59,  3.31it/s]\u001b[A\nIteration:  97%|█████████▋| 5609/5804 [28:19<00:59,  3.30it/s]\u001b[A\nIteration:  97%|█████████▋| 5610/5804 [28:19<00:58,  3.30it/s]\u001b[A\nIteration:  97%|█████████▋| 5611/5804 [28:20<00:58,  3.30it/s]\u001b[A\nIteration:  97%|█████████▋| 5612/5804 [28:20<00:58,  3.30it/s]\u001b[A\nIteration:  97%|█████████▋| 5613/5804 [28:20<00:57,  3.30it/s]\u001b[A\nIteration:  97%|█████████▋| 5614/5804 [28:21<00:57,  3.30it/s]\u001b[A\nIteration:  97%|█████████▋| 5615/5804 [28:21<00:57,  3.31it/s]\u001b[A\nIteration:  97%|█████████▋| 5616/5804 [28:21<00:56,  3.31it/s]\u001b[A\nIteration:  97%|█████████▋| 5617/5804 [28:21<00:56,  3.31it/s]\u001b[A\nIteration:  97%|█████████▋| 5618/5804 [28:22<00:56,  3.31it/s]\u001b[A\nIteration:  97%|█████████▋| 5619/5804 [28:22<00:55,  3.31it/s]\u001b[A\nIteration:  97%|█████████▋| 5620/5804 [28:22<00:55,  3.31it/s]\u001b[A\nIteration:  97%|█████████▋| 5621/5804 [28:23<00:55,  3.31it/s]\u001b[A\nIteration:  97%|█████████▋| 5622/5804 [28:23<00:54,  3.31it/s]\u001b[A\nIteration:  97%|█████████▋| 5623/5804 [28:23<00:54,  3.31it/s]\u001b[A\nIteration:  97%|█████████▋| 5624/5804 [28:24<00:54,  3.31it/s]\u001b[A\nIteration:  97%|█████████▋| 5625/5804 [28:24<00:54,  3.31it/s]\u001b[A\nIteration:  97%|█████████▋| 5626/5804 [28:24<00:53,  3.31it/s]\u001b[A\nIteration:  97%|█████████▋| 5627/5804 [28:24<00:53,  3.30it/s]\u001b[A\nIteration:  97%|█████████▋| 5628/5804 [28:25<00:53,  3.30it/s]\u001b[A\nIteration:  97%|█████████▋| 5629/5804 [28:25<00:53,  3.29it/s]\u001b[A\nIteration:  97%|█████████▋| 5630/5804 [28:25<00:52,  3.29it/s]\u001b[A\nIteration:  97%|█████████▋| 5631/5804 [28:26<00:52,  3.29it/s]\u001b[A\nIteration:  97%|█████████▋| 5632/5804 [28:26<00:52,  3.29it/s]\u001b[A\nIteration:  97%|█████████▋| 5633/5804 [28:26<00:51,  3.29it/s]\u001b[A\nIteration:  97%|█████████▋| 5634/5804 [28:27<00:51,  3.29it/s]\u001b[A\nIteration:  97%|█████████▋| 5635/5804 [28:27<00:51,  3.29it/s]\u001b[A\nIteration:  97%|█████████▋| 5636/5804 [28:27<00:51,  3.29it/s]\u001b[A\nIteration:  97%|█████████▋| 5637/5804 [28:27<00:50,  3.29it/s]\u001b[A\nIteration:  97%|█████████▋| 5638/5804 [28:28<00:50,  3.29it/s]\u001b[A\nIteration:  97%|█████████▋| 5639/5804 [28:28<00:50,  3.30it/s]\u001b[A\nIteration:  97%|█████████▋| 5640/5804 [28:28<00:49,  3.29it/s]\u001b[A\nIteration:  97%|█████████▋| 5641/5804 [28:29<00:49,  3.29it/s]\u001b[A\nIteration:  97%|█████████▋| 5642/5804 [28:29<00:49,  3.29it/s]\u001b[A\nIteration:  97%|█████████▋| 5643/5804 [28:29<00:48,  3.29it/s]\u001b[A\nIteration:  97%|█████████▋| 5644/5804 [28:30<00:48,  3.29it/s]\u001b[A\nIteration:  97%|█████████▋| 5645/5804 [28:30<00:48,  3.30it/s]\u001b[A\nIteration:  97%|█████████▋| 5646/5804 [28:30<00:47,  3.30it/s]\u001b[A\nIteration:  97%|█████████▋| 5647/5804 [28:31<00:47,  3.30it/s]\u001b[A\nIteration:  97%|█████████▋| 5648/5804 [28:31<00:47,  3.30it/s]\u001b[A\nIteration:  97%|█████████▋| 5649/5804 [28:31<00:46,  3.30it/s]\u001b[A\nIteration:  97%|█████████▋| 5650/5804 [28:31<00:46,  3.30it/s]\u001b[A\nIteration:  97%|█████████▋| 5651/5804 [28:32<00:46,  3.31it/s]\u001b[A\nIteration:  97%|█████████▋| 5652/5804 [28:32<00:45,  3.31it/s]\u001b[A\nIteration:  97%|█████████▋| 5653/5804 [28:32<00:45,  3.31it/s]\u001b[A\nIteration:  97%|█████████▋| 5654/5804 [28:33<00:45,  3.31it/s]\u001b[A\nIteration:  97%|█████████▋| 5655/5804 [28:33<00:45,  3.31it/s]\u001b[A\nIteration:  97%|█████████▋| 5656/5804 [28:33<00:44,  3.31it/s]\u001b[A\nIteration:  97%|█████████▋| 5657/5804 [28:34<00:44,  3.31it/s]\u001b[A\nIteration:  97%|█████████▋| 5658/5804 [28:34<00:44,  3.31it/s]\u001b[A\nIteration:  98%|█████████▊| 5659/5804 [28:34<00:43,  3.31it/s]\u001b[A\nIteration:  98%|█████████▊| 5660/5804 [28:34<00:43,  3.31it/s]\u001b[A\nIteration:  98%|█████████▊| 5661/5804 [28:35<00:43,  3.31it/s]\u001b[A\nIteration:  98%|█████████▊| 5662/5804 [28:35<00:42,  3.31it/s]\u001b[A\nIteration:  98%|█████████▊| 5663/5804 [28:35<00:42,  3.31it/s]\u001b[A\nIteration:  98%|█████████▊| 5664/5804 [28:36<00:42,  3.31it/s]\u001b[A\nIteration:  98%|█████████▊| 5665/5804 [28:36<00:42,  3.31it/s]\u001b[A\nIteration:  98%|█████████▊| 5666/5804 [28:36<00:41,  3.31it/s]\u001b[A\nIteration:  98%|█████████▊| 5667/5804 [28:37<00:41,  3.31it/s]\u001b[A\nIteration:  98%|█████████▊| 5668/5804 [28:37<00:41,  3.31it/s]\u001b[A\nIteration:  98%|█████████▊| 5669/5804 [28:37<00:40,  3.31it/s]\u001b[A\nIteration:  98%|█████████▊| 5670/5804 [28:37<00:40,  3.31it/s]\u001b[A\nIteration:  98%|█████████▊| 5671/5804 [28:38<00:40,  3.31it/s]\u001b[A\nIteration:  98%|█████████▊| 5672/5804 [28:38<00:39,  3.31it/s]\u001b[A\nIteration:  98%|█████████▊| 5673/5804 [28:38<00:39,  3.31it/s]\u001b[A\nIteration:  98%|█████████▊| 5674/5804 [28:39<00:39,  3.31it/s]\u001b[A\nIteration:  98%|█████████▊| 5675/5804 [28:39<00:38,  3.31it/s]\u001b[A\nIteration:  98%|█████████▊| 5676/5804 [28:39<00:38,  3.31it/s]\u001b[A\nIteration:  98%|█████████▊| 5677/5804 [28:40<00:38,  3.31it/s]\u001b[A\nIteration:  98%|█████████▊| 5678/5804 [28:40<00:38,  3.31it/s]\u001b[A\nIteration:  98%|█████████▊| 5679/5804 [28:40<00:37,  3.31it/s]\u001b[A\nIteration:  98%|█████████▊| 5680/5804 [28:40<00:37,  3.31it/s]\u001b[A\nIteration:  98%|█████████▊| 5681/5804 [28:41<00:37,  3.31it/s]\u001b[A\nIteration:  98%|█████████▊| 5682/5804 [28:41<00:36,  3.31it/s]\u001b[A\nIteration:  98%|█████████▊| 5683/5804 [28:41<00:36,  3.31it/s]\u001b[A\nIteration:  98%|█████████▊| 5684/5804 [28:42<00:36,  3.31it/s]\u001b[A\nIteration:  98%|█████████▊| 5685/5804 [28:42<00:35,  3.31it/s]\u001b[A\nIteration:  98%|█████████▊| 5686/5804 [28:42<00:35,  3.31it/s]\u001b[A\nIteration:  98%|█████████▊| 5687/5804 [28:43<00:35,  3.31it/s]\u001b[A\nIteration:  98%|█████████▊| 5688/5804 [28:43<00:35,  3.31it/s]\u001b[A\nIteration:  98%|█████████▊| 5689/5804 [28:43<00:34,  3.31it/s]\u001b[A\nIteration:  98%|█████████▊| 5690/5804 [28:44<00:34,  3.31it/s]\u001b[A\nIteration:  98%|█████████▊| 5691/5804 [28:44<00:34,  3.30it/s]\u001b[A\nIteration:  98%|█████████▊| 5692/5804 [28:44<00:33,  3.30it/s]\u001b[A\nIteration:  98%|█████████▊| 5693/5804 [28:44<00:33,  3.30it/s]\u001b[A\nIteration:  98%|█████████▊| 5694/5804 [28:45<00:33,  3.30it/s]\u001b[A\nIteration:  98%|█████████▊| 5695/5804 [28:45<00:33,  3.30it/s]\u001b[A\nIteration:  98%|█████████▊| 5696/5804 [28:45<00:32,  3.29it/s]\u001b[A\nIteration:  98%|█████████▊| 5697/5804 [28:46<00:32,  3.29it/s]\u001b[A\nIteration:  98%|█████████▊| 5698/5804 [28:46<00:32,  3.29it/s]\u001b[A\nIteration:  98%|█████████▊| 5699/5804 [28:46<00:31,  3.29it/s]\u001b[A\nIteration:  98%|█████████▊| 5700/5804 [28:47<00:31,  3.29it/s]\u001b[A\nIteration:  98%|█████████▊| 5701/5804 [28:47<00:31,  3.29it/s]\u001b[A\nIteration:  98%|█████████▊| 5702/5804 [28:47<00:31,  3.29it/s]\u001b[A\nIteration:  98%|█████████▊| 5703/5804 [28:47<00:30,  3.29it/s]\u001b[A\nIteration:  98%|█████████▊| 5704/5804 [28:48<00:30,  3.28it/s]\u001b[A\nIteration:  98%|█████████▊| 5705/5804 [28:48<00:30,  3.29it/s]\u001b[A\nIteration:  98%|█████████▊| 5706/5804 [28:48<00:29,  3.29it/s]\u001b[A\nIteration:  98%|█████████▊| 5707/5804 [28:49<00:29,  3.29it/s]\u001b[A\nIteration:  98%|█████████▊| 5708/5804 [28:49<00:29,  3.29it/s]\u001b[A\nIteration:  98%|█████████▊| 5709/5804 [28:49<00:28,  3.29it/s]\u001b[A\nIteration:  98%|█████████▊| 5710/5804 [28:50<00:28,  3.29it/s]\u001b[A\nIteration:  98%|█████████▊| 5711/5804 [28:50<00:28,  3.29it/s]\u001b[A\nIteration:  98%|█████████▊| 5712/5804 [28:50<00:27,  3.29it/s]\u001b[A\nIteration:  98%|█████████▊| 5713/5804 [28:50<00:27,  3.29it/s]\u001b[A\nIteration:  98%|█████████▊| 5714/5804 [28:51<00:27,  3.29it/s]\u001b[A\nIteration:  98%|█████████▊| 5715/5804 [28:51<00:27,  3.29it/s]\u001b[A\nIteration:  98%|█████████▊| 5716/5804 [28:51<00:26,  3.29it/s]\u001b[A\nIteration:  99%|█████████▊| 5717/5804 [28:52<00:26,  3.29it/s]\u001b[A\nIteration:  99%|█████████▊| 5718/5804 [28:52<00:26,  3.30it/s]\u001b[A\nIteration:  99%|█████████▊| 5719/5804 [28:52<00:25,  3.30it/s]\u001b[A\nIteration:  99%|█████████▊| 5720/5804 [28:53<00:25,  3.30it/s]\u001b[A\nIteration:  99%|█████████▊| 5721/5804 [28:53<00:25,  3.30it/s]\u001b[A\nIteration:  99%|█████████▊| 5722/5804 [28:53<00:24,  3.30it/s]\u001b[A\nIteration:  99%|█████████▊| 5723/5804 [28:54<00:24,  3.31it/s]\u001b[A\nIteration:  99%|█████████▊| 5724/5804 [28:54<00:24,  3.31it/s]\u001b[A\nIteration:  99%|█████████▊| 5725/5804 [28:54<00:23,  3.31it/s]\u001b[A\nIteration:  99%|█████████▊| 5726/5804 [28:54<00:23,  3.31it/s]\u001b[A\nIteration:  99%|█████████▊| 5727/5804 [28:55<00:23,  3.31it/s]\u001b[A\nIteration:  99%|█████████▊| 5728/5804 [28:55<00:22,  3.30it/s]\u001b[A\nIteration:  99%|█████████▊| 5729/5804 [28:55<00:22,  3.30it/s]\u001b[A\nIteration:  99%|█████████▊| 5730/5804 [28:56<00:22,  3.31it/s]\u001b[A\nIteration:  99%|█████████▊| 5731/5804 [28:56<00:22,  3.30it/s]\u001b[A\nIteration:  99%|█████████▉| 5732/5804 [28:56<00:21,  3.31it/s]\u001b[A\nIteration:  99%|█████████▉| 5733/5804 [28:57<00:21,  3.31it/s]\u001b[A\nIteration:  99%|█████████▉| 5734/5804 [28:57<00:21,  3.31it/s]\u001b[A\nIteration:  99%|█████████▉| 5735/5804 [28:57<00:20,  3.31it/s]\u001b[A\nIteration:  99%|█████████▉| 5736/5804 [28:57<00:20,  3.31it/s]\u001b[A\nIteration:  99%|█████████▉| 5737/5804 [28:58<00:20,  3.31it/s]\u001b[A\nIteration:  99%|█████████▉| 5738/5804 [28:58<00:19,  3.31it/s]\u001b[A\nIteration:  99%|█████████▉| 5739/5804 [28:58<00:19,  3.31it/s]\u001b[A\nIteration:  99%|█████████▉| 5740/5804 [28:59<00:19,  3.30it/s]\u001b[A\nIteration:  99%|█████████▉| 5741/5804 [28:59<00:19,  3.30it/s]\u001b[A\nIteration:  99%|█████████▉| 5742/5804 [28:59<00:18,  3.30it/s]\u001b[A\nIteration:  99%|█████████▉| 5743/5804 [29:00<00:18,  3.30it/s]\u001b[A\nIteration:  99%|█████████▉| 5744/5804 [29:00<00:18,  3.30it/s]\u001b[A\nIteration:  99%|█████████▉| 5745/5804 [29:00<00:17,  3.30it/s]\u001b[A\nIteration:  99%|█████████▉| 5746/5804 [29:00<00:17,  3.30it/s]\u001b[A\nIteration:  99%|█████████▉| 5747/5804 [29:01<00:17,  3.30it/s]\u001b[A\nIteration:  99%|█████████▉| 5748/5804 [29:01<00:16,  3.30it/s]\u001b[A\nIteration:  99%|█████████▉| 5749/5804 [29:01<00:16,  3.30it/s]\u001b[A\nIteration:  99%|█████████▉| 5750/5804 [29:02<00:16,  3.29it/s]\u001b[A\nIteration:  99%|█████████▉| 5751/5804 [29:02<00:16,  3.29it/s]\u001b[A\nIteration:  99%|█████████▉| 5752/5804 [29:02<00:15,  3.29it/s]\u001b[A\nIteration:  99%|█████████▉| 5753/5804 [29:03<00:15,  3.29it/s]\u001b[A\nIteration:  99%|█████████▉| 5754/5804 [29:03<00:15,  3.29it/s]\u001b[A\nIteration:  99%|█████████▉| 5755/5804 [29:03<00:14,  3.29it/s]\u001b[A\nIteration:  99%|█████████▉| 5756/5804 [29:04<00:14,  3.29it/s]\u001b[A\nIteration:  99%|█████████▉| 5757/5804 [29:04<00:14,  3.29it/s]\u001b[A\nIteration:  99%|█████████▉| 5758/5804 [29:04<00:13,  3.29it/s]\u001b[A\nIteration:  99%|█████████▉| 5759/5804 [29:04<00:13,  3.29it/s]\u001b[A\nIteration:  99%|█████████▉| 5760/5804 [29:05<00:13,  3.29it/s]\u001b[A\nIteration:  99%|█████████▉| 5761/5804 [29:05<00:13,  3.29it/s]\u001b[A\nIteration:  99%|█████████▉| 5762/5804 [29:05<00:12,  3.29it/s]\u001b[A\nIteration:  99%|█████████▉| 5763/5804 [29:06<00:12,  3.29it/s]\u001b[A\nIteration:  99%|█████████▉| 5764/5804 [29:06<00:12,  3.29it/s]\u001b[A\nIteration:  99%|█████████▉| 5765/5804 [29:06<00:11,  3.29it/s]\u001b[A\nIteration:  99%|█████████▉| 5766/5804 [29:07<00:11,  3.30it/s]\u001b[A\nIteration:  99%|█████████▉| 5767/5804 [29:07<00:11,  3.30it/s]\u001b[A\nIteration:  99%|█████████▉| 5768/5804 [29:07<00:10,  3.30it/s]\u001b[A\nIteration:  99%|█████████▉| 5769/5804 [29:07<00:10,  3.30it/s]\u001b[A\nIteration:  99%|█████████▉| 5770/5804 [29:08<00:10,  3.30it/s]\u001b[A\nIteration:  99%|█████████▉| 5771/5804 [29:08<00:09,  3.30it/s]\u001b[A\nIteration:  99%|█████████▉| 5772/5804 [29:08<00:09,  3.30it/s]\u001b[A\nIteration:  99%|█████████▉| 5773/5804 [29:09<00:09,  3.30it/s]\u001b[A\nIteration:  99%|█████████▉| 5774/5804 [29:09<00:09,  3.31it/s]\u001b[A\nIteration: 100%|█████████▉| 5775/5804 [29:09<00:08,  3.30it/s]\u001b[A\nIteration: 100%|█████████▉| 5776/5804 [29:10<00:08,  3.31it/s]\u001b[A\nIteration: 100%|█████████▉| 5777/5804 [29:10<00:08,  3.30it/s]\u001b[A\nIteration: 100%|█████████▉| 5778/5804 [29:10<00:07,  3.30it/s]\u001b[A\nIteration: 100%|█████████▉| 5779/5804 [29:10<00:07,  3.31it/s]\u001b[A\nIteration: 100%|█████████▉| 5780/5804 [29:11<00:07,  3.30it/s]\u001b[A\nIteration: 100%|█████████▉| 5781/5804 [29:11<00:06,  3.30it/s]\u001b[A\nIteration: 100%|█████████▉| 5782/5804 [29:11<00:06,  3.30it/s]\u001b[A\nIteration: 100%|█████████▉| 5783/5804 [29:12<00:06,  3.30it/s]\u001b[A\nIteration: 100%|█████████▉| 5784/5804 [29:12<00:06,  3.30it/s]\u001b[A\nIteration: 100%|█████████▉| 5785/5804 [29:12<00:05,  3.30it/s]\u001b[A\nIteration: 100%|█████████▉| 5786/5804 [29:13<00:05,  3.30it/s]\u001b[A\nIteration: 100%|█████████▉| 5787/5804 [29:13<00:05,  3.30it/s]\u001b[A\nIteration: 100%|█████████▉| 5788/5804 [29:13<00:04,  3.31it/s]\u001b[A\nIteration: 100%|█████████▉| 5789/5804 [29:14<00:04,  3.31it/s]\u001b[A\nIteration: 100%|█████████▉| 5790/5804 [29:14<00:04,  3.31it/s]\u001b[A\nIteration: 100%|█████████▉| 5791/5804 [29:14<00:03,  3.31it/s]\u001b[A\nIteration: 100%|█████████▉| 5792/5804 [29:14<00:03,  3.30it/s]\u001b[A\nIteration: 100%|█████████▉| 5793/5804 [29:15<00:03,  3.30it/s]\u001b[A\nIteration: 100%|█████████▉| 5794/5804 [29:15<00:03,  3.30it/s]\u001b[A\nIteration: 100%|█████████▉| 5795/5804 [29:15<00:02,  3.30it/s]\u001b[A\nIteration: 100%|█████████▉| 5796/5804 [29:16<00:02,  3.30it/s]\u001b[A\nIteration: 100%|█████████▉| 5797/5804 [29:16<00:02,  3.31it/s]\u001b[A\nIteration: 100%|█████████▉| 5798/5804 [29:16<00:01,  3.31it/s]\u001b[A\nIteration: 100%|█████████▉| 5799/5804 [29:17<00:01,  3.31it/s]\u001b[A\nIteration: 100%|█████████▉| 5800/5804 [29:17<00:01,  3.31it/s]\u001b[A\nIteration: 100%|█████████▉| 5801/5804 [29:17<00:00,  3.31it/s]\u001b[A\nIteration: 100%|█████████▉| 5802/5804 [29:17<00:00,  3.32it/s]\u001b[A\nIteration: 100%|█████████▉| 5803/5804 [29:18<00:00,  3.32it/s]\u001b[A\nIteration: 100%|██████████| 5804/5804 [29:18<00:00,  3.30it/s]\nEpoch: 100%|██████████| 2/2 [58:34<00:00, 1757.27s/it]\n"
    }
   ],
   "source": [
    "global_step = 0\n",
    "model.train()\n",
    "# Q9. Pretrained BERT를 fine tuning하는 training 코드를 채워주세요.\n",
    "###################################################################################################\n",
    "# [2] TITAN Xp         | 61'C,  65 % |  6757 / 12196 MB | gyuhyeon(6745M)\n",
    "for _ in trange(int(num_train_epochs), desc=\"Epoch\"):\n",
    "    for step, batch in enumerate(tqdm(train_dataloader, desc=\"Iteration\")):\n",
    "        # loss.backward()까지의 코드를 채워주세요.\n",
    "        # blank:: batch to device#\n",
    "        batch = [b.to(device) for b in batch]\n",
    "        input_ids, input_mask, segment_ids, start_positions, end_positions = batch\n",
    "        loss = model(input_ids, segment_ids, input_mask, start_positions, end_positions)\n",
    "        loss.backward()\n",
    "        \n",
    "        \"\"\"\n",
    "        Added Explaination for the Below Code\n",
    "        f your data set is highly differentiated, you can suffer from a sort of \"early over-fitting\". \n",
    "        If your shuffled data happens to include a cluster of related, strongly-featured observations,\n",
    "        your model's initial training can skew badly toward those features -- or worse,\n",
    "        toward incidental features that aren't truly related to the topic at all.\n",
    "\n",
    "        Warm-up is a way to reduce the primacy effect of the early training examples.\n",
    "        Without it, you may need to run a few extra epochs to get the convergence desired,\n",
    "        as the model un-trains those early superstitions.\n",
    "        \"\"\"\n",
    "        lr_this_step = learning_rate * warmup_linear(global_step/t_total, warmup_proportion)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr_this_step\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        global_step += 1\n",
    "        \n",
    "###################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'\\ntorch.save({\"model_type\": self.model_type,\\n            \"start_epoch\": epoch + 1,\\n            \"network\": self.net.state_dict(),\\n            \"optimizer\": self.optim.state_dict(),\\n            \"best_metric\": self.best_metric,\\n            }, str(save_path) + \"/%s.pth.tar\" % (filename))\\n\\n## how to load            \\nself.net.load_state_dict(ckpoint[\\'network\\'])\\nself.optim.load_state_dict(ckpoint[\\'optimizer\\'])\\nself.start_epoch = ckpoint[\\'start_epoch\\']\\nself.best_metric = ckpoint[\"best_metric\"]\\n'"
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n",
    "# output_model_file = os.path.join(\"/Model\", \"pytorch_model.bin\")\n",
    "torch.save(model_to_save.state_dict(), \"./save/models/squad_finetuned_bert_128_epochs_2.bin\")\n",
    "\n",
    "os.listdir('./save/models')\n",
    "\n",
    "\"\"\"\n",
    "torch.save({\"model_type\": self.model_type,\n",
    "            \"start_epoch\": epoch + 1,\n",
    "            \"network\": self.net.state_dict(),\n",
    "            \"optimizer\": self.optim.state_dict(),\n",
    "            \"best_metric\": self.best_metric,\n",
    "            }, str(save_path) + \"/%s.pth.tar\" % (filename))\n",
    "\n",
    "## how to load            \n",
    "self.net.load_state_dict(ckpoint['network'])\n",
    "self.optim.load_state_dict(ckpoint['optimizer'])\n",
    "self.start_epoch = ckpoint['start_epoch']\n",
    "self.best_metric = ckpoint[\"best_metric\"]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "08/12/2020 13:01:49 - INFO - pytorch_pretrained_bert.modeling -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased.tar.gz from cache at /home/com10/.pytorch_pretrained_bert/731c19ddf94e294e00ec1ba9a930c69cc2a0fd489b25d3d691373fae4c0986bd.4e367b0d0155d801930846bb6ed98f8a7c23e0ded37888b29caa37009a40c7b9\n08/12/2020 13:01:49 - INFO - pytorch_pretrained_bert.modeling -   extracting archive file /home/com10/.pytorch_pretrained_bert/731c19ddf94e294e00ec1ba9a930c69cc2a0fd489b25d3d691373fae4c0986bd.4e367b0d0155d801930846bb6ed98f8a7c23e0ded37888b29caa37009a40c7b9 to temp dir /tmp/tmpfwqs94vv\n08/12/2020 13:01:53 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n  \"attention_probs_dropout_prob\": 0.1,\n  \"directionality\": \"bidi\",\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"max_position_embeddings\": 512,\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pooler_fc_size\": 768,\n  \"pooler_num_attention_heads\": 12,\n  \"pooler_num_fc_layers\": 3,\n  \"pooler_size_per_head\": 128,\n  \"pooler_type\": \"first_token_transform\",\n  \"type_vocab_size\": 2,\n  \"vocab_size\": 119547\n}\n\n08/12/2020 13:01:56 - INFO - pytorch_pretrained_bert.modeling -   Weights of BertForQuestionAnswering not initialized from pretrained model: ['qa_outputs.weight', 'qa_outputs.bias']\n08/12/2020 13:01:56 - INFO - pytorch_pretrained_bert.modeling -   Weights from pretrained model not used in BertForQuestionAnswering: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n08/12/2020 13:01:56 - WARNING - pytorch_pretrained_bert.tokenization -   The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n08/12/2020 13:01:57 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-vocab.txt from cache at /home/com10/.pytorch_pretrained_bert/96435fa287fbf7e469185f1062386e05a075cadbf6838b74da22bf64b080bc32.99bcd55fc66f4f3360bc49ba472b940b8dcf223ea6a345deb969d607ca900729\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "BertForQuestionAnswering(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): BertLayerNorm()\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (1): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (2): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (3): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (4): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (5): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (6): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (7): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (8): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (9): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (10): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (11): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n)"
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "gpu_num =0\n",
    "device = torch.device(f\"cuda:{gpu_num}\")\n",
    "\n",
    "config = 'bert-base-multilingual-cased'\n",
    "model = BertForQuestionAnswering.from_pretrained(config)\n",
    "tokenizer = BertTokenizer.from_pretrained(config)\n",
    "\n",
    "# 10. fine tuned BERT 모델 파라미터를 로드하는 코드를 채워주세요.\n",
    "###################################################################################################\n",
    "saving_point = torch.load(\"./save/models/squad_finetuned_bert_128_epochs_2.bin\")\n",
    "model.load_state_dict(saving_point)\n",
    "model.to(device)\n",
    "###################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "8 - INFO - __main__ -   tokens: [CLS] What city did Super Bowl 50 take place in ? [SEP] Super Bowl 50 was an American football game to determine the champion of the National Football League ( NFL ) for the 2015 season . The American Football Conference ( AFC ) champion Denver Broncos defeated the National Football Conference ( NFC ) champion Carolina Panthers 24 [UNK] 10 to earn their third Super Bowl title . The game was played on February 7 , 2016 , at Levi ' s Stadium in the San Francisco Bay Area at Santa Clara , California . As this was the 50th Super Bowl , the league em ##pha ##sized the \" golden anniversary \" with various gold - theme ##d initiatives , as well as temporarily sus [SEP]\n08/12/2020 13:01:58 - INFO - __main__ -   token_to_orig_map: 12:0 13:1 14:2 15:3 16:4 17:5 18:6 19:7 20:8 21:9 22:10 23:11 24:12 25:13 26:14 27:15 28:16 29:17 30:17 31:17 32:18 33:19 34:20 35:21 36:21 37:22 38:23 39:24 40:25 41:26 42:26 43:26 44:27 45:28 46:29 47:30 48:31 49:32 50:33 51:34 52:35 53:35 54:35 55:36 56:37 57:38 58:39 59:39 60:39 61:40 62:41 63:42 64:43 65:44 66:45 67:46 68:46 69:47 70:48 71:49 72:50 73:51 74:52 75:53 76:53 77:54 78:54 79:55 80:56 81:56 82:56 83:57 84:58 85:59 86:60 87:61 88:62 89:63 90:64 91:65 92:66 93:66 94:67 95:67 96:68 97:69 98:70 99:71 100:72 101:73 102:74 103:74 104:75 105:76 106:77 107:77 108:77 109:78 110:79 111:79 112:80 113:80 114:81 115:82 116:83 117:83 118:83 119:83 120:84 121:84 122:85 123:86 124:87 125:88 126:89\n08/12/2020 13:01:58 - INFO - __main__ -   token_is_max_context: 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True\n08/12/2020 13:01:58 - INFO - __main__ -   input_ids: 101 12489 11584 12172 12786 23010 10462 13574 11192 10106 136 102 12786 23010 10462 10134 10151 10536 12485 11661 10114 37284 10105 17689 10108 10105 10655 12499 11074 113 20179 114 10142 10105 10222 11226 119 10117 10536 12499 16424 113 26523 114 17689 29357 87078 18058 10105 10655 12499 16424 113 80893 114 17689 14329 69440 10233 100 10150 10114 65065 10455 12628 12786 23010 12887 119 10117 11661 10134 11553 10135 11508 128 117 10255 117 10160 33875 112 187 16632 10106 10105 10469 11798 13146 17815 10160 11154 23086 117 11621 119 10882 10531 10134 10105 98105 12786 23010 117 10105 15616 10266 37590 109461 10105 107 52477 37157 107 10169 13547 18128 118 26648 10162 82311 117 10146 11206 10146 69175 10846 102\n08/12/2020 13:01:58 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n08/12/2020 13:01:58 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n08/12/2020 13:01:58 - INFO - __main__ -   *** Example ***\n08/12/2020 13:01:58 - INFO - __main__ -   unique_id: 1000000035\n08/12/2020 13:01:58 - INFO - __main__ -   example_index: 17\n08/12/2020 13:01:58 - INFO - __main__ -   doc_span_index: 1\n08/12/2020 13:01:58 - INFO - __main__ -   tokens: [CLS] What city did Super Bowl 50 take place in ? [SEP] ##pend ##ing the tradition of naming each Super Bowl game with Roman numera ##ls ( under which the game would have been known as \" Super Bowl L \" ) , so that the logo could prominent ##ly feature the Arabic numera ##ls 50 . [SEP]\n08/12/2020 13:01:58 - INFO - __main__ -   token_to_orig_map: 12:89 13:89 14:90 15:91 16:92 17:93 18:94 19:95 20:96 21:97 22:98 23:99 24:100 25:100 26:101 27:101 28:102 29:103 30:104 31:105 32:106 33:107 34:108 35:109 36:110 37:110 38:111 39:112 40:112 41:112 42:112 43:113 44:114 45:115 46:116 47:117 48:118 49:118 50:119 51:120 52:121 53:122 54:122 55:123 56:123\n08/12/2020 13:01:58 - INFO - __main__ -   token_is_max_context: 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True\n08/12/2020 13:01:58 - INFO - __main__ -   input_ids: 101 12489 11584 12172 12786 23010 10462 13574 11192 10106 136 102 63592 10230 10105 20049 10108 90742 11948 12786 23010 11661 10169 12359 76274 11747 113 10571 10319 10105 11661 10894 10529 10590 11053 10146 107 12786 23010 149 107 114 117 10380 10189 10105 18655 12174 22861 10454 19072 10105 26571 76274 11747 10462 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n08/12/2020 13:01:58 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n08/12/2020 13:01:58 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n08/12/2020 13:01:58 - INFO - __main__ -   *** Example ***\n08/12/2020 13:01:58 - INFO - __main__ -   unique_id: 1000000036\n08/12/2020 13:01:58 - INFO - __main__ -   example_index: 18\n08/12/2020 13:01:58 - INFO - __main__ -   doc_span_index: 0\n08/12/2020 13:01:58 - INFO - __main__ -   tokens: [CLS] What stadium did Super Bowl 50 take place in ? [SEP] Super Bowl 50 was an American football game to determine the champion of the National Football League ( NFL ) for the 2015 season . The American Football Conference ( AFC ) champion Denver Broncos defeated the National Football Conference ( NFC ) champion Carolina Panthers 24 [UNK] 10 to earn their third Super Bowl title . The game was played on February 7 , 2016 , at Levi ' s Stadium in the San Francisco Bay Area at Santa Clara , California . As this was the 50th Super Bowl , the league em ##pha ##sized the \" golden anniversary \" with various gold - theme ##d initiatives , as well as temporarily sus [SEP]\n08/12/2020 13:01:58 - INFO - __main__ -   token_to_orig_map: 12:0 13:1 14:2 15:3 16:4 17:5 18:6 19:7 20:8 21:9 22:10 23:11 24:12 25:13 26:14 27:15 28:16 29:17 30:17 31:17 32:18 33:19 34:20 35:21 36:21 37:22 38:23 39:24 40:25 41:26 42:26 43:26 44:27 45:28 46:29 47:30 48:31 49:32 50:33 51:34 52:35 53:35 54:35 55:36 56:37 57:38 58:39 59:39 60:39 61:40 62:41 63:42 64:43 65:44 66:45 67:46 68:46 69:47 70:48 71:49 72:50 73:51 74:52 75:53 76:53 77:54 78:54 79:55 80:56 81:56 82:56 83:57 84:58 85:59 86:60 87:61 88:62 89:63 90:64 91:65 92:66 93:66 94:67 95:67 96:68 97:69 98:70 99:71 100:72 101:73 102:74 103:74 104:75 105:76 106:77 107:77 108:77 109:78 110:79 111:79 112:80 113:80 114:81 115:82 116:83 117:83 118:83 119:83 120:84 121:84 122:85 123:86 124:87 125:88 126:89\n08/12/2020 13:01:58 - INFO - __main__ -   token_is_max_context: 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True\n08/12/2020 13:01:58 - INFO - __main__ -   input_ids: 101 12489 27915 12172 12786 23010 10462 13574 11192 10106 136 102 12786 23010 10462 10134 10151 10536 12485 11661 10114 37284 10105 17689 10108 10105 10655 12499 11074 113 20179 114 10142 10105 10222 11226 119 10117 10536 12499 16424 113 26523 114 17689 29357 87078 18058 10105 10655 12499 16424 113 80893 114 17689 14329 69440 10233 100 10150 10114 65065 10455 12628 12786 23010 12887 119 10117 11661 10134 11553 10135 11508 128 117 10255 117 10160 33875 112 187 16632 10106 10105 10469 11798 13146 17815 10160 11154 23086 117 11621 119 10882 10531 10134 10105 98105 12786 23010 117 10105 15616 10266 37590 109461 10105 107 52477 37157 107 10169 13547 18128 118 26648 10162 82311 117 10146 11206 10146 69175 10846 102\n08/12/2020 13:01:58 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n08/12/2020 13:01:58 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n08/12/2020 13:01:58 - INFO - __main__ -   *** Example ***\n08/12/2020 13:01:58 - INFO - __main__ -   unique_id: 1000000037\n08/12/2020 13:01:58 - INFO - __main__ -   example_index: 18\n08/12/2020 13:01:58 - INFO - __main__ -   doc_span_index: 1\n08/12/2020 13:01:58 - INFO - __main__ -   tokens: [CLS] What stadium did Super Bowl 50 take place in ? [SEP] ##pend ##ing the tradition of naming each Super Bowl game with Roman numera ##ls ( under which the game would have been known as \" Super Bowl L \" ) , so that the logo could prominent ##ly feature the Arabic numera ##ls 50 . [SEP]\n08/12/2020 13:01:58 - INFO - __main__ -   token_to_orig_map: 12:89 13:89 14:90 15:91 16:92 17:93 18:94 19:95 20:96 21:97 22:98 23:99 24:100 25:100 26:101 27:101 28:102 29:103 30:104 31:105 32:106 33:107 34:108 35:109 36:110 37:110 38:111 39:112 40:112 41:112 42:112 43:113 44:114 45:115 46:116 47:117 48:118 49:118 50:119 51:120 52:121 53:122 54:122 55:123 56:123\n08/12/2020 13:01:58 - INFO - __main__ -   token_is_max_context: 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True\n08/12/2020 13:01:58 - INFO - __main__ -   input_ids: 101 12489 27915 12172 12786 23010 10462 13574 11192 10106 136 102 63592 10230 10105 20049 10108 90742 11948 12786 23010 11661 10169 12359 76274 11747 113 10571 10319 10105 11661 10894 10529 10590 11053 10146 107 12786 23010 149 107 114 117 10380 10189 10105 18655 12174 22861 10454 19072 10105 26571 76274 11747 10462 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n08/12/2020 13:01:58 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n08/12/2020 13:01:58 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n08/12/2020 13:01:58 - INFO - __main__ -   *** Example ***\n08/12/2020 13:01:58 - INFO - __main__ -   unique_id: 1000000038\n08/12/2020 13:01:58 - INFO - __main__ -   example_index: 19\n08/12/2020 13:01:58 - INFO - __main__ -   doc_span_index: 0\n08/12/2020 13:01:58 - INFO - __main__ -   tokens: [CLS] What was the final score of Super Bowl 50 ? [SEP] Super Bowl 50 was an American football game to determine the champion of the National Football League ( NFL ) for the 2015 season . The American Football Conference ( AFC ) champion Denver Broncos defeated the National Football Conference ( NFC ) champion Carolina Panthers 24 [UNK] 10 to earn their third Super Bowl title . The game was played on February 7 , 2016 , at Levi ' s Stadium in the San Francisco Bay Area at Santa Clara , California . As this was the 50th Super Bowl , the league em ##pha ##sized the \" golden anniversary \" with various gold - theme ##d initiatives , as well as temporarily sus [SEP]\n08/12/2020 13:01:58 - INFO - __main__ -   token_to_orig_map: 12:0 13:1 14:2 15:3 16:4 17:5 18:6 19:7 20:8 21:9 22:10 23:11 24:12 25:13 26:14 27:15 28:16 29:17 30:17 31:17 32:18 33:19 34:20 35:21 36:21 37:22 38:23 39:24 40:25 41:26 42:26 43:26 44:27 45:28 46:29 47:30 48:31 49:32 50:33 51:34 52:35 53:35 54:35 55:36 56:37 57:38 58:39 59:39 60:39 61:40 62:41 63:42 64:43 65:44 66:45 67:46 68:46 69:47 70:48 71:49 72:50 73:51 74:52 75:53 76:53 77:54 78:54 79:55 80:56 81:56 82:56 83:57 84:58 85:59 86:60 87:61 88:62 89:63 90:64 91:65 92:66 93:66 94:67 95:67 96:68 97:69 98:70 99:71 100:72 101:73 102:74 103:74 104:75 105:76 106:77 107:77 108:77 109:78 110:79 111:79 112:80 113:80 114:81 115:82 116:83 117:83 118:83 119:83 120:84 121:84 122:85 123:86 124:87 125:88 126:89\n08/12/2020 13:01:58 - INFO - __main__ -   token_is_max_context: 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True\n08/12/2020 13:01:58 - INFO - __main__ -   input_ids: 101 12489 10134 10105 11070 17704 10108 12786 23010 10462 136 102 12786 23010 10462 10134 10151 10536 12485 11661 10114 37284 10105 17689 10108 10105 10655 12499 11074 113 20179 114 10142 10105 10222 11226 119 10117 10536 12499 16424 113 26523 114 17689 29357 87078 18058 10105 10655 12499 16424 113 80893 114 17689 14329 69440 10233 100 10150 10114 65065 10455 12628 12786 23010 12887 119 10117 11661 10134 11553 10135 11508 128 117 10255 117 10160 33875 112 187 16632 10106 10105 10469 11798 13146 17815 10160 11154 23086 117 11621 119 10882 10531 10134 10105 98105 12786 23010 117 10105 15616 10266 37590 109461 10105 107 52477 37157 107 10169 13547 18128 118 26648 10162 82311 117 10146 11206 10146 69175 10846 102\n08/12/2020 13:01:58 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n08/12/2020 13:01:58 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n08/12/2020 13:01:58 - INFO - __main__ -   *** Example ***\n08/12/2020 13:01:58 - INFO - __main__ -   unique_id: 1000000039\n08/12/2020 13:01:58 - INFO - __main__ -   example_index: 19\n08/12/2020 13:01:58 - INFO - __main__ -   doc_span_index: 1\n08/12/2020 13:01:58 - INFO - __main__ -   tokens: [CLS] What was the final score of Super Bowl 50 ? [SEP] ##pend ##ing the tradition of naming each Super Bowl game with Roman numera ##ls ( under which the game would have been known as \" Super Bowl L \" ) , so that the logo could prominent ##ly feature the Arabic numera ##ls 50 . [SEP]\n08/12/2020 13:01:58 - INFO - __main__ -   token_to_orig_map: 12:89 13:89 14:90 15:91 16:92 17:93 18:94 19:95 20:96 21:97 22:98 23:99 24:100 25:100 26:101 27:101 28:102 29:103 30:104 31:105 32:106 33:107 34:108 35:109 36:110 37:110 38:111 39:112 40:112 41:112 42:112 43:113 44:114 45:115 46:116 47:117 48:118 49:118 50:119 51:120 52:121 53:122 54:122 55:123 56:123\n08/12/2020 13:01:58 - INFO - __main__ -   token_is_max_context: 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True\n08/12/2020 13:01:58 - INFO - __main__ -   input_ids: 101 12489 10134 10105 11070 17704 10108 12786 23010 10462 136 102 63592 10230 10105 20049 10108 90742 11948 12786 23010 11661 10169 12359 76274 11747 113 10571 10319 10105 11661 10894 10529 10590 11053 10146 107 12786 23010 149 107 114 117 10380 10189 10105 18655 12174 22861 10454 19072 10105 26571 76274 11747 10462 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n08/12/2020 13:01:58 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n08/12/2020 13:01:58 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\nsuccess to convert input data into a set of 10570 examples\n08/12/2020 13:02:16 - INFO - __main__ -   ***** Running predictions *****\n08/12/2020 13:02:16 - INFO - __main__ -     Num orig examples = 10570\n08/12/2020 13:02:16 - INFO - __main__ -     Num split examples = 21730\n08/12/2020 13:02:16 - INFO - __main__ -     Batch size = 50\n"
    }
   ],
   "source": [
    "max_seq_length=128\n",
    "doc_stride=128\n",
    "max_query_length=64\n",
    "predict_batch_size=50\n",
    "\n",
    "# 11. dev data를 로드하고 전처리 해주세요\n",
    "###################################################################################################\n",
    "input_data = read_json(dev_file)\n",
    "eval_examples = parse_json_squad(\n",
    "    input_data=input_data, is_train=False)\n",
    "eval_features = convert_examples_to_features(\n",
    "    examples=eval_examples,\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_length=max_seq_length,\n",
    "    doc_stride=doc_stride,\n",
    "    max_query_length=max_query_length,\n",
    "    is_training=False)\n",
    "###################################################################################################\n",
    "\n",
    "logger.info(\"***** Running predictions *****\")\n",
    "logger.info(\"  Num orig examples = %d\", len(eval_examples))\n",
    "logger.info(\"  Num split examples = %d\", len(eval_features))\n",
    "logger.info(\"  Batch size = %d\",predict_batch_size)\n",
    "\n",
    "all_input_ids = torch.tensor([f.input_ids for f in eval_features], dtype=torch.long)\n",
    "all_input_mask = torch.tensor([f.input_mask for f in eval_features], dtype=torch.long)\n",
    "all_segment_ids = torch.tensor([f.segment_ids for f in eval_features], dtype=torch.long)\n",
    "all_example_index = torch.arange(all_input_ids.size(0), dtype=torch.long)\n",
    "eval_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_example_index)\n",
    "\n",
    "# 12. inference를 위한 데이터로더를 만들어주세요.\n",
    "###################################################################################################\n",
    "# Run prediction for full data\n",
    "eval_sampler = SequentialSampler(eval_data)\n",
    "eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=predict_batch_size)\n",
    "###################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "08/12/2020 13:02:16 - INFO - __main__ -   Start evaluating\nEvaluating:   0%|          | 0/435 [00:00<?, ?it/s]08/12/2020 13:02:16 - INFO - __main__ -   Processing example: 0\nEvaluating:   5%|▍         | 20/435 [00:04<01:32,  4.47it/s]08/12/2020 13:02:21 - INFO - __main__ -   Processing example: 1000\nEvaluating:   9%|▉         | 40/435 [00:09<01:28,  4.48it/s]08/12/2020 13:02:25 - INFO - __main__ -   Processing example: 2000\nEvaluating:  14%|█▍        | 60/435 [00:13<01:23,  4.48it/s]08/12/2020 13:02:30 - INFO - __main__ -   Processing example: 3000\nEvaluating:  18%|█▊        | 80/435 [00:18<01:19,  4.47it/s]08/12/2020 13:02:34 - INFO - __main__ -   Processing example: 4000\nEvaluating:  23%|██▎       | 100/435 [00:22<01:14,  4.47it/s]08/12/2020 13:02:39 - INFO - __main__ -   Processing example: 5000\nEvaluating:  28%|██▊       | 120/435 [00:27<01:10,  4.47it/s]08/12/2020 13:02:43 - INFO - __main__ -   Processing example: 6000\nEvaluating:  32%|███▏      | 140/435 [00:31<01:06,  4.46it/s]08/12/2020 13:02:48 - INFO - __main__ -   Processing example: 7000\nEvaluating:  37%|███▋      | 160/435 [00:35<01:01,  4.47it/s]08/12/2020 13:02:52 - INFO - __main__ -   Processing example: 8000\nEvaluating:  41%|████▏     | 180/435 [00:40<00:57,  4.45it/s]08/12/2020 13:02:57 - INFO - __main__ -   Processing example: 9000\nEvaluating:  46%|████▌     | 200/435 [00:44<00:52,  4.45it/s]08/12/2020 13:03:01 - INFO - __main__ -   Processing example: 10000\nEvaluating:  51%|█████     | 220/435 [00:49<00:48,  4.45it/s]08/12/2020 13:03:06 - INFO - __main__ -   Processing example: 11000\nEvaluating:  55%|█████▌    | 240/435 [00:53<00:43,  4.45it/s]08/12/2020 13:03:10 - INFO - __main__ -   Processing example: 12000\nEvaluating:  60%|█████▉    | 260/435 [00:58<00:39,  4.45it/s]08/12/2020 13:03:15 - INFO - __main__ -   Processing example: 13000\nEvaluating:  64%|██████▍   | 280/435 [01:02<00:34,  4.45it/s]08/12/2020 13:03:19 - INFO - __main__ -   Processing example: 14000\nEvaluating:  69%|██████▉   | 300/435 [01:07<00:30,  4.43it/s]08/12/2020 13:03:24 - INFO - __main__ -   Processing example: 15000\nEvaluating:  74%|███████▎  | 320/435 [01:12<00:26,  4.42it/s]08/12/2020 13:03:28 - INFO - __main__ -   Processing example: 16000\nEvaluating:  78%|███████▊  | 340/435 [01:16<00:21,  4.43it/s]08/12/2020 13:03:33 - INFO - __main__ -   Processing example: 17000\nEvaluating:  83%|████████▎ | 360/435 [01:21<00:16,  4.43it/s]08/12/2020 13:03:37 - INFO - __main__ -   Processing example: 18000\nEvaluating:  87%|████████▋ | 380/435 [01:25<00:12,  4.43it/s]08/12/2020 13:03:42 - INFO - __main__ -   Processing example: 19000\nEvaluating:  92%|█████████▏| 400/435 [01:30<00:07,  4.43it/s]08/12/2020 13:03:46 - INFO - __main__ -   Processing example: 20000\nEvaluating:  97%|█████████▋| 420/435 [01:34<00:03,  4.43it/s]08/12/2020 13:03:51 - INFO - __main__ -   Processing example: 21000\nEvaluating: 100%|██████████| 435/435 [01:38<00:00,  4.42it/s]\n"
    }
   ],
   "source": [
    "model.eval()\n",
    "all_results = []\n",
    "logger.info(\"Start evaluating\")\n",
    "for input_ids, input_mask, segment_ids, example_indices in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "    if len(all_results) % 1000 == 0:\n",
    "        logger.info(\"Processing example: %d\" % (len(all_results)))\n",
    "    input_ids = input_ids.to(device)\n",
    "    input_mask = input_mask.to(device)\n",
    "    segment_ids = segment_ids.to(device)\n",
    "    with torch.no_grad():\n",
    "        batch_start_logits, batch_end_logits = model(input_ids, segment_ids, input_mask)\n",
    "    for i, example_index in enumerate(example_indices):\n",
    "        start_logits = batch_start_logits[i].detach().cpu().tolist()\n",
    "        end_logits = batch_end_logits[i].detach().cpu().tolist()\n",
    "        eval_feature = eval_features[example_index.item()]\n",
    "        unique_id = int(eval_feature.unique_id)\n",
    "        all_results.append(RawResult(unique_id=unique_id,\n",
    "                                     start_logits=start_logits,\n",
    "                                     end_logits=end_logits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![nn](images/image_1.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 377.304688 248.518125\" width=\"377.304688pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 248.518125 \nL 377.304688 248.518125 \nL 377.304688 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 35.304688 224.64 \nL 370.104688 224.64 \nL 370.104688 7.2 \nL 35.304688 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m9d48f4e6e5\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.522869\" xlink:href=\"#m9d48f4e6e5\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <defs>\n       <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n      </defs>\n      <g transform=\"translate(47.341619 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"98.454151\" xlink:href=\"#m9d48f4e6e5\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 20 -->\n      <defs>\n       <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n      </defs>\n      <g transform=\"translate(92.091651 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"146.385432\" xlink:href=\"#m9d48f4e6e5\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 40 -->\n      <defs>\n       <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n      </defs>\n      <g transform=\"translate(140.022932 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"194.316713\" xlink:href=\"#m9d48f4e6e5\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 60 -->\n      <defs>\n       <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n      </defs>\n      <g transform=\"translate(187.954213 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"242.247995\" xlink:href=\"#m9d48f4e6e5\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 80 -->\n      <defs>\n       <path d=\"M 31.78125 34.625 \nQ 24.75 34.625 20.71875 30.859375 \nQ 16.703125 27.09375 16.703125 20.515625 \nQ 16.703125 13.921875 20.71875 10.15625 \nQ 24.75 6.390625 31.78125 6.390625 \nQ 38.8125 6.390625 42.859375 10.171875 \nQ 46.921875 13.96875 46.921875 20.515625 \nQ 46.921875 27.09375 42.890625 30.859375 \nQ 38.875 34.625 31.78125 34.625 \nz\nM 21.921875 38.8125 \nQ 15.578125 40.375 12.03125 44.71875 \nQ 8.5 49.078125 8.5 55.328125 \nQ 8.5 64.0625 14.71875 69.140625 \nQ 20.953125 74.21875 31.78125 74.21875 \nQ 42.671875 74.21875 48.875 69.140625 \nQ 55.078125 64.0625 55.078125 55.328125 \nQ 55.078125 49.078125 51.53125 44.71875 \nQ 48 40.375 41.703125 38.8125 \nQ 48.828125 37.15625 52.796875 32.3125 \nQ 56.78125 27.484375 56.78125 20.515625 \nQ 56.78125 9.90625 50.3125 4.234375 \nQ 43.84375 -1.421875 31.78125 -1.421875 \nQ 19.734375 -1.421875 13.25 4.234375 \nQ 6.78125 9.90625 6.78125 20.515625 \nQ 6.78125 27.484375 10.78125 32.3125 \nQ 14.796875 37.15625 21.921875 38.8125 \nz\nM 18.3125 54.390625 \nQ 18.3125 48.734375 21.84375 45.5625 \nQ 25.390625 42.390625 31.78125 42.390625 \nQ 38.140625 42.390625 41.71875 45.5625 \nQ 45.3125 48.734375 45.3125 54.390625 \nQ 45.3125 60.0625 41.71875 63.234375 \nQ 38.140625 66.40625 31.78125 66.40625 \nQ 25.390625 66.40625 21.84375 63.234375 \nQ 18.3125 60.0625 18.3125 54.390625 \nz\n\" id=\"DejaVuSans-56\"/>\n      </defs>\n      <g transform=\"translate(235.885495 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-56\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"290.179276\" xlink:href=\"#m9d48f4e6e5\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 100 -->\n      <defs>\n       <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n      </defs>\n      <g transform=\"translate(280.635526 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"338.110557\" xlink:href=\"#m9d48f4e6e5\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 120 -->\n      <g transform=\"translate(328.566807 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_8\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m86d01633cc\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"35.304688\" xlink:href=\"#m86d01633cc\" y=\"216.532157\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- −10 -->\n      <defs>\n       <path d=\"M 10.59375 35.5 \nL 73.1875 35.5 \nL 73.1875 27.203125 \nL 10.59375 27.203125 \nz\n\" id=\"DejaVuSans-8722\"/>\n      </defs>\n      <g transform=\"translate(7.2 220.331375)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-8722\"/>\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"147.412109\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"35.304688\" xlink:href=\"#m86d01633cc\" y=\"191.016181\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- −8 -->\n      <g transform=\"translate(13.5625 194.815399)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-8722\"/>\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-56\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"35.304688\" xlink:href=\"#m86d01633cc\" y=\"165.500205\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- −6 -->\n      <g transform=\"translate(13.5625 169.299423)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-8722\"/>\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-54\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"35.304688\" xlink:href=\"#m86d01633cc\" y=\"139.984229\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- −4 -->\n      <g transform=\"translate(13.5625 143.783448)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-8722\"/>\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"35.304688\" xlink:href=\"#m86d01633cc\" y=\"114.468253\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- −2 -->\n      <g transform=\"translate(13.5625 118.267472)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-8722\"/>\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"35.304688\" xlink:href=\"#m86d01633cc\" y=\"88.952277\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 0 -->\n      <g transform=\"translate(21.942188 92.751496)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"35.304688\" xlink:href=\"#m86d01633cc\" y=\"63.436301\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 2 -->\n      <g transform=\"translate(21.942188 67.23552)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_15\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"35.304688\" xlink:href=\"#m86d01633cc\" y=\"37.920325\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- 4 -->\n      <g transform=\"translate(21.942188 41.719544)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_9\">\n     <g id=\"line2d_16\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"35.304688\" xlink:href=\"#m86d01633cc\" y=\"12.404349\"/>\n      </g>\n     </g>\n     <g id=\"text_16\">\n      <!-- 6 -->\n      <g transform=\"translate(21.942188 16.203568)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-54\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_17\">\n    <path clip-path=\"url(#p88fa3979ea)\" d=\"M 50.522869 198.181891 \nL 52.919433 204.803603 \nL 55.315997 214.756364 \nL 57.712562 209.674222 \nL 60.109126 198.102878 \nL 62.50569 205.352163 \nL 67.298818 193.514886 \nL 69.695382 198.633468 \nL 72.091946 200.570193 \nL 74.48851 195.358376 \nL 76.885074 173.899286 \nL 79.281638 133.200209 \nL 81.678202 179.250555 \nL 84.074766 158.659255 \nL 86.47133 173.294278 \nL 88.867894 149.675132 \nL 91.264458 151.542268 \nL 93.661023 181.994431 \nL 96.057587 190.685835 \nL 98.454151 174.511266 \nL 100.850715 168.797677 \nL 103.247279 182.190726 \nL 105.643843 159.478658 \nL 108.040407 198.792465 \nL 110.436971 154.065934 \nL 112.833535 124.796093 \nL 115.230099 161.514228 \nL 117.626663 165.806648 \nL 120.023227 163.992553 \nL 122.419791 146.513181 \nL 124.816355 169.739169 \nL 127.212919 174.582327 \nL 129.609483 168.56791 \nL 132.006048 142.864204 \nL 134.402612 179.425291 \nL 136.799176 158.780286 \nL 139.19574 52.919535 \nL 141.592304 96.707546 \nL 143.988868 165.253453 \nL 146.385432 163.057218 \nL 148.781996 155.81642 \nL 151.17856 134.658791 \nL 153.575124 159.041134 \nL 155.971688 127.56781 \nL 158.368252 17.083636 \nL 160.764816 62.451358 \nL 163.16138 142.980569 \nL 165.557944 92.334345 \nL 167.954509 112.18757 \nL 170.351073 169.116555 \nL 172.747637 171.467365 \nL 175.144201 165.260011 \nL 177.540765 145.629013 \nL 179.937329 173.095512 \nL 182.333893 149.88082 \nL 184.730457 26.4773 \nL 187.127021 90.481663 \nL 189.523585 139.884794 \nL 191.920149 172.014416 \nL 194.316713 143.017569 \nL 196.713277 162.968959 \nL 199.109841 171.433079 \nL 201.506405 172.801874 \nL 203.90297 147.541283 \nL 206.299534 166.453018 \nL 208.696098 189.46746 \nL 213.489226 161.879517 \nL 215.88579 158.045566 \nL 218.282354 189.844027 \nL 220.678918 194.46077 \nL 223.075482 187.002585 \nL 225.472046 189.558098 \nL 227.86861 157.596288 \nL 230.265174 187.453091 \nL 232.661738 199.514928 \nL 235.058302 145.074035 \nL 237.454866 199.629954 \nL 239.851431 186.19645 \nL 242.247995 153.524449 \nL 244.644559 194.755332 \nL 247.041123 199.336292 \nL 249.437687 187.344598 \nL 251.834251 192.904518 \nL 254.230815 188.029221 \nL 256.627379 179.478107 \nL 259.023943 193.209422 \nL 261.420507 191.589683 \nL 263.817071 198.937313 \nL 266.213635 193.832432 \nL 268.610199 175.379854 \nL 271.006763 194.243553 \nL 273.403327 194.805217 \nL 275.799892 182.549348 \nL 278.196456 177.039349 \nL 280.59302 186.722489 \nL 282.989584 191.753231 \nL 285.386148 196.299722 \nL 287.782712 180.743934 \nL 290.179276 157.569053 \nL 292.57584 190.919861 \nL 294.972404 197.938503 \nL 297.368968 200.811646 \nL 299.765532 179.281307 \nL 302.162096 189.95005 \nL 304.55866 192.172517 \nL 306.955224 193.152894 \nL 309.351788 196.126379 \nL 311.748352 191.067343 \nL 314.144917 185.482602 \nL 316.541481 182.468504 \nL 318.938045 198.329695 \nL 321.334609 200.892204 \nL 323.731173 194.389423 \nL 326.127737 190.755978 \nL 328.524301 190.107089 \nL 330.920865 195.544056 \nL 333.317429 191.710939 \nL 335.713993 195.543046 \nL 338.110557 197.227258 \nL 340.507121 201.077677 \nL 342.903685 193.593436 \nL 345.300249 196.279075 \nL 347.696813 196.548622 \nL 350.093378 188.980994 \nL 352.489942 187.870138 \nL 354.886506 153.224412 \nL 354.886506 153.224412 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_18\">\n    <path clip-path=\"url(#p88fa3979ea)\" d=\"M 50.522869 197.39745 \nL 52.919433 193.705056 \nL 55.315997 180.210741 \nL 57.712562 187.117818 \nL 60.109126 198.441704 \nL 62.50569 192.014918 \nL 64.902254 194.681637 \nL 67.298818 202.035339 \nL 69.695382 197.6751 \nL 72.091946 196.012728 \nL 74.48851 189.874688 \nL 76.885074 146.10445 \nL 79.281638 180.91926 \nL 81.678202 166.097876 \nL 84.074766 133.750359 \nL 86.47133 182.584309 \nL 88.867894 186.610808 \nL 91.264458 171.655649 \nL 93.661023 163.91999 \nL 98.454151 198.133576 \nL 100.850715 184.343032 \nL 103.247279 202.08217 \nL 105.643843 160.607838 \nL 108.040407 190.62508 \nL 110.436971 203.136425 \nL 112.833535 170.642238 \nL 115.230099 158.846949 \nL 117.626663 108.027863 \nL 120.023227 179.910473 \nL 122.419791 120.448943 \nL 124.816355 127.32987 \nL 127.212919 191.151368 \nL 129.609483 203.610365 \nL 132.006048 130.094287 \nL 134.402612 131.520237 \nL 136.799176 129.643248 \nL 139.19574 150.759804 \nL 141.592304 165.020875 \nL 143.988868 154.009139 \nL 146.385432 126.077439 \nL 148.781996 199.426632 \nL 151.17856 138.237731 \nL 153.575124 125.199592 \nL 155.971688 162.048942 \nL 158.368252 109.670771 \nL 160.764816 21.756457 \nL 163.16138 173.224731 \nL 165.557944 198.54662 \nL 167.954509 184.623335 \nL 170.351073 155.268097 \nL 172.747637 129.903804 \nL 175.144201 200.358378 \nL 177.540765 134.752899 \nL 179.937329 146.646914 \nL 182.333893 174.585247 \nL 184.730457 125.32267 \nL 187.127021 29.209173 \nL 189.523585 141.218537 \nL 191.920149 184.066519 \nL 194.316713 95.285738 \nL 196.713277 191.462757 \nL 199.109841 189.981538 \nL 201.506405 201.41452 \nL 203.90297 151.278549 \nL 206.299534 183.309521 \nL 208.696098 172.051726 \nL 211.092662 114.537018 \nL 213.489226 128.952621 \nL 215.88579 201.700662 \nL 218.282354 188.314549 \nL 220.678918 189.414126 \nL 223.075482 191.456832 \nL 225.472046 196.899651 \nL 227.86861 176.605511 \nL 230.265174 181.240973 \nL 232.661738 179.205835 \nL 235.058302 141.303925 \nL 237.454866 157.930132 \nL 239.851431 195.289328 \nL 242.247995 164.963185 \nL 244.644559 178.062718 \nL 247.041123 176.809381 \nL 249.437687 144.859392 \nL 251.834251 186.657676 \nL 254.230815 190.972361 \nL 256.627379 182.085409 \nL 259.023943 174.971841 \nL 261.420507 174.759296 \nL 263.817071 165.923427 \nL 266.213635 187.070951 \nL 268.610199 182.178249 \nL 271.006763 166.190796 \nL 273.403327 182.049675 \nL 275.799892 145.354916 \nL 278.196456 140.276899 \nL 280.59302 197.861352 \nL 282.989584 196.85456 \nL 285.386148 196.194745 \nL 287.782712 205.419836 \nL 290.179276 172.805768 \nL 292.57584 190.498774 \nL 294.972404 178.043853 \nL 297.368968 180.906144 \nL 299.765532 202.29194 \nL 302.162096 188.28887 \nL 304.55866 188.902322 \nL 306.955224 186.58165 \nL 309.351788 186.810365 \nL 311.748352 195.916876 \nL 314.144917 196.159534 \nL 316.541481 197.243878 \nL 318.938045 181.116821 \nL 321.334609 187.188897 \nL 323.731173 191.119125 \nL 326.127737 194.511129 \nL 328.524301 192.083795 \nL 330.920865 187.966452 \nL 333.317429 187.313962 \nL 335.713993 186.468491 \nL 338.110557 184.17278 \nL 340.507121 184.867179 \nL 342.903685 188.999634 \nL 345.300249 187.399003 \nL 347.696813 189.309709 \nL 350.093378 192.341273 \nL 352.489942 188.364245 \nL 354.886506 159.719862 \nL 354.886506 159.719862 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 35.304688 224.64 \nL 35.304688 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 370.104688 224.64 \nL 370.104688 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 35.304688 224.64 \nL 370.104688 224.64 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 35.304688 7.2 \nL 370.104688 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 280.871875 45.1125 \nL 363.104688 45.1125 \nQ 365.104688 45.1125 365.104688 43.1125 \nL 365.104688 14.2 \nQ 365.104688 12.2 363.104688 12.2 \nL 280.871875 12.2 \nQ 278.871875 12.2 278.871875 14.2 \nL 278.871875 43.1125 \nQ 278.871875 45.1125 280.871875 45.1125 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"line2d_19\">\n     <path d=\"M 282.871875 20.298437 \nL 302.871875 20.298437 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_20\"/>\n    <g id=\"text_17\">\n     <!-- start_logit -->\n     <defs>\n      <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n      <path d=\"M 18.3125 70.21875 \nL 18.3125 54.6875 \nL 36.8125 54.6875 \nL 36.8125 47.703125 \nL 18.3125 47.703125 \nL 18.3125 18.015625 \nQ 18.3125 11.328125 20.140625 9.421875 \nQ 21.96875 7.515625 27.59375 7.515625 \nL 36.8125 7.515625 \nL 36.8125 0 \nL 27.59375 0 \nQ 17.1875 0 13.234375 3.875 \nQ 9.28125 7.765625 9.28125 18.015625 \nL 9.28125 47.703125 \nL 2.6875 47.703125 \nL 2.6875 54.6875 \nL 9.28125 54.6875 \nL 9.28125 70.21875 \nz\n\" id=\"DejaVuSans-116\"/>\n      <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n      <path d=\"M 41.109375 46.296875 \nQ 39.59375 47.171875 37.8125 47.578125 \nQ 36.03125 48 33.890625 48 \nQ 26.265625 48 22.1875 43.046875 \nQ 18.109375 38.09375 18.109375 28.8125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 20.953125 51.171875 25.484375 53.578125 \nQ 30.03125 56 36.53125 56 \nQ 37.453125 56 38.578125 55.875 \nQ 39.703125 55.765625 41.0625 55.515625 \nz\n\" id=\"DejaVuSans-114\"/>\n      <path d=\"M 50.984375 -16.609375 \nL 50.984375 -23.578125 \nL -0.984375 -23.578125 \nL -0.984375 -16.609375 \nz\n\" id=\"DejaVuSans-95\"/>\n      <path d=\"M 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 0 \nL 9.421875 0 \nz\n\" id=\"DejaVuSans-108\"/>\n      <path d=\"M 30.609375 48.390625 \nQ 23.390625 48.390625 19.1875 42.75 \nQ 14.984375 37.109375 14.984375 27.296875 \nQ 14.984375 17.484375 19.15625 11.84375 \nQ 23.34375 6.203125 30.609375 6.203125 \nQ 37.796875 6.203125 41.984375 11.859375 \nQ 46.1875 17.53125 46.1875 27.296875 \nQ 46.1875 37.015625 41.984375 42.703125 \nQ 37.796875 48.390625 30.609375 48.390625 \nz\nM 30.609375 56 \nQ 42.328125 56 49.015625 48.375 \nQ 55.71875 40.765625 55.71875 27.296875 \nQ 55.71875 13.875 49.015625 6.21875 \nQ 42.328125 -1.421875 30.609375 -1.421875 \nQ 18.84375 -1.421875 12.171875 6.21875 \nQ 5.515625 13.875 5.515625 27.296875 \nQ 5.515625 40.765625 12.171875 48.375 \nQ 18.84375 56 30.609375 56 \nz\n\" id=\"DejaVuSans-111\"/>\n      <path d=\"M 45.40625 27.984375 \nQ 45.40625 37.75 41.375 43.109375 \nQ 37.359375 48.484375 30.078125 48.484375 \nQ 22.859375 48.484375 18.828125 43.109375 \nQ 14.796875 37.75 14.796875 27.984375 \nQ 14.796875 18.265625 18.828125 12.890625 \nQ 22.859375 7.515625 30.078125 7.515625 \nQ 37.359375 7.515625 41.375 12.890625 \nQ 45.40625 18.265625 45.40625 27.984375 \nz\nM 54.390625 6.78125 \nQ 54.390625 -7.171875 48.1875 -13.984375 \nQ 42 -20.796875 29.203125 -20.796875 \nQ 24.46875 -20.796875 20.265625 -20.09375 \nQ 16.0625 -19.390625 12.109375 -17.921875 \nL 12.109375 -9.1875 \nQ 16.0625 -11.328125 19.921875 -12.34375 \nQ 23.78125 -13.375 27.78125 -13.375 \nQ 36.625 -13.375 41.015625 -8.765625 \nQ 45.40625 -4.15625 45.40625 5.171875 \nL 45.40625 9.625 \nQ 42.625 4.78125 38.28125 2.390625 \nQ 33.9375 0 27.875 0 \nQ 17.828125 0 11.671875 7.65625 \nQ 5.515625 15.328125 5.515625 27.984375 \nQ 5.515625 40.671875 11.671875 48.328125 \nQ 17.828125 56 27.875 56 \nQ 33.9375 56 38.28125 53.609375 \nQ 42.625 51.21875 45.40625 46.390625 \nL 45.40625 54.6875 \nL 54.390625 54.6875 \nz\n\" id=\"DejaVuSans-103\"/>\n      <path d=\"M 9.421875 54.6875 \nL 18.40625 54.6875 \nL 18.40625 0 \nL 9.421875 0 \nz\nM 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 64.59375 \nL 9.421875 64.59375 \nz\n\" id=\"DejaVuSans-105\"/>\n     </defs>\n     <g transform=\"translate(310.871875 23.798437)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"52.099609\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"91.308594\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"152.587891\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"193.701172\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"232.910156\" xlink:href=\"#DejaVuSans-95\"/>\n      <use x=\"282.910156\" xlink:href=\"#DejaVuSans-108\"/>\n      <use x=\"310.693359\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"371.875\" xlink:href=\"#DejaVuSans-103\"/>\n      <use x=\"435.351562\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"463.134766\" xlink:href=\"#DejaVuSans-116\"/>\n     </g>\n    </g>\n    <g id=\"line2d_21\">\n     <path d=\"M 282.871875 35.254687 \nL 302.871875 35.254687 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_22\"/>\n    <g id=\"text_18\">\n     <!-- end_logit -->\n     <defs>\n      <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n      <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-110\"/>\n      <path d=\"M 45.40625 46.390625 \nL 45.40625 75.984375 \nL 54.390625 75.984375 \nL 54.390625 0 \nL 45.40625 0 \nL 45.40625 8.203125 \nQ 42.578125 3.328125 38.25 0.953125 \nQ 33.9375 -1.421875 27.875 -1.421875 \nQ 17.96875 -1.421875 11.734375 6.484375 \nQ 5.515625 14.40625 5.515625 27.296875 \nQ 5.515625 40.1875 11.734375 48.09375 \nQ 17.96875 56 27.875 56 \nQ 33.9375 56 38.25 53.625 \nQ 42.578125 51.265625 45.40625 46.390625 \nz\nM 14.796875 27.296875 \nQ 14.796875 17.390625 18.875 11.75 \nQ 22.953125 6.109375 30.078125 6.109375 \nQ 37.203125 6.109375 41.296875 11.75 \nQ 45.40625 17.390625 45.40625 27.296875 \nQ 45.40625 37.203125 41.296875 42.84375 \nQ 37.203125 48.484375 30.078125 48.484375 \nQ 22.953125 48.484375 18.875 42.84375 \nQ 14.796875 37.203125 14.796875 27.296875 \nz\n\" id=\"DejaVuSans-100\"/>\n     </defs>\n     <g transform=\"translate(310.871875 38.754687)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"61.523438\" xlink:href=\"#DejaVuSans-110\"/>\n      <use x=\"124.902344\" xlink:href=\"#DejaVuSans-100\"/>\n      <use x=\"188.378906\" xlink:href=\"#DejaVuSans-95\"/>\n      <use x=\"238.378906\" xlink:href=\"#DejaVuSans-108\"/>\n      <use x=\"266.162109\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"327.34375\" xlink:href=\"#DejaVuSans-103\"/>\n      <use x=\"390.820312\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"418.603516\" xlink:href=\"#DejaVuSans-116\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p88fa3979ea\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"35.304688\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 372.103125 248.518125\" width=\"372.103125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 248.518125 \nL 372.103125 248.518125 \nL 372.103125 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 30.103125 224.64 \nL 364.903125 224.64 \nL 364.903125 7.2 \nL 30.103125 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"mf637f24e40\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"45.321307\" xlink:href=\"#mf637f24e40\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <defs>\n       <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n      </defs>\n      <g transform=\"translate(42.140057 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"93.252588\" xlink:href=\"#mf637f24e40\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 20 -->\n      <defs>\n       <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n      </defs>\n      <g transform=\"translate(86.890088 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"141.183869\" xlink:href=\"#mf637f24e40\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 40 -->\n      <defs>\n       <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n      </defs>\n      <g transform=\"translate(134.821369 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"189.115151\" xlink:href=\"#mf637f24e40\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 60 -->\n      <defs>\n       <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n      </defs>\n      <g transform=\"translate(182.752651 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"237.046432\" xlink:href=\"#mf637f24e40\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 80 -->\n      <defs>\n       <path d=\"M 31.78125 34.625 \nQ 24.75 34.625 20.71875 30.859375 \nQ 16.703125 27.09375 16.703125 20.515625 \nQ 16.703125 13.921875 20.71875 10.15625 \nQ 24.75 6.390625 31.78125 6.390625 \nQ 38.8125 6.390625 42.859375 10.171875 \nQ 46.921875 13.96875 46.921875 20.515625 \nQ 46.921875 27.09375 42.890625 30.859375 \nQ 38.875 34.625 31.78125 34.625 \nz\nM 21.921875 38.8125 \nQ 15.578125 40.375 12.03125 44.71875 \nQ 8.5 49.078125 8.5 55.328125 \nQ 8.5 64.0625 14.71875 69.140625 \nQ 20.953125 74.21875 31.78125 74.21875 \nQ 42.671875 74.21875 48.875 69.140625 \nQ 55.078125 64.0625 55.078125 55.328125 \nQ 55.078125 49.078125 51.53125 44.71875 \nQ 48 40.375 41.703125 38.8125 \nQ 48.828125 37.15625 52.796875 32.3125 \nQ 56.78125 27.484375 56.78125 20.515625 \nQ 56.78125 9.90625 50.3125 4.234375 \nQ 43.84375 -1.421875 31.78125 -1.421875 \nQ 19.734375 -1.421875 13.25 4.234375 \nQ 6.78125 9.90625 6.78125 20.515625 \nQ 6.78125 27.484375 10.78125 32.3125 \nQ 14.796875 37.15625 21.921875 38.8125 \nz\nM 18.3125 54.390625 \nQ 18.3125 48.734375 21.84375 45.5625 \nQ 25.390625 42.390625 31.78125 42.390625 \nQ 38.140625 42.390625 41.71875 45.5625 \nQ 45.3125 48.734375 45.3125 54.390625 \nQ 45.3125 60.0625 41.71875 63.234375 \nQ 38.140625 66.40625 31.78125 66.40625 \nQ 25.390625 66.40625 21.84375 63.234375 \nQ 18.3125 60.0625 18.3125 54.390625 \nz\n\" id=\"DejaVuSans-56\"/>\n      </defs>\n      <g transform=\"translate(230.683932 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-56\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"284.977713\" xlink:href=\"#mf637f24e40\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 100 -->\n      <defs>\n       <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n      </defs>\n      <g transform=\"translate(275.433963 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"332.908995\" xlink:href=\"#mf637f24e40\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 120 -->\n      <g transform=\"translate(323.365245 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_8\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"mdaa7362d79\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#mdaa7362d79\" y=\"214.7564\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 0.0 -->\n      <defs>\n       <path d=\"M 10.6875 12.40625 \nL 21 12.40625 \nL 21 0 \nL 10.6875 0 \nz\n\" id=\"DejaVuSans-46\"/>\n      </defs>\n      <g transform=\"translate(7.2 218.555619)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#mdaa7362d79\" y=\"183.76838\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 0.1 -->\n      <g transform=\"translate(7.2 187.567599)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-49\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#mdaa7362d79\" y=\"152.78036\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 0.2 -->\n      <g transform=\"translate(7.2 156.579578)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#mdaa7362d79\" y=\"121.792339\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 0.3 -->\n      <defs>\n       <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n      </defs>\n      <g transform=\"translate(7.2 125.591558)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-51\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#mdaa7362d79\" y=\"90.804319\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 0.4 -->\n      <g transform=\"translate(7.2 94.603538)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#mdaa7362d79\" y=\"59.816298\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 0.5 -->\n      <defs>\n       <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n      </defs>\n      <g transform=\"translate(7.2 63.615517)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#mdaa7362d79\" y=\"28.828278\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 0.6 -->\n      <g transform=\"translate(7.2 32.627497)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-54\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_15\">\n    <path clip-path=\"url(#pad4760a599)\" d=\"M 45.321307 214.756266 \nL 131.597613 214.753451 \nL 133.994177 202.91814 \nL 136.390741 214.373839 \nL 138.787305 214.754625 \nL 150.770126 214.722345 \nL 153.16669 18.340253 \nL 155.563254 209.148307 \nL 157.959818 214.746226 \nL 160.356382 214.217425 \nL 162.752946 214.642704 \nL 165.14951 214.755089 \nL 177.13233 214.750476 \nL 179.528895 120.695828 \nL 181.925459 214.133188 \nL 184.322023 214.743431 \nL 208.287663 214.754087 \nL 349.684943 214.751842 \nL 349.684943 214.751842 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_16\">\n    <path clip-path=\"url(#pad4760a599)\" d=\"M 45.321307 214.756193 \nL 110.028537 214.752142 \nL 112.425101 214.527744 \nL 114.821665 214.755583 \nL 117.218229 214.670031 \nL 126.804485 214.715848 \nL 150.770126 214.753087 \nL 153.16669 214.555372 \nL 155.563254 17.083636 \nL 157.959818 214.75502 \nL 179.528895 214.697455 \nL 181.925459 104.53929 \nL 184.322023 214.739444 \nL 186.718587 214.75581 \nL 189.115151 214.13562 \nL 191.511715 214.75607 \nL 203.494535 214.754888 \nL 205.891099 214.619121 \nL 210.684227 214.756252 \nL 349.684943 214.752423 \nL 349.684943 214.752423 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 30.103125 224.64 \nL 30.103125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 364.903125 224.64 \nL 364.903125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 30.103125 224.64 \nL 364.903125 224.64 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 30.103125 7.2 \nL 364.903125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 275.670313 45.1125 \nL 357.903125 45.1125 \nQ 359.903125 45.1125 359.903125 43.1125 \nL 359.903125 14.2 \nQ 359.903125 12.2 357.903125 12.2 \nL 275.670313 12.2 \nQ 273.670313 12.2 273.670313 14.2 \nL 273.670313 43.1125 \nQ 273.670313 45.1125 275.670313 45.1125 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"line2d_17\">\n     <path d=\"M 277.670313 20.298437 \nL 297.670313 20.298437 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_18\"/>\n    <g id=\"text_15\">\n     <!-- start_logit -->\n     <defs>\n      <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n      <path d=\"M 18.3125 70.21875 \nL 18.3125 54.6875 \nL 36.8125 54.6875 \nL 36.8125 47.703125 \nL 18.3125 47.703125 \nL 18.3125 18.015625 \nQ 18.3125 11.328125 20.140625 9.421875 \nQ 21.96875 7.515625 27.59375 7.515625 \nL 36.8125 7.515625 \nL 36.8125 0 \nL 27.59375 0 \nQ 17.1875 0 13.234375 3.875 \nQ 9.28125 7.765625 9.28125 18.015625 \nL 9.28125 47.703125 \nL 2.6875 47.703125 \nL 2.6875 54.6875 \nL 9.28125 54.6875 \nL 9.28125 70.21875 \nz\n\" id=\"DejaVuSans-116\"/>\n      <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n      <path d=\"M 41.109375 46.296875 \nQ 39.59375 47.171875 37.8125 47.578125 \nQ 36.03125 48 33.890625 48 \nQ 26.265625 48 22.1875 43.046875 \nQ 18.109375 38.09375 18.109375 28.8125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 20.953125 51.171875 25.484375 53.578125 \nQ 30.03125 56 36.53125 56 \nQ 37.453125 56 38.578125 55.875 \nQ 39.703125 55.765625 41.0625 55.515625 \nz\n\" id=\"DejaVuSans-114\"/>\n      <path d=\"M 50.984375 -16.609375 \nL 50.984375 -23.578125 \nL -0.984375 -23.578125 \nL -0.984375 -16.609375 \nz\n\" id=\"DejaVuSans-95\"/>\n      <path d=\"M 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 0 \nL 9.421875 0 \nz\n\" id=\"DejaVuSans-108\"/>\n      <path d=\"M 30.609375 48.390625 \nQ 23.390625 48.390625 19.1875 42.75 \nQ 14.984375 37.109375 14.984375 27.296875 \nQ 14.984375 17.484375 19.15625 11.84375 \nQ 23.34375 6.203125 30.609375 6.203125 \nQ 37.796875 6.203125 41.984375 11.859375 \nQ 46.1875 17.53125 46.1875 27.296875 \nQ 46.1875 37.015625 41.984375 42.703125 \nQ 37.796875 48.390625 30.609375 48.390625 \nz\nM 30.609375 56 \nQ 42.328125 56 49.015625 48.375 \nQ 55.71875 40.765625 55.71875 27.296875 \nQ 55.71875 13.875 49.015625 6.21875 \nQ 42.328125 -1.421875 30.609375 -1.421875 \nQ 18.84375 -1.421875 12.171875 6.21875 \nQ 5.515625 13.875 5.515625 27.296875 \nQ 5.515625 40.765625 12.171875 48.375 \nQ 18.84375 56 30.609375 56 \nz\n\" id=\"DejaVuSans-111\"/>\n      <path d=\"M 45.40625 27.984375 \nQ 45.40625 37.75 41.375 43.109375 \nQ 37.359375 48.484375 30.078125 48.484375 \nQ 22.859375 48.484375 18.828125 43.109375 \nQ 14.796875 37.75 14.796875 27.984375 \nQ 14.796875 18.265625 18.828125 12.890625 \nQ 22.859375 7.515625 30.078125 7.515625 \nQ 37.359375 7.515625 41.375 12.890625 \nQ 45.40625 18.265625 45.40625 27.984375 \nz\nM 54.390625 6.78125 \nQ 54.390625 -7.171875 48.1875 -13.984375 \nQ 42 -20.796875 29.203125 -20.796875 \nQ 24.46875 -20.796875 20.265625 -20.09375 \nQ 16.0625 -19.390625 12.109375 -17.921875 \nL 12.109375 -9.1875 \nQ 16.0625 -11.328125 19.921875 -12.34375 \nQ 23.78125 -13.375 27.78125 -13.375 \nQ 36.625 -13.375 41.015625 -8.765625 \nQ 45.40625 -4.15625 45.40625 5.171875 \nL 45.40625 9.625 \nQ 42.625 4.78125 38.28125 2.390625 \nQ 33.9375 0 27.875 0 \nQ 17.828125 0 11.671875 7.65625 \nQ 5.515625 15.328125 5.515625 27.984375 \nQ 5.515625 40.671875 11.671875 48.328125 \nQ 17.828125 56 27.875 56 \nQ 33.9375 56 38.28125 53.609375 \nQ 42.625 51.21875 45.40625 46.390625 \nL 45.40625 54.6875 \nL 54.390625 54.6875 \nz\n\" id=\"DejaVuSans-103\"/>\n      <path d=\"M 9.421875 54.6875 \nL 18.40625 54.6875 \nL 18.40625 0 \nL 9.421875 0 \nz\nM 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 64.59375 \nL 9.421875 64.59375 \nz\n\" id=\"DejaVuSans-105\"/>\n     </defs>\n     <g transform=\"translate(305.670313 23.798437)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"52.099609\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"91.308594\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"152.587891\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"193.701172\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"232.910156\" xlink:href=\"#DejaVuSans-95\"/>\n      <use x=\"282.910156\" xlink:href=\"#DejaVuSans-108\"/>\n      <use x=\"310.693359\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"371.875\" xlink:href=\"#DejaVuSans-103\"/>\n      <use x=\"435.351562\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"463.134766\" xlink:href=\"#DejaVuSans-116\"/>\n     </g>\n    </g>\n    <g id=\"line2d_19\">\n     <path d=\"M 277.670313 35.254687 \nL 297.670313 35.254687 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_20\"/>\n    <g id=\"text_16\">\n     <!-- end_logit -->\n     <defs>\n      <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n      <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-110\"/>\n      <path d=\"M 45.40625 46.390625 \nL 45.40625 75.984375 \nL 54.390625 75.984375 \nL 54.390625 0 \nL 45.40625 0 \nL 45.40625 8.203125 \nQ 42.578125 3.328125 38.25 0.953125 \nQ 33.9375 -1.421875 27.875 -1.421875 \nQ 17.96875 -1.421875 11.734375 6.484375 \nQ 5.515625 14.40625 5.515625 27.296875 \nQ 5.515625 40.1875 11.734375 48.09375 \nQ 17.96875 56 27.875 56 \nQ 33.9375 56 38.25 53.625 \nQ 42.578125 51.265625 45.40625 46.390625 \nz\nM 14.796875 27.296875 \nQ 14.796875 17.390625 18.875 11.75 \nQ 22.953125 6.109375 30.078125 6.109375 \nQ 37.203125 6.109375 41.296875 11.75 \nQ 45.40625 17.390625 45.40625 27.296875 \nQ 45.40625 37.203125 41.296875 42.84375 \nQ 37.203125 48.484375 30.078125 48.484375 \nQ 22.953125 48.484375 18.875 42.84375 \nQ 14.796875 37.203125 14.796875 27.296875 \nz\n\" id=\"DejaVuSans-100\"/>\n     </defs>\n     <g transform=\"translate(305.670313 38.754687)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"61.523438\" xlink:href=\"#DejaVuSans-110\"/>\n      <use x=\"124.902344\" xlink:href=\"#DejaVuSans-100\"/>\n      <use x=\"188.378906\" xlink:href=\"#DejaVuSans-95\"/>\n      <use x=\"238.378906\" xlink:href=\"#DejaVuSans-108\"/>\n      <use x=\"266.162109\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"327.34375\" xlink:href=\"#DejaVuSans-103\"/>\n      <use x=\"390.820312\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"418.603516\" xlink:href=\"#DejaVuSans-116\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pad4760a599\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"30.103125\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "45 46\n"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def softmax(a):\n",
    "    exp_a = np.exp(a)\n",
    "    sum_exp_a = np.sum(exp_a)\n",
    "    y = exp_a / sum_exp_a\n",
    "    return y\n",
    "\n",
    "start_logit = all_results[6].start_logits\n",
    "end_logit = all_results[6].end_logits\n",
    "index = np.arange(len(start_logit))\n",
    "\n",
    "# 13. Infer 모드의 model output을 plot해주세요.\n",
    "###################################################################################################\n",
    "# berore softmax\n",
    "plt.plot(index, start_logit, label='start_logit')\n",
    "plt.plot(index, end_logit, label='end_logit')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "\n",
    "# after softmax\n",
    "plt.plot(index, softmax(start_logit), label='start_logit')\n",
    "plt.plot(index, softmax(end_logit), label='end_logit')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "\n",
    "###################################################################################################\n",
    "\n",
    "\n",
    "pred_start_index = np.argmax(start_logit)\n",
    "pred_end_index = np.argmax(end_logit)\n",
    "print(np.argmax(start_logit), np.argmax(end_logit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "qas_id: 56be4db0acb8001400a502ef\n\n, question_text: Which NFL team won Super Bowl 50?\n\n, orig_answer_text: None\n\n, doc_tokens: [Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.]\n\n\nAnswer: Denver Broncos\n"
    }
   ],
   "source": [
    "pred_start_index = np.argmax(start_logit)\n",
    "pred_end_index = np.argmax(end_logit)\n",
    "orig_doc_start = eval_features[6].token_to_orig_map[pred_start_index]\n",
    "orig_doc_end = eval_features[6].token_to_orig_map[pred_end_index]\n",
    "orig_tokens = eval_examples[eval_features[6].example_index].doc_tokens[orig_doc_start:(orig_doc_end + 1)]\n",
    "\n",
    "print(eval_examples[eval_features[6].example_index])\n",
    "print(\"\\n\\nAnswer: {}\".format(\" \".join(orig_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "08/12/2020 13:04:43 - INFO - bert_utils.bert_utils -   Writing predictions to: ./save/predictions/squad_test_predictions.json\n08/12/2020 13:04:43 - INFO - bert_utils.bert_utils -   Writing nbest to: ./save/predictions/nbest_squad_test_predictions.json\n"
    }
   ],
   "source": [
    "output_dir='./save/predictions'\n",
    "n_best_size=20\n",
    "max_answer_length=30\n",
    "do_lower_case=True\n",
    "verbose_logging=False\n",
    "\n",
    "output_prediction_file = os.path.join(output_dir, \"squad_test_predictions.json\")\n",
    "output_nbest_file = os.path.join(output_dir, \"nbest_squad_test_predictions.json\")\n",
    "all_predictions = write_predictions(eval_examples, eval_features, all_results,\n",
    "                  n_best_size, max_answer_length,\n",
    "                  do_lower_case, output_prediction_file,\n",
    "                  output_nbest_file, verbose_logging)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('./datasets/squad_v1.1/dev-v1.1.json') as f:\n",
    "    dataset_json = json.load(f)\n",
    "    dataset = dataset_json['data']\n",
    "with open('./save/predictions/squad_test_predictions.json') as f:\n",
    "    preds = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "OrderedDict([('exact', 76.80227057710502),\n             ('f1', 84.01673185840188),\n             ('total', 10570)])"
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "qid_to_has_ans = make_qid_to_has_ans(dataset)  # maps qid to True/False\n",
    "has_ans_qids = [k for k, v in qid_to_has_ans.items() if v]\n",
    "no_ans_qids = [k for k, v in qid_to_has_ans.items() if not v]\n",
    "exact, f1 = get_raw_scores(dataset, preds)\n",
    "\n",
    "out_eval = make_eval_dict(exact, f1)\n",
    "out_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KorQuAD v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('0811_qa': conda)",
   "language": "python",
   "name": "python_defaultSpec_1597203855447"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}