{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"4_StarGAN.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"AAk8pbp-DrM-"},"source":["# StarGAN 실습"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"W0tExsC2DrNA","colab":{}},"source":["# pyTorch 관련 된 라이브러리.\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim # optimization에 관한 모듈.\n","import torchvision # 이미지 관련 전처리, pretrained된 모델, 데이터 로딩에 관한 패키지입니다.\n","from torchvision.utils import save_image # 이미지 저장을 위한 torchvision의 모듈\n","import torchvision.datasets as vision_dsets\n","import torchvision.transforms as T # 이미지 전처리 모듈입니다.\n","from torchvision.datasets import ImageFolder\n","from torch.utils import data\n","\n","# 기타 필요한 라이브러리.\n","from PIL import Image\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import os\n","import time\n","import datetime\n","import random\n","os.environ['CUDA_VISIBLE_DEVICES'] = '0'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"S9pELYxsDrNI"},"source":["## StarGAN\n","\n","#### StarGAN은 multi-domain의 image translation이 가능하다는 것에 가장 큰 contribution을 갖는다.\n","\n","![구조](./imgs/4_stargan1.jpg)\n","\n","---\n","\n","#### 위 구조를 실제 dataset 상황에서 보자면 아래와 같다.\n","\n","\n","\n","![구조2](./imgs/4_stargan2.jpg)"]},{"cell_type":"markdown","metadata":{"id":"OAxU-RF3DyPg","colab_type":"text"},"source":["# Training에 사용될 Hyper-parameter를 지정합니다."]},{"cell_type":"code","metadata":{"id":"Tngb6EsLDyPi","colab_type":"code","colab":{}},"source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # tensor.to(device) 방식을 통해서 cpu -> gpu로 보낼 수 있습니다.\n","g_lr = 0.0001 # learning rate for Generator\n","d_lr = 0.0001 # learning rate for Discriminator\n","batch_size = 16 # mini-batch size"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"whsfuA3qDrNJ"},"source":["# Training에 사용될 데이터를 불러옵니다."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"VKACetcGDrNK"},"source":["!bash download.sh celeba"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"UxnuKEqcDrNK"},"source":["## Data Loader\n","\n","CelebA의 경우에는 pytorch에서 기본으로 제공하는 data loader가 없기에 조금 복잡한 data loader를 구현해줘야 합니다. dataloader의 경우는 각각의 dataset들마다 코딩해줘야하는 방향이 천차만별입니다."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"eNxOlytGDrNL","colab":{}},"source":["class CelebA(data.Dataset):\n","    \"\"\"Dataset class for the CelebA dataset.\"\"\"\n","\n","    def __init__(self, image_dir, attr_path, selected_attrs, transform, mode):\n","        \"\"\"Initialize and preprocess the CelebA dataset.\"\"\"\n","        self.image_dir = image_dir\n","        self.attr_path = attr_path\n","        self.selected_attrs = selected_attrs\n","        self.transform = transform\n","        self.mode = mode\n","        self.train_dataset = []\n","        self.test_dataset = []\n","        self.attr2idx = {}\n","        self.idx2attr = {}\n","        self.preprocess()\n","\n","        if mode == 'train':\n","            self.num_images = len(self.train_dataset)\n","        else:\n","            self.num_images = len(self.test_dataset)\n","\n","    def preprocess(self):\n","        \"\"\"Preprocess the CelebA attribute file.\"\"\"\n","        lines = [line.rstrip() for line in open(self.attr_path, 'r')]\n","        all_attr_names = lines[1].split()\n","        for i, attr_name in enumerate(all_attr_names):\n","            self.attr2idx[attr_name] = i\n","            self.idx2attr[i] = attr_name\n","\n","        lines = lines[2:]\n","        random.seed(1234)\n","        random.shuffle(lines)\n","        for i, line in enumerate(lines):\n","            split = line.split()\n","            filename = split[0]\n","            values = split[1:]\n","\n","            label = []\n","            for attr_name in self.selected_attrs:\n","                idx = self.attr2idx[attr_name]\n","                label.append(values[idx] == '1')\n","\n","            if (i+1) < 2000:\n","                self.test_dataset.append([filename, label])\n","            else:\n","                self.train_dataset.append([filename, label])\n","\n","        print('Finished preprocessing the CelebA dataset...')\n","\n","    def __getitem__(self, index):\n","        \"\"\"Return one image and its corresponding attribute label.\"\"\"\n","        dataset = self.train_dataset if self.mode == 'train' else self.test_dataset\n","        filename, label = dataset[index]\n","        image = Image.open(os.path.join(self.image_dir, filename))\n","        return self.transform(image), torch.FloatTensor(label)\n","\n","    def __len__(self):\n","        \"\"\"Return the number of images.\"\"\"\n","        return self.num_images\n","    \n","def get_loader(image_dir, attr_path, selected_attrs, crop_size=178, image_size=128, \n","               batch_size=16, dataset='CelebA', mode='train', num_workers=1):\n","    \"\"\"Build and return a data loader.\"\"\"\n","    transform = []\n","    if mode == 'train':\n","        transform.append(T.RandomHorizontalFlip())\n","    transform.append(T.CenterCrop(crop_size))\n","    transform.append(T.Resize(image_size))\n","    transform.append(T.ToTensor())\n","    transform.append(T.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)))\n","    transform = T.Compose(transform)\n","\n","    dataset = CelebA(image_dir, attr_path, selected_attrs, transform, mode)\n","\n","    data_loader = data.DataLoader(dataset=dataset,\n","                                  batch_size=batch_size,\n","                                  shuffle=(mode=='train'),\n","                                  num_workers=num_workers)\n","    return data_loader"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nFMXM5C5DyPs","colab_type":"text"},"source":["!bash download.sh celeba"]},{"cell_type":"code","metadata":{"id":"bvOQzi6aDyPs","colab_type":"code","colab":{},"outputId":"8649b073-89ed-4884-c8af-570f55d78d1e"},"source":["#Dataloader들을 불러옵니다.\n","celeba_image_dir = 'data/celeba/images'\n","attr_path = 'data/celeba/list_attr_celeba.txt'\n","selected_attrs = ['Black_Hair', 'Blond_Hair', 'Brown_Hair', 'Male', 'Young']\n","celeba_crop_size = 178\n","image_size = 128\n","num_workers = 1\n","\n","celeba_trainloader = get_loader(celeba_image_dir, attr_path, selected_attrs,\n","                           celeba_crop_size, image_size, batch_size,\n","                           'CelebA', mode='train', num_workers=num_workers)\n","\n","celeba_testloader = get_loader(celeba_image_dir, attr_path, selected_attrs,\n","                           celeba_crop_size, image_size, batch_size,\n","                           'CelebA', mode='test', num_workers=num_workers)\n"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'get_loader' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-d38064653e5d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mnum_workers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m celeba_trainloader = get_loader(celeba_image_dir, attr_path, selected_attrs,\n\u001b[0m\u001b[1;32m     10\u001b[0m                            \u001b[0mceleba_crop_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                            'CelebA', mode='train', num_workers=num_workers)\n","\u001b[0;31mNameError\u001b[0m: name 'get_loader' is not defined"]}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Tp97xEmmDrNN"},"source":["## 모델 구현\n","\n","### ResidualBlock\n","\n","Residual Block 이라는 개념은 2015년 Imagenet Challenge에서 ResNet이라는 모델이 우승을 차지하며 그 효과를 입증받은 모델이다.\n","\n","![resnet](./imgs/4_stargan3.png)"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"099rkpwuDrNO","colab":{}},"source":["'''\n","코드 단순화를 위한 함수들을 정의해 줍니다.\n","'''\n","class ResidualBlock(nn.Module):\n","    \"\"\"Residual Block with instance normalization.\"\"\"\n","    def __init__(self, dim_in, dim_out):\n","        super(ResidualBlock, self).__init__()\n","        self.main = nn.Sequential(\n","            nn.Conv2d(dim_in, dim_out, kernel_size=3, stride=1, padding=1, bias=False),\n","            nn.InstanceNorm2d(dim_out, affine=True, track_running_stats=True),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(dim_out, dim_out, kernel_size=3, stride=1, padding=1, bias=False),\n","            nn.InstanceNorm2d(dim_out, affine=True, track_running_stats=True))\n","\n","    def forward(self, x):\n","        return x + self.main(x)\n","\n","def conv(c_in, c_out, k_size, stride=2, pad=1, In=True, activation='relu'):\n","    \"\"\" \n","    코드 단순화를 위한 convolution block 생성을 위한 함수입니다.\n","    Conv -> Instancenorm -> Activation function 으로 이어지는 일련의 레이어를 생성합니다.\n","    \"\"\"\n","    layers = []\n","    \n","    # Conv.\n","    layers.append(nn.Conv2d(c_in, c_out, k_size, stride, pad, bias=False))\n","    \n","    # Instance Norm\n","    if In:\n","        layers.append(nn.InstanceNorm2d(c_out, affine=True, track_running_stats=True))\n","    \n","    # Activation\n","    if activation == 'lrelu':\n","        layers.append(nn.LeakyReLU(0.01))\n","    if activation == 'tanh':\n","        layers.append(nn.Tanh())\n","    if activation == 'relu':\n","        layers.append(nn.ReLU(inplace=True))\n","    if activation == 'none':\n","        pass\n","                \n","    return nn.Sequential(*layers)\n","    \n","def deconv(c_in, c_out, k_size, stride=2, pad=1, In=True, activation='relu'):\n","    \"\"\" \n","    코드 단순화를 위한 deconvolution block 생성을 위한 함수입니다.\n","    Deconv -> Activation function 으로 이어지는 일련의 레이어를 생성합니다.\n","    \"\"\"\n","    \n","    layers = []\n","    \n","    # Deconv.\n","    layers.append(nn.ConvTranspose2d(c_in, c_out, k_size, stride, pad, bias=False))\n","    \n","    if In:\n","        layers.append(nn.InstanceNorm2d(c_out, affine=True, track_running_stats=True))\n","    \n","    # Activation\n","    if activation == 'lrelu':\n","        layers.append(nn.LeakyReLU(0.01))\n","    if activation == 'tanh':\n","        layers.append(nn.Tanh())\n","    if activation == 'relu':\n","        layers.append(nn.ReLU(inplace=True))\n","    if activation == 'none':\n","        pass\n","                \n","    return nn.Sequential(*layers)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sDKm0oP0DyP4","colab_type":"code","colab":{}},"source":["'''\n","Generator와 Discriminator를 선언해 줍니다. \n","이 때 StarGAN의 Generator에는 input image와 target attribute에 대한 입력값을 받는 것을 기억합니다!!! \n","'''\n","\n","class Generator(nn.Module):\n","    \"\"\"Generator network.\"\"\"\n","    def __init__(self, conv_dim=64, c_dim=5):\n","        super(Generator, self).__init__()\n","\n","        layers = []\n","        self.conv1 = conv(3+c_dim, conv_dim, 7, 1, 3)\n","\n","        # Down-sampling layers.\n","        self.conv2 = conv(conv_dim, conv_dim * 2, 4, 2, 1)\n","        self.conv3 = conv(conv_dim * 2, conv_dim * 4, 4, 2, 1)\n","\n","        # Bottleneck layers.\n","        self.res1 = ResidualBlock(dim_in=conv_dim * 4, dim_out=conv_dim * 4)\n","        self.res2 = ResidualBlock(dim_in=conv_dim * 4, dim_out=conv_dim * 4)\n","        self.res3 = ResidualBlock(dim_in=conv_dim * 4, dim_out=conv_dim * 4)\n","        self.res4 = ResidualBlock(dim_in=conv_dim * 4, dim_out=conv_dim * 4)\n","        self.res5 = ResidualBlock(dim_in=conv_dim * 4, dim_out=conv_dim * 4)\n","        self.res6 = ResidualBlock(dim_in=conv_dim * 4, dim_out=conv_dim * 4)\n","\n","        # Up-sampling layers.\n","        self.deconv1 = deconv(conv_dim * 4, conv_dim * 2, 4, 2, 1)\n","        self.deconv2 = deconv(conv_dim * 2, conv_dim, 4, 2, 1)\n","        self.conv4 = conv(conv_dim, 3, 7, 1, 3, In=False, activation=None)\n","        self.tanh = nn.Tanh()\n","\n","    def forward(self, x, c):\n","        # target domain인 c를 입력 이미지 x의 사이즈와 동일하게 만들어준뒤, concatenate하여 모델에 입력으로 줍니다.\n","        c = c.view(c.size(0), c.size(1), 1, 1)\n","        c = c.repeat(1, 1, x.size(2), x.size(3))\n","        x = torch.cat([x, c], dim=1)\n","        \n","        out = self.conv1(x)\n","        out = self.conv2(out)\n","        out = self.conv3(out)\n","        \n","        out = self.res1(out)\n","        out = self.res2(out)\n","        out = self.res3(out)\n","        out = self.res4(out)\n","        out = self.res5(out)\n","        out = self.res6(out)\n","        \n","        out = self.deconv1(out)\n","        out = self.deconv2(out)\n","        out = self.conv4(out)\n","        out = self.tanh(out)\n","        \n","        return out\n","\n","class Discriminator(nn.Module):\n","    \"\"\"Discriminator network with PatchGAN.\"\"\"\n","    def __init__(self, image_size=128, conv_dim=64, c_dim=5):\n","        super(Discriminator, self).__init__()\n","        \n","        self.conv1 = conv(3, conv_dim, 4, 2, 1, In=False, activation='lrelu')\n","        \n","        self.conv2 = conv(conv_dim, conv_dim * 2, 4, 2, 1, In=False, activation='lrelu')\n","        self.conv3 = conv(conv_dim * 2, conv_dim * 4, 4, 2, 1, In=False, activation='lrelu')\n","        self.conv4 = conv(conv_dim * 4, conv_dim * 8, 4, 2, 1, In=False, activation='lrelu')\n","        self.conv5 = conv(conv_dim * 8, conv_dim * 16, 4, 2, 1, In=False, activation='lrelu')\n","        self.conv6 = conv(conv_dim * 16, conv_dim * 32, 4, 2, 1, In=False, activation='lrelu')\n","\n","        kernel_size = int(image_size / np.power(2, 6))\n","        self.gen = nn.Conv2d(conv_dim * 32, 1, kernel_size=3, stride=1, padding=1, bias=False)\n","        self.cls = nn.Conv2d(conv_dim * 32, c_dim, kernel_size=kernel_size, bias=False)\n","        \n","    def forward(self, x):\n","        out = self.conv1(x)\n","        out = self.conv2(out)\n","        out = self.conv3(out)\n","        out = self.conv4(out)\n","        out = self.conv5(out)\n","        out = self.conv6(out)\n","        \n","        out_gen = self.gen(out)\n","        out_cls = self.cls(out)\n","\n","        return out_gen, out_cls.view(out_cls.size(0), out_cls.size(1))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Nvuhw35nDyP8","colab_type":"text"},"source":["## 학습을 위한 사전 모델/optimizer 선언 & 기타준비"]},{"cell_type":"code","metadata":{"id":"N9_WhrvHDyP9","colab_type":"code","colab":{}},"source":["# 모델 선언 및 train모드로 만들어 줍니다.\n","Gen = Generator(64, 5).train()\n","Dis = Discriminator(128, 64, 5).train()\n","\n","# 선언한 모델들을 GPU에서 사용할 수 있도록 해 줍니다.\n","Gen.to(device)\n","Dis.to(device)\n","\n","# Optimizer 선언\n","g_optimizer = torch.optim.Adam(Gen.parameters(), lr=g_lr, betas=(0.5, 0.999))\n","d_optimizer = torch.optim.Adam(Dis.parameters(), lr=d_lr, betas=(0.5, 0.999))\n","\n","# Iteration 수 선언\n","num_iters = 20000\n","\n","# trainig 과정에서 생성되는 이미지가 어떻게 변화하는지 볼 수 있도록 고정된 샘플 데이터를 지정합니다.\n","celeba_iter = iter(celeba_trainloader)\n","x_fixed, c_org = next(celeba_iter)\n","x_fixed = x_fixed.to(device)\n","sample_step = 10000\n","sample_dir = 'stargan_celeba/samples' #샘플 데이터에 대한 모델 출력 결과가 저장되는 디렉토리입니다.\n","if not os.path.exists(sample_dir):\n","    os.makedirs(sample_dir)\n","    \n","#training 과정에서 모델의 파라미터를 저장합니다.\n","model_save_dir = 'stargan_celeba/models'\n","if not os.path.exists(model_save_dir):\n","    os.makedirs(model_save_dir)\n","\n","def create_labels(c_org, c_dim=5, selected_attrs=None):\n","        \"\"\"Generate target domain labels for debugging and testing.\"\"\"\n","        # Get hair color indices.\n","        hair_color_indices = []\n","        for i, attr_name in enumerate(selected_attrs):\n","            if attr_name in ['Black_Hair', 'Blond_Hair', 'Brown_Hair', 'Gray_Hair']:\n","                hair_color_indices.append(i)\n","\n","        c_trg_list = []\n","        for i in range(c_dim):\n","            c_trg = c_org.clone()\n","            if i in hair_color_indices:  # Set one hair color to 1 and the rest to 0.\n","                c_trg[:, i] = 1\n","                for j in hair_color_indices:\n","                    if j != i:\n","                        c_trg[:, j] = 0\n","            else:\n","                c_trg[:, i] = (c_trg[:, i] == 0)  # Reverse attribute value.\n","\n","            c_trg_list.append(c_trg.to(device))\n","        return c_trg_list\n","    \n","c_fixed_list = create_labels(c_org, 5, selected_attrs=selected_attrs)\n","\n","#training 과정에서 사용되는 gradient penalty loss의 함수입니다.\n","def gradient_penalty(y, x):\n","        \"\"\"Compute gradient penalty: (L2_norm(dy/dx) - 1)**2.\"\"\"\n","        weight = torch.ones(y.size()).to(device)\n","        dydx = torch.autograd.grad(outputs=y,\n","                                   inputs=x,\n","                                   grad_outputs=weight,\n","                                   retain_graph=True,\n","                                   create_graph=True,\n","                                   only_inputs=True)[0]\n","\n","        dydx = dydx.view(dydx.size(0), -1)\n","        dydx_l2norm = torch.sqrt(torch.sum(dydx**2, dim=1))\n","        return torch.mean((dydx_l2norm-1)**2)\n","\n","# 이미지 저장 과정에서 사용되는 스케일링 함수입니다.\n","def denorm(x):\n","    \"\"\"Convert the range from [-1, 1] to [0, 1].\"\"\"\n","    out = (x + 1) / 2\n","    return out.clamp_(0, 1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"s4_ZwpptDyQD","colab_type":"text"},"source":["## Training StarGAN \n","\n","여기서의 loss는 크게 7가지로 나누어 집니다.\n","\n","- D: Real images들을 1로 분류하기 위한 loss (d_loss_real)\n","- D: Fake images들을 0로 분류하기 위한 loss (d_loss_fake)\n","- D: Images의 domain을 분류하기 위한 loss (d_loss_cls)\n","- D: gradient penalty loss (d_loss_gp)\n","\n","- G: D를 속이는 Fake images들을 만들기 위한 loss (D에서 1로 분류함) (g_loss_fake)\n","- G: 다시 돌아 갔을 때 reconstruction을 위한 loss (g_loss_rec)\n","- G: Images의 domain을 분류하기 위한 loss (g_loss_cls)"]},{"cell_type":"code","metadata":{"id":"pCj9rkjwDyQD","colab_type":"code","colab":{}},"source":["start_time = time.time()\n","for step in range(num_iters):\n","    try:\n","        x_real, label_org = next(celeba_iter)\n","    except:\n","        celeba_iter = iter(celeba_trainloader)\n","        x_real, label_org = next(celeba_iter)\n","        \n","    # 변환하고자 하는 domain의 label을 랜덤생성합니다.\n","    rand_idx = torch.randperm(label_org.size(0))\n","    label_trg = label_org[rand_idx]\n","\n","    c_org = label_org.clone()\n","    c_trg = label_trg.clone()\n","\n","    x_real = x_real.to(device)           # Input images.\n","    c_org = c_org.to(device)             # Original domain labels.\n","    c_trg = c_trg.to(device)             # Target domain labels.\n","    label_org = label_org.to(device)     # Labels for computing classification loss.\n","    label_trg = label_trg.to(device)     # Labels for computing classification loss.\n","\n","    # =================================================================================== #\n","    #                             1. Train the discriminator                              #\n","    # =================================================================================== #\n","\n","    # Compute loss with real images.\n","    out_src, out_cls = Dis(x_real)\n","    d_loss_real = - torch.mean(out_src)\n","    d_loss_cls = F.binary_cross_entropy_with_logits(out_cls, label_org, size_average=False) / out_cls.size(0)\n","    \n","    # Compute loss with fake images.\n","    x_fake = Gen(x_real, c_trg)\n","    out_src, out_cls = Dis(x_fake.detach())\n","    d_loss_fake = torch.mean(out_src)\n","\n","    # Compute loss for gradient penalty.\n","    alpha = torch.rand(x_real.size(0), 1, 1, 1).to(device)\n","    x_hat = (alpha * x_real.data + (1 - alpha) * x_fake.data).requires_grad_(True)\n","    out_src, _ = Dis(x_hat)\n","    d_loss_gp = gradient_penalty(out_src, x_hat)\n","\n","    # Backward and optimize.\n","    d_loss = d_loss_real + d_loss_fake + 1.0 * d_loss_cls + 10.0 * d_loss_gp\n","    Dis.zero_grad()\n","    d_loss.backward()\n","    d_optimizer.step()\n","\n","    # Logging.\n","    loss = {}\n","    loss['D/loss_real'] = d_loss_real.item()\n","    loss['D/loss_fake'] = d_loss_fake.item()\n","    loss['D/loss_cls'] = d_loss_cls.item()\n","    loss['D/loss_gp'] = d_loss_gp.item()\n","\n","    # =================================================================================== #\n","    #                               2. Train the generator                                #\n","    # =================================================================================== #\n","\n","    if (step +1) % 5 == 0:\n","        # Original-to-target domain.\n","        x_fake = Gen(x_real, c_trg)\n","        out_src, out_cls = Dis(x_fake)\n","        g_loss_fake = - torch.mean(out_src)\n","        g_loss_cls = F.binary_cross_entropy_with_logits(out_cls, label_trg, size_average=False) / out_cls.size(0)\n","\n","        # Target-to-original domain.\n","        x_reconst = Gen(x_fake, c_org)\n","        g_loss_rec = torch.mean(torch.abs(x_real - x_reconst))\n","\n","        # Backward and optimize.\n","        g_loss = g_loss_fake + 10.0 * g_loss_rec + 1.0 * g_loss_cls\n","        Gen.zero_grad()\n","        g_loss.backward()\n","        g_optimizer.step()\n","\n","        # Logging.\n","        loss['G/loss_fake'] = g_loss_fake.item()\n","        loss['G/loss_rec'] = g_loss_rec.item()\n","        loss['G/loss_cls'] = g_loss_cls.item()\n","\n","    if (step + 1) % 10 == 0: #10 iteration마다 학습 중의 Loss들을 출력합니다.\n","            et = time.time() - start_time\n","            et = str(datetime.timedelta(seconds=et))[:-7]\n","            log = \"Elapsed [{}], Iteration [{}/{}]\".format(et, step+1, num_iters)\n","            for tag, value in loss.items():\n","                log += \", {}: {:.4f}\".format(tag, value)\n","            print(log)\n","\n","    if (step + 1) % 1000 == 0: #1000 iteration마다 특정 이미지의 translation 결과를 저장합니다.\n","        with torch.no_grad():\n","            x_fake_list = [x_fixed]\n","            for c_fixed in c_fixed_list:\n","                x_fake_list.append(Gen(x_fixed, c_fixed))\n","            x_concat = torch.cat(x_fake_list, dim=3)\n","            sample_path = os.path.join(sample_dir, '{}-images.jpg'.format(step+1))\n","            save_image(denorm(x_concat.data.cpu()), sample_path, nrow=1, padding=0)\n","            print('Saved real and fake images into {}...'.format(sample_path))\n","            \n","    if (step + 1) % 10000 == 0: #10000 iteration마다 모델을 저장합니다.\n","        G_path = os.path.join(model_save_dir, '{}-G.ckpt'.format(step+1))\n","        D_path = os.path.join(model_save_dir, '{}-D.ckpt'.format(step+1))\n","        torch.save(Gen.state_dict(), G_path)\n","        torch.save(Dis.state_dict(), D_path)\n","        print('Saved model checkpoints into {}...'.format(model_save_dir))\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9gXnsC4JDyQG","colab_type":"text"},"source":["## Testing StarGAN "]},{"cell_type":"code","metadata":{"scrolled":false,"id":"tFR8QcRyDyQH","colab_type":"code","colab":{}},"source":["resume_iters = 30\n","\n","# resume_iters만큼 학습하여 저장한 모델의 파라미터를 Load합니다.\n","print('Loading the trained models from step {}...'.format(resume_iters))\n","Gen = Generator(64, 5)\n","\n","Gen.to(device)\n","\n","G_path = os.path.join(model_save_dir, '{}-G.ckpt'.format(resume_iters))\n","Gen.load_state_dict(torch.load(G_path, map_location=lambda storage, loc: storage))\n","\n","#결과를 저장할 디렉토리 입니다.\n","result_dir = 'stargan_celeba/results'\n","if not os.path.exists(result_dir):\n","    os.makedirs(result_dir)\n","\n","#test 이미지에 대한 이미지 변환 결과를 출력합니다.\n","with torch.no_grad():\n","    for i, (x_real, c_org) in enumerate(celeba_testloader):\n","        # Prepare input images and target domain labels.\n","        x_real = x_real.to(device)\n","        c_trg_list = create_labels(c_org, 5, selected_attrs=selected_attrs)\n","\n","        # Translate images.\n","        x_fake_list = [x_real]\n","        for c_trg in c_trg_list:\n","            x_fake_list.append(Gen(x_real, c_trg))\n","\n","        # Save the translated images.\n","        x_concat = torch.cat(x_fake_list, dim=3)\n","        result_path = os.path.join(result_dir, '{}-images.jpg'.format(i+1))\n","        save_image(denorm(x_concat.data.cpu()), result_path, nrow=1, padding=0)\n","        print('Saved real and fake images into {}...'.format(result_path))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hxcV99P5DyQP","colab_type":"text"},"source":["## Pretrained Weight Download\n","\n","StarGAN의 경우 모델 학습에 요구되는 iteration 수가 크기 때문에 실습 시간이라는 제한 내에서 결과를 보기 어렵습니다. 따라서 이번 실습에서는 위의 train code과 test code의 진행 가능 여부를 확인했으니, 많은 iteration으로 미리 학습해둔, 즉 pretrained weight를 통해 test를 진행하겠습니다.\n","\n","Pretrained Weight는 보통 pt, pth, ckpt 등의 확장자를 가지고 있는 파일로, 이전에 학습해둔 모델의 weight 값들을 모델의 구조에 맞추어 저장하고 있습니다."]},{"cell_type":"markdown","metadata":{"id":"9wks4gVPDyQS","colab_type":"text"},"source":["!bash download.sh pretrained-celeba-128x128"]},{"cell_type":"code","metadata":{"scrolled":false,"id":"xIIJzvIdDyQT","colab_type":"code","colab":{}},"source":["# resume_iters만큼 학습하여 저장한 모델의 파라미터를 Load합니다.\n","print('Loading the trained models with pretrained weights...')\n","resume_iters = 200000\n","\n","Gen = Generator(64, 5)\n","\n","Gen.to(device)\n","\n","G_path = os.path.join(model_save_dir, '{}-G.ckpt'.format(resume_iters))\n","\n","G_pretrained_dict = torch.load(G_path, map_location=lambda storage, loc: storage)\n","G_new_pretrained_dict = {}\n","\n","G_dict = Gen.state_dict()\n","\n","for k in list(G_dict.keys()):\n","    if k.split('.')[-1] == 'num_batches_tracked':\n","        del G_dict[k]\n","\n","key_list = list(G_dict.keys())\n","value_list = list(G_pretrained_dict.values())\n","G_new_pretrained_dict = {}\n","\n","for i in range(len(G_pretrained_dict.items())):\n","    dict_key = key_list[i]\n","    dict_value = value_list[i]\n","\n","    G_new_pretrained_dict.update({dict_key : dict_value})\n","    \n","G_dict.update(G_new_pretrained_dict)\n","Gen.load_state_dict(G_new_pretrained_dict)\n","\n","#결과를 저장할 디렉토리 입니다.\n","result_dir = 'stargan_celeba/results'\n","if not os.path.exists(result_dir):\n","    os.makedirs(result_dir)\n","\n","#test 이미지에 대한 이미지 변환 결과를 출력합니다.\n","with torch.no_grad():\n","    for i, (x_real, c_org) in enumerate(celeba_testloader):\n","        # Prepare input images and target domain labels.\n","        x_real = x_real.to(device)\n","        c_trg_list = create_labels(c_org, 5, selected_attrs=selected_attrs)\n","\n","        # Translate images.\n","        x_fake_list = [x_real]\n","        for c_trg in c_trg_list:\n","            x_fake_list.append(Gen(x_real, c_trg))\n","\n","        # Save the translated images.\n","        x_concat = torch.cat(x_fake_list, dim=3)\n","        result_path = os.path.join(result_dir, '{}-images.jpg'.format(i+1))\n","        save_image(denorm(x_concat.data.cpu()), result_path, nrow=1, padding=0)\n","        print('Saved real and fake images into {}...'.format(result_path))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NrrUAXgZDyQW","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}