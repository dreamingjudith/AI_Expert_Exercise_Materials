{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1597303351710",
   "display_name": "Python 3.7.7 64-bit ('0813_dialogue_system': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice 1. BERT를 이용한 NLU 실습\n",
    "\n",
    "# 목차\n",
    "## Step 1. 데이터 전처리 (Data Preprocessing)\n",
    "## Step 2. BERT 세부조정하기 (Fine-tuning)\n",
    "## Step 3. 학습한 모델 평가하기\n",
    "\n",
    "최근 자연어처리 모듈에서 높은 성능을 보이는 대규모 사전학습 언어모델의 하나인 BERT를 활용하여 대화시스템의 모듈 중 하나인 NLU를 구현해보는 것이 이번 실습의 목표입니다.\n",
    "\n",
    "## Step 0. 사전설정\n",
    "아래 코드 블럭에서는 이 실습을 수행하기 위해 필요한 모듈들을 import합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "[nltk_data] Downloading package punkt to /home/com10/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n"
    }
   ],
   "source": [
    "# For data preprocessing\n",
    "import json\n",
    "import os\n",
    "import zipfile\n",
    "import sys\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# For Bert pretraining and postprecessing\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from convlab2.nlu.jointBERT.dataloader import Dataloader\n",
    "from convlab2.nlu.jointBERT.jointBERT import JointBERT\n",
    "\n",
    "CUDA_IDX = '0'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = CUDA_IDX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 코드 블럭에서는 다른 코드 구현을 간편하게 해주는 helper function들을 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "\n",
    "def read_zipped_json(filepath, filename):\n",
    "    archive = zipfile.ZipFile(filepath, 'r')\n",
    "    return json.load(archive.open(filename))\n",
    "\n",
    "\n",
    "def phrase_in_utt(phrase, utt):\n",
    "    phrase_low = phrase.lower()\n",
    "    utt_low = utt.lower()\n",
    "    return (' ' + phrase_low in utt_low) or utt_low.startswith(phrase_low)\n",
    "\n",
    "\n",
    "def phrase_idx_utt(phrase, utt):\n",
    "    phrase_low = phrase.lower()\n",
    "    utt_low = utt.lower()\n",
    "    if ' ' + phrase_low in utt_low or utt_low.startswith(phrase_low):\n",
    "        return get_idx(phrase_low, utt_low)\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_idx(phrase, utt):\n",
    "    char_index_begin = utt.index(phrase)\n",
    "    char_index_end = char_index_begin + len(phrase)\n",
    "    word_index_begin = len(utt[:char_index_begin].split())\n",
    "    word_index_end = len(utt[:char_index_end].split()) - 1\n",
    "    return word_index_begin, word_index_end\n",
    "\n",
    "\n",
    "def da2triples(dialog_act):\n",
    "    triples = []\n",
    "    for intent, svs in dialog_act.items():\n",
    "        for slot, value in svs:\n",
    "            triples.append([intent, slot, value])\n",
    "    return triples\n",
    "\n",
    "\n",
    "def das2tags(sen, das):\n",
    "    tokens = word_tokenize(sen)\n",
    "    new_sen = ' '.join(tokens)\n",
    "    new_das = {}\n",
    "    span_info = []\n",
    "    intents = []\n",
    "    for da, svs in das.items():\n",
    "        new_das.setdefault(da, [])\n",
    "        if da == 'inform':\n",
    "            for s, v in svs:\n",
    "                v = ' '.join(word_tokenize(v))\n",
    "                if v != 'dontcare' and phrase_in_utt(v, new_sen):\n",
    "                    word_index_begin, word_index_end = phrase_idx_utt(v, new_sen)\n",
    "                    span_info.append((da, s, v, word_index_begin, word_index_end))\n",
    "                else:\n",
    "                    intents.append(da + '+' + s + '*' + v)\n",
    "                new_das[da].append([s, v])\n",
    "        else:\n",
    "            for s, v in svs:\n",
    "                new_das[da].append([s, v])\n",
    "                intents.append(da + '+' + s + '*' + v)\n",
    "    tags = []\n",
    "    for i, _ in enumerate(tokens):\n",
    "        for span in span_info:\n",
    "            if i == span[3]:\n",
    "                tag = \"B-\" + span[0] + \"+\" + span[1]\n",
    "                tags.append(tag)\n",
    "                break\n",
    "            if span[3] < i <= span[4]:\n",
    "                tag = \"I-\" + span[0] + \"+\" + span[1]\n",
    "                tags.append(tag)\n",
    "                break\n",
    "        else:\n",
    "            tags.append(\"O\")\n",
    "\n",
    "    return tokens, tags, intents, da2triples(new_das)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1. 데이터 전처리 (Data Preprocessing)\n",
    "\n",
    "- BERT는 토큰화된 자연어(Natural Language)들을 입력받는 모델인데, NLU의 예측 타겟(prediction target)들은 모두 구조화된 데이터입니다. \n",
    "- 데이터 전처리 단계에서는 특수토큰(special token)들을 이용하여 구조화된 데이터를 자연어로 바꿔주는 처리를 합니다.\n",
    "- 또한 원래 하나였던 데이터셋을 train, validation, test 셋으로 분리합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "먼저 아래 코드블럭에서는 실습에서 사용할 데이터셋인 Camrest 데이터셋을 불러옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "load train, size 406\nload val, size 135\nload test, size 135\n"
    }
   ],
   "source": [
    "cur_dir = os.path.abspath(os.curdir)\n",
    "data_dir = \"ConvLab-2/data/camrest\"\n",
    "processed_data_dir = os.path.join(cur_dir, 'data/all_data')\n",
    "if not os.path.exists(processed_data_dir):\n",
    "    os.makedirs(processed_data_dir)\n",
    "\n",
    "data_key = ['train', 'val', 'test']\n",
    "data = {}\n",
    "for key in data_key:\n",
    "    data[key] = read_zipped_json(os.path.join(data_dir, key + '.json.zip'), key + '.json')\n",
    "    print('load {}, size {}'.format(key, len(data[key])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 코드블럭에서는 본격적으로 전처리 작업을 진행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "loaded train, size 3342\nloaded val, size 1076\nloaded test, size 1070\n"
    }
   ],
   "source": [
    "mode = 'all'\n",
    "processed_data = {}\n",
    "all_da = []\n",
    "all_intent = []\n",
    "all_tag = []\n",
    "context_size = 3\n",
    "for key in data_key:\n",
    "    processed_data[key] = []\n",
    "    for dialog in data[key]:\n",
    "        context = []\n",
    "        for turn in dialog['dial']:\n",
    "            if mode == 'usr' or mode == 'all':\n",
    "                tokens, tags, intents, new_das = das2tags(turn['usr']['transcript'], turn['usr']['dialog_act'])\n",
    "\n",
    "                processed_data[key].append([tokens, tags, intents, new_das, context[-context_size:]])\n",
    "\n",
    "                all_da += [da for da in turn['usr']['dialog_act']]\n",
    "                all_intent += intents\n",
    "                all_tag += tags\n",
    "\n",
    "            context.append(turn['usr']['transcript'])\n",
    "\n",
    "            if mode == 'sys' or mode == 'all':\n",
    "                tokens, tags, intents, new_das = das2tags(turn['sys']['sent'], turn['sys']['dialog_act'])\n",
    "\n",
    "                processed_data[key].append([tokens, tags, intents, new_das, context[-context_size:]])\n",
    "                all_da += [da for da in turn['sys']['dialog_act']]\n",
    "                all_intent += intents\n",
    "                all_tag += tags\n",
    "\n",
    "            context.append(turn['sys']['sent'])\n",
    "    \n",
    "    all_da = [x[0] for x in dict(Counter(all_da)).items() if x[1]]\n",
    "    all_intent = [x[0] for x in dict(Counter(all_intent)).items() if x[1]]\n",
    "    all_tag = [x[0] for x in dict(Counter(all_tag)).items() if x[1]]\n",
    "\n",
    "    print('loaded {}, size {}'.format(key, len(processed_data[key])))\n",
    "    json.dump(processed_data[key], \n",
    "              open(os.path.join(processed_data_dir, '{}_data.json'.format(key)), 'w'),\n",
    "              indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 코드블럭에서는 전처리 작업결과를 각각 json 파일에 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "dialog act num: 3\nsentence label num: 22\ntag num: 13\n"
    }
   ],
   "source": [
    "print('dialog act num:', len(all_da))\n",
    "print('sentence label num:', len(all_intent))\n",
    "print('tag num:', len(all_tag))\n",
    "json.dump(all_da, open(os.path.join(processed_data_dir, 'all_act.json'), 'w'), indent=2)\n",
    "json.dump(all_intent, open(os.path.join(processed_data_dir, 'intent_vocab.json'), 'w'), indent=2)\n",
    "json.dump(all_tag, open(os.path.join(processed_data_dir, 'tag_vocab.json'), 'w'), indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2. BERT 세부조정하기 (Fine-tuning)\n",
    "Pre-training된 BERT의 파라메터를 불러온 뒤 Camrest 데이터셋을 맞게 파라메터를 Fine-tuning 하는 부분을 실습해봅시다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.1 환경설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "--------------------dataset:camrest--------------------\n"
    }
   ],
   "source": [
    "### Configurations ###\n",
    "main_dir = processed_data_dir\n",
    "config = {\n",
    "  \"data_dir\": main_dir,\n",
    "  \"output_dir\": main_dir + \"/output/all\",\n",
    "  \"zipped_model_path\": main_dir + \"/output/all/bert_camrest_all.zip\",\n",
    "  \"log_dir\": main_dir + \"/log/all\",\n",
    "  \"DEVICE\": \"cuda:\"+CUDA_IDX,\n",
    "  \"seed\": 2019,\n",
    "  \"cut_sen_len\": 40,\n",
    "  \"use_bert_tokenizer\": True,\n",
    "  \"model\": {\n",
    "    \"finetune\": True,\n",
    "    \"context\": False,\n",
    "    \"context_grad\": False,\n",
    "    \"pretrained_weights\": \"bert-base-uncased\",\n",
    "    \"check_step\": 1000,\n",
    "    \"max_step\": 10000,\n",
    "    \"batch_size\": 20,\n",
    "    \"learning_rate\": 3e-5,\n",
    "    \"adam_epsilon\": 1e-8,\n",
    "    \"warmup_steps\": 0,\n",
    "    \"weight_decay\": 0.0,\n",
    "    \"dropout\": 0.1,\n",
    "    \"hidden_units\": 768\n",
    "  }\n",
    "}\n",
    "\n",
    "data_dir = config['data_dir']\n",
    "output_dir = config['output_dir']\n",
    "log_dir = config['log_dir']\n",
    "DEVICE = config['DEVICE']\n",
    "\n",
    "set_seed(config['seed'])\n",
    "\n",
    "print('-' * 20 + 'dataset:camrest' + '-' * 20)\n",
    "from convlab2.nlu.jointBERT.camrest.postprocess import is_slot_da, calculateF1, recover_intent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.2 Pre-trained BERT 모델 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "75), (6, 168), (7, 165), (8, 203), (9, 223), (10, 129), (11, 159), (12, 157), (13, 124), (14, 127), (15, 102), (16, 114), (17, 110), (18, 107), (19, 102), (20, 84), (21, 79), (22, 68), (23, 51), (24, 36), (25, 45), (26, 31), (27, 36), (28, 31), (29, 22), (30, 24), (31, 15), (32, 15), (33, 12), (34, 7), (35, 7), (36, 1), (37, 1), (38, 1), (39, 2), (40, 2), (41, 2), (42, 2), (43, 1), (46, 1), (47, 2)]\nmax context bert len 112\n[(3, 406), (7, 5), (8, 5), (9, 3), (10, 10), (11, 12), (12, 26), (13, 27), (14, 32), (15, 38), (16, 40), (17, 48), (18, 51), (19, 29), (20, 36), (21, 36), (22, 29), (23, 39), (24, 34), (25, 37), (26, 23), (27, 39), (28, 37), (29, 32), (30, 36), (31, 43), (32, 51), (33, 64), (34, 68), (35, 57), (36, 64), (37, 56), (38, 77), (39, 72), (40, 67), (41, 71), (42, 79), (43, 77), (44, 81), (45, 62), (46, 76), (47, 69), (48, 53), (49, 72), (50, 60), (51, 66), (52, 51), (53, 71), (54, 52), (55, 62), (56, 53), (57, 54), (58, 41), (59, 42), (60, 46), (61, 37), (62, 29), (63, 32), (64, 20), (65, 26), (66, 20), (67, 16), (68, 19), (69, 8), (70, 14), (71, 19), (72, 20), (73, 12), (74, 12), (75, 11), (76, 8), (77, 9), (78, 5), (79, 4), (80, 5), (81, 7), (82, 7), (83, 3), (84, 6), (85, 8), (86, 3), (88, 4), (89, 2), (91, 1), (94, 1), (96, 1), (97, 1), (100, 2), (104, 1), (106, 1), (112, 1)]\ntrain set size: 3342\nval\nmax sen bert len 50\n[(1, 4), (2, 19), (3, 17), (4, 68), (5, 125), (6, 43), (7, 45), (8, 62), (9, 73), (10, 41), (11, 53), (12, 44), (13, 54), (14, 42), (15, 43), (16, 37), (17, 38), (18, 29), (19, 27), (20, 26), (21, 18), (22, 26), (23, 26), (24, 21), (25, 21), (26, 13), (27, 6), (28, 8), (29, 7), (30, 6), (31, 13), (32, 4), (33, 1), (34, 4), (35, 2), (36, 1), (39, 4), (40, 2), (43, 2), (50, 1)]\nmax context bert len 108\n[(3, 135), (9, 3), (10, 3), (11, 3), (12, 7), (13, 11), (14, 8), (15, 12), (16, 10), (17, 12), (18, 15), (19, 11), (20, 16), (21, 4), (22, 10), (23, 9), (24, 10), (25, 14), (26, 13), (27, 15), (28, 8), (29, 10), (30, 15), (31, 22), (32, 18), (33, 17), (34, 12), (35, 23), (36, 17), (37, 21), (38, 17), (39, 29), (40, 19), (41, 18), (42, 25), (43, 28), (44, 25), (45, 23), (46, 18), (47, 11), (48, 19), (49, 27), (50, 20), (51, 21), (52, 26), (53, 22), (54, 12), (55, 16), (56, 15), (57, 13), (58, 15), (59, 9), (60, 6), (61, 11), (62, 15), (63, 14), (64, 11), (65, 7), (66, 8), (67, 12), (68, 5), (69, 5), (70, 5), (71, 7), (72, 3), (73, 4), (74, 4), (75, 6), (76, 3), (77, 2), (78, 2), (79, 4), (80, 6), (81, 1), (82, 2), (83, 4), (84, 2), (85, 3), (86, 2), (88, 2), (89, 1), (90, 1), (91, 1), (92, 1), (95, 1), (96, 1), (99, 1), (108, 1)]\nval set size: 1076\ntest\nmax sen bert len 47\n[(1, 6), (2, 34), (3, 28), (4, 57), (5, 114), (6, 58), (7, 59), (8, 62), (9, 44), (10, 43), (11, 48), (12, 41), (13, 42), (14, 45), (15, 40), (16, 31), (17, 53), (18, 42), (19, 29), (20, 27), (21, 25), (22, 20), (23, 14), (24, 19), (25, 15), (26, 17), (27, 12), (28, 6), (29, 4), (30, 4), (31, 5), (32, 7), (33, 1), (34, 4), (35, 2), (36, 6), (38, 1), (39, 2), (40, 1), (41, 1), (47, 1)]\nmax context bert len 110\n[(3, 135), (7, 1), (9, 2), (10, 1), (11, 1), (12, 10), (13, 10), (14, 8), (15, 11), (16, 12), (17, 12), (18, 15), (19, 10), (20, 7), (21, 18), (22, 13), (23, 13), (24, 8), (25, 10), (26, 9), (27, 9), (28, 7), (29, 13), (30, 12), (31, 15), (32, 11), (33, 16), (34, 18), (35, 13), (36, 22), (37, 10), (38, 22), (39, 27), (40, 25), (41, 27), (42, 37), (43, 19), (44, 25), (45, 23), (46, 19), (47, 23), (48, 20), (49, 23), (50, 14), (51, 28), (52, 11), (53, 20), (54, 31), (55, 10), (56, 21), (57, 16), (58, 12), (59, 20), (60, 18), (61, 12), (62, 11), (63, 8), (64, 7), (65, 4), (66, 4), (67, 7), (68, 10), (69, 3), (70, 6), (71, 9), (72, 4), (73, 6), (74, 4), (75, 3), (76, 2), (77, 5), (78, 3), (79, 2), (80, 2), (81, 5), (83, 1), (85, 1), (86, 1), (91, 2), (93, 1), (94, 1), (103, 2), (110, 1)]\ntest set size: 1070\nbert-base-uncased\nbert.embeddings.word_embeddings.weight torch.Size([30522, 768]) cuda:0 True\nbert.embeddings.position_embeddings.weight torch.Size([512, 768]) cuda:0 True\nbert.embeddings.token_type_embeddings.weight torch.Size([2, 768]) cuda:0 True\nbert.embeddings.LayerNorm.weight torch.Size([768]) cuda:0 True\nbert.embeddings.LayerNorm.bias torch.Size([768]) cuda:0 True\nbert.encoder.layer.0.attention.self.query.weight torch.Size([768, 768]) cuda:0 True\nbert.encoder.layer.0.attention.self.query.bias torch.Size([768]) cuda:0 True\nbert.encoder.layer.0.attention.self.key.weight torch.Size([768, 768]) cuda:0 True\nbert.encoder.layer.0.attention.self.key.bias torch.Size([768]) cuda:0 True\nbert.encoder.layer.0.attention.self.value.weight torch.Size([768, 768]) cuda:0 True\nbert.encoder.layer.0.attention.self.value.bias torch.Size([768]) cuda:0 True\nbert.encoder.layer.0.attention.output.dense.weight torch.Size([768, 768]) cuda:0 True\nbert.encoder.layer.0.attention.output.dense.bias torch.Size([768]) cuda:0 True\nbert.encoder.layer.0.attention.output.LayerNorm.weight torch.Size([768]) cuda:0 True\nbert.encoder.layer.0.attention.output.LayerNorm.bias torch.Size([768]) cuda:0 True\nbert.encoder.layer.0.intermediate.dense.weight torch.Size([3072, 768]) cuda:0 True\nbert.encoder.layer.0.intermediate.dense.bias torch.Size([3072]) cuda:0 True\nbert.encoder.layer.0.output.dense.weight torch.Size([768, 3072]) cuda:0 True\nbert.encoder.layer.0.output.dense.bias torch.Size([768]) cuda:0 True\nbert.encoder.layer.0.output.LayerNorm.weight torch.Size([768]) cuda:0 True\nbert.encoder.layer.0.output.LayerNorm.bias torch.Size([768]) cuda:0 True\nbert.encoder.layer.1.attention.self.query.weight torch.Size([768, 768]) cuda:0 True\nbert.encoder.layer.1.attention.self.query.bias torch.Size([768]) cuda:0 True\nbert.encoder.layer.1.attention.self.key.weight torch.Size([768, 768]) cuda:0 True\nbert.encoder.layer.1.attention.self.key.bias torch.Size([768]) cuda:0 True\nbert.encoder.layer.1.attention.self.value.weight torch.Size([768, 768]) cuda:0 True\nbert.encoder.layer.1.attention.self.value.bias torch.Size([768]) cuda:0 True\nbert.encoder.layer.1.attention.output.dense.weight torch.Size([768, 768]) cuda:0 True\nbert.encoder.layer.1.attention.output.dense.bias torch.Size([768]) cuda:0 True\nbert.encoder.layer.1.attention.output.LayerNorm.weight torch.Size([768]) cuda:0 True\nbert.encoder.layer.1.attention.output.LayerNorm.bias torch.Size([768]) cuda:0 True\nbert.encoder.layer.1.intermediate.dense.weight torch.Size([3072, 768]) cuda:0 True\nbert.encoder.layer.1.intermediate.dense.bias torch.Size([3072]) cuda:0 True\nbert.encoder.layer.1.output.dense.weight torch.Size([768, 3072]) cuda:0 True\nbert.encoder.layer.1.output.dense.bias torch.Size([768]) cuda:0 True\nbert.encoder.layer.1.output.LayerNorm.weight torch.Size([768]) cuda:0 True\nbert.encoder.layer.1.output.LayerNorm.bias torch.Size([768]) cuda:0 True\nbert.encoder.layer.2.attention.self.query.weight torch.Size([768, 768]) cuda:0 True\nbert.encoder.layer.2.attention.self.query.bias torch.Size([768]) cuda:0 True\nbert.encoder.layer.2.attention.self.key.weight torch.Size([768, 768]) cuda:0 True\nbert.encoder.layer.2.attention.self.key.bias torch.Size([768]) cuda:0 True\nbert.encoder.layer.2.attention.self.value.weight torch.Size([768, 768]) cuda:0 True\nbert.encoder.layer.2.attention.self.value.bias torch.Size([768]) cuda:0 True\nbert.encoder.layer.2.attention.output.dense.weight torch.Size([768, 768]) cuda:0 True\nbert.encoder.layer.2.attention.output.dense.bias torch.Size([768]) cuda:0 True\nbert.encoder.layer.2.attention.output.LayerNorm.weight torch.Size([768]) cuda:0 True\nbert.encoder.layer.2.attention.output.LayerNorm.bias torch.Size([768]) cuda:0 True\nbert.encoder.layer.2.intermediate.dense.weight torch.Size([3072, 768]) cuda:0 True\nbert.encoder.layer.2.intermediate.dense.bias torch.Size([3072]) cuda:0 True\nbert.encoder.layer.2.output.dense.weight torch.Size([768, 3072]) cuda:0 True\nbert.encoder.layer.2.output.dense.bias torch.Size([768]) cuda:0 True\nbert.encoder.layer.2.output.LayerNorm.weight torch.Size([768]) cuda:0 True\nbert.encoder.layer.2.output.LayerNorm.bias torch.Size([768]) cuda:0 True\nbert.encoder.layer.3.attention.self.query.weight torch.Size([768, 768]) cuda:0 True\nbert.encoder.layer.3.attention.self.query.bias torch.Size([768]) cuda:0 True\nbert.encoder.layer.3.attention.self.key.weight torch.Size([768, 768]) cuda:0 True\nbert.encoder.layer.3.attention.self.key.bias torch.Size([768]) cuda:0 True\nbert.encoder.layer.3.attention.self.value.weight torch.Size([768, 768]) cuda:0 True\nbert.encoder.layer.3.attention.self.value.bias torch.Size([768]) cuda:0 True\nbert.encoder.layer.3.attention.output.dense.weight torch.Size([768, 768]) cuda:0 True\nbert.encoder.layer.3.attention.output.dense.bias torch.Size([768]) cuda:0 True\nbert.encoder.layer.3.attention.output.LayerNorm.weight torch.Size([768]) cuda:0 True\nbert.encoder.layer.3.attention.output.LayerNorm.bias torch.Size([768]) cuda:0 True\nbert.encoder.layer.3.intermediate.dense.weight torch.Size([3072, 768]) cuda:0 True\nbert.encoder.layer.3.intermediate.dense.bias torch.Size([3072]) cuda:0 True\nbert.encoder.layer.3.output.dense.weight torch.Size([768, 3072]) cuda:0 True\nbert.encoder.layer.3.output.dense.bias torch.Size([768]) cuda:0 True\nbert.encoder.layer.3.output.LayerNorm.weight torch.Size([768]) cuda:0 True\nbert.encoder.layer.3.output.LayerNorm.bias torch.Size([768]) cuda:0 True\nbert.encoder.layer.4.attention.self.query.weight torch.Size([768, 768]) cuda:0 True\nbert.encoder.layer.4.attention.self.query.bias torch.Size([768]) cuda:0 True\nbert.encoder.layer.4.attention.self.key.weight torch.Size([768, 768]) cuda:0 True\nbert.encoder.layer.4.attention.self.key.bias torch.Size([768]) cuda:0 True\nbert.encoder.layer.4.attention.self.value.weight torch.Size([768, 768]) cuda:0 True\nbert.encoder.layer.4.attention.self.value.bias torch.Size([768]) cuda:0 True\nbert.encoder.layer.4.attention.output.dense.weight torch.Size([768, 768]) cuda:0 True\nbert.encoder.layer.4.attention.output.dense.bias torch.Size([768]) cuda:0 True\nbert.encoder.layer.4.attention.output.LayerNorm.weight torch.Size([768]) cuda:0 True\nbert.encoder.layer.4.attention.output.LayerNorm.bias torch.Size([768]) cuda:0 True\nbert.encoder.layer.4.intermediate.dense.weight torch.Size([3072, 768]) cuda:0 True\nbert.encoder.layer.4.intermediate.dense.bias torch.Size([3072]) cuda:0 True\nbert.encoder.layer.4.output.dense.weight torch.Size([768, 3072]) cuda:0 True\nbert.encoder.layer.4.output.dense.bias torch.Size([768]) cuda:0 True\nbert.encoder.layer.4.output.LayerNorm.weight torch.Size([768]) cuda:0 True\nbert.encoder.layer.4.output.LayerNorm.bias torch.Size([768]) cuda:0 True\nbert.encoder.layer.5.attention.self.query.weight torch.Size([768, 768]) cuda:0 True\nbert.encoder.layer.5.attention.self.query.bias torch.Size([768]) cuda:0 True\nbert.encoder.layer.5.attention.self.key.weight torch.Size([768, 768]) cuda:0 True\nbert.encoder.layer.5.attention.self.key.bias torch.Size([768]) cuda:0 True\nbert.encoder.layer.5.attention.self.value.weight torch.Size([768, 768]) cuda:0 True\nbert.encoder.layer.5.attention.self.value.bias torch.Size([768]) cuda:0 True\nbert.encoder.layer.5.attention.output.dense.weight torch.Size([768, 768]) cuda:0 True\nbert.encoder.layer.5.attention.output.dense.bias torch.Size([768]) cuda:0 True\nbert.encoder.layer.5.attention.output.LayerNorm.weight torch.Size([768]) cuda:0 True\nbert.encoder.layer.5.attention.output.LayerNorm.bias torch.Size([768]) cuda:0 True\nbert.encoder.layer.5.intermediate.dense.weight torch.Size([3072, 768]) cuda:0 True\nbert.encoder.layer.5.intermediate.dense.bias torch.Size([3072]) cuda:0 True\nbert.encoder.layer.5.output.dense.weight torch.Size([768, 3072]) cuda:0 True\nbert.encoder.layer.5.output.dense.bias torch.Size([768]) cuda:0 True\nbert.encoder.layer.5.output.LayerNorm.weight torch.Size([768]) cuda:0 True\nbert.encoder.layer.5.output.LayerNorm.bias torch.Size([768]) cuda:0 True\nbert.encoder.layer.6.attention.self.query.weight torch.Size([768, 768]) cuda:0 True\nbert.encoder.layer.6.attention.self.query.bias torch.Size([768]) cuda:0 True\nbert.encoder.layer.6.attention.self.key.weight torch.Size([768, 768]) cuda:0 True\nbert.encoder.layer.6.attention.self.key.bias torch.Size([768]) cuda:0 True\nbert.encoder.layer.6.attention.self.value.weight torch.Size([768, 768]) cuda:0 True\nbert.encoder.layer.6.attention.self.value.bias torch.Size([768]) cuda:0 True\nbert.encoder.layer.6.attention.output.dense.weight torch.Size([768, 768]) cuda:0 True\nbert.encoder.layer.6.attention.output.dense.bias torch.Size([768]) cuda:0 True\nbert.encoder.layer.6.attention.output.LayerNorm.weight torch.Size([768]) cuda:0 True\nbert.encoder.layer.6.attention.output.LayerNorm.bias torch.Size([768]) cuda:0 True\nbert.encoder.layer.6.intermediate.dense.weight torch.Size([3072, 768]) cuda:0 True\nbert.encoder.layer.6.intermediate.dense.bias torch.Size([3072]) cuda:0 True\nbert.encoder.layer.6.output.dense.weight torch.Size([768, 3072]) cuda:0 True\nbert.encoder.layer.6.output.dense.bias torch.Size([768]) cuda:0 True\nbert.encoder.layer.6.output.LayerNorm.weight torch.Size([768]) cuda:0 True\nbert.encoder.layer.6.output.LayerNorm.bias torch.Size([768]) cuda:0 True\nbert.encoder.layer.7.attention.self.query.weight torch.Size([768, 768]) cuda:0 True\nbert.encoder.layer.7.attention.self.query.bias torch.Size([768]) cuda:0 True\nbert.encoder.layer.7.attention.self.key.weight torch.Size([768, 768]) cuda:0 True\nbert.encoder.layer.7.attention.self.key.bias torch.Size([768]) cuda:0 True\nbert.encoder.layer.7.attention.self.value.weight torch.Size([768, 768]) cuda:0 True\nbert.encoder.layer.7.attention.self.value.bias torch.Size([768]) cuda:0 True\nbert.encoder.layer.7.attention.output.dense.weight torch.Size([768, 768]) cuda:0 True\nbert.encoder.layer.7.attention.output.dense.bias torch.Size([768]) cuda:0 True\nbert.encoder.layer.7.attention.output.LayerNorm.weight torch.Size([768]) cuda:0 True\nbert.encoder.layer.7.attention.output.LayerNorm.bias torch.Size([768]) cuda:0 True\nbert.encoder.layer.7.intermediate.dense.weight torch.Size([3072, 768]) cuda:0 True\nbert.encoder.layer.7.intermediate.dense.bias torch.Size([3072]) cuda:0 True\nbert.encoder.layer.7.output.dense.weight torch.Size([768, 3072]) cuda:0 True\nbert.encoder.layer.7.output.dense.bias torch.Size([768]) cuda:0 True\nbert.encoder.layer.7.output.LayerNorm.weight torch.Size([768]) cuda:0 True\nbert.encoder.layer.7.output.LayerNorm.bias torch.Size([768]) cuda:0 True\nbert.encoder.layer.8.attention.self.query.weight torch.Size([768, 768]) cuda:0 True\nbert.encoder.layer.8.attention.self.query.bias torch.Size([768]) cuda:0 True\nbert.encoder.layer.8.attention.self.key.weight torch.Size([768, 768]) cuda:0 True\nbert.encoder.layer.8.attention.self.key.bias torch.Size([768]) cuda:0 True\nbert.encoder.layer.8.attention.self.value.weight torch.Size([768, 768]) cuda:0 True\nbert.encoder.layer.8.attention.self.value.bias torch.Size([768]) cuda:0 True\nbert.encoder.layer.8.attention.output.dense.weight torch.Size([768, 768]) cuda:0 True\nbert.encoder.layer.8.attention.output.dense.bias torch.Size([768]) cuda:0 True\nbert.encoder.layer.8.attention.output.LayerNorm.weight torch.Size([768]) cuda:0 True\nbert.encoder.layer.8.attention.output.LayerNorm.bias torch.Size([768]) cuda:0 True\nbert.encoder.layer.8.intermediate.dense.weight torch.Size([3072, 768]) cuda:0 True\nbert.encoder.layer.8.intermediate.dense.bias torch.Size([3072]) cuda:0 True\nbert.encoder.layer.8.output.dense.weight torch.Size([768, 3072]) cuda:0 True\nbert.encoder.layer.8.output.dense.bias torch.Size([768]) cuda:0 True\nbert.encoder.layer.8.output.LayerNorm.weight torch.Size([768]) cuda:0 True\nbert.encoder.layer.8.output.LayerNorm.bias torch.Size([768]) cuda:0 True\nbert.encoder.layer.9.attention.self.query.weight torch.Size([768, 768]) cuda:0 True\nbert.encoder.layer.9.attention.self.query.bias torch.Size([768]) cuda:0 True\nbert.encoder.layer.9.attention.self.key.weight torch.Size([768, 768]) cuda:0 True\nbert.encoder.layer.9.attention.self.key.bias torch.Size([768]) cuda:0 True\nbert.encoder.layer.9.attention.self.value.weight torch.Size([768, 768]) cuda:0 True\nbert.encoder.layer.9.attention.self.value.bias torch.Size([768]) cuda:0 True\nbert.encoder.layer.9.attention.output.dense.weight torch.Size([768, 768]) cuda:0 True\nbert.encoder.layer.9.attention.output.dense.bias torch.Size([768]) cuda:0 True\nbert.encoder.layer.9.attention.output.LayerNorm.weight torch.Size([768]) cuda:0 True\nbert.encoder.layer.9.attention.output.LayerNorm.bias torch.Size([768]) cuda:0 True\nbert.encoder.layer.9.intermediate.dense.weight torch.Size([3072, 768]) cuda:0 True\nbert.encoder.layer.9.intermediate.dense.bias torch.Size([3072]) cuda:0 True\nbert.encoder.layer.9.output.dense.weight torch.Size([768, 3072]) cuda:0 True\nbert.encoder.layer.9.output.dense.bias torch.Size([768]) cuda:0 True\nbert.encoder.layer.9.output.LayerNorm.weight torch.Size([768]) cuda:0 True\nbert.encoder.layer.9.output.LayerNorm.bias torch.Size([768]) cuda:0 True\nbert.encoder.layer.10.attention.self.query.weight torch.Size([768, 768]) cuda:0 True\nbert.encoder.layer.10.attention.self.query.bias torch.Size([768]) cuda:0 True\nbert.encoder.layer.10.attention.self.key.weight torch.Size([768, 768]) cuda:0 True\nbert.encoder.layer.10.attention.self.key.bias torch.Size([768]) cuda:0 True\nbert.encoder.layer.10.attention.self.value.weight torch.Size([768, 768]) cuda:0 True\nbert.encoder.layer.10.attention.self.value.bias torch.Size([768]) cuda:0 True\nbert.encoder.layer.10.attention.output.dense.weight torch.Size([768, 768]) cuda:0 True\nbert.encoder.layer.10.attention.output.dense.bias torch.Size([768]) cuda:0 True\nbert.encoder.layer.10.attention.output.LayerNorm.weight torch.Size([768]) cuda:0 True\nbert.encoder.layer.10.attention.output.LayerNorm.bias torch.Size([768]) cuda:0 True\nbert.encoder.layer.10.intermediate.dense.weight torch.Size([3072, 768]) cuda:0 True\nbert.encoder.layer.10.intermediate.dense.bias torch.Size([3072]) cuda:0 True\nbert.encoder.layer.10.output.dense.weight torch.Size([768, 3072]) cuda:0 True\nbert.encoder.layer.10.output.dense.bias torch.Size([768]) cuda:0 True\nbert.encoder.layer.10.output.LayerNorm.weight torch.Size([768]) cuda:0 True\nbert.encoder.layer.10.output.LayerNorm.bias torch.Size([768]) cuda:0 True\nbert.encoder.layer.11.attention.self.query.weight torch.Size([768, 768]) cuda:0 True\nbert.encoder.layer.11.attention.self.query.bias torch.Size([768]) cuda:0 True\nbert.encoder.layer.11.attention.self.key.weight torch.Size([768, 768]) cuda:0 True\nbert.encoder.layer.11.attention.self.key.bias torch.Size([768]) cuda:0 True\nbert.encoder.layer.11.attention.self.value.weight torch.Size([768, 768]) cuda:0 True\nbert.encoder.layer.11.attention.self.value.bias torch.Size([768]) cuda:0 True\nbert.encoder.layer.11.attention.output.dense.weight torch.Size([768, 768]) cuda:0 True\nbert.encoder.layer.11.attention.output.dense.bias torch.Size([768]) cuda:0 True\nbert.encoder.layer.11.attention.output.LayerNorm.weight torch.Size([768]) cuda:0 True\nbert.encoder.layer.11.attention.output.LayerNorm.bias torch.Size([768]) cuda:0 True\nbert.encoder.layer.11.intermediate.dense.weight torch.Size([3072, 768]) cuda:0 True\nbert.encoder.layer.11.intermediate.dense.bias torch.Size([3072]) cuda:0 True\nbert.encoder.layer.11.output.dense.weight torch.Size([768, 3072]) cuda:0 True\nbert.encoder.layer.11.output.dense.bias torch.Size([768]) cuda:0 True\nbert.encoder.layer.11.output.LayerNorm.weight torch.Size([768]) cuda:0 True\nbert.encoder.layer.11.output.LayerNorm.bias torch.Size([768]) cuda:0 True\nbert.pooler.dense.weight torch.Size([768, 768]) cuda:0 True\nbert.pooler.dense.bias torch.Size([768]) cuda:0 True\nintent_classifier.weight torch.Size([22, 768]) cuda:0 True\nintent_classifier.bias torch.Size([22]) cuda:0 True\nslot_classifier.weight torch.Size([13, 768]) cuda:0 True\nslot_classifier.bias torch.Size([13]) cuda:0 True\nintent_hidden.weight torch.Size([768, 768]) cuda:0 True\nintent_hidden.bias torch.Size([768]) cuda:0 True\nslot_hidden.weight torch.Size([768, 768]) cuda:0 True\nslot_hidden.bias torch.Size([768]) cuda:0 True\n"
    }
   ],
   "source": [
    "intent_vocab = json.load(open(os.path.join(data_dir, 'intent_vocab.json')))\n",
    "tag_vocab = json.load(open(os.path.join(data_dir, 'tag_vocab.json')))\n",
    "dataloader = Dataloader(intent_vocab=intent_vocab, tag_vocab=tag_vocab,\n",
    "                        pretrained_weights=config['model']['pretrained_weights'])\n",
    "print('intent num:', len(intent_vocab))\n",
    "print('tag num:', len(tag_vocab))\n",
    "for data_key in ['train', 'val', 'test']:\n",
    "    print(data_key)\n",
    "    dataloader.load_data(json.load(open(os.path.join(data_dir, '{}_data.json'.format(data_key)))), data_key,\n",
    "                            cut_sen_len=config['cut_sen_len'], use_bert_tokenizer=config['use_bert_tokenizer'])\n",
    "    print('{} set size: {}'.format(data_key, len(dataloader.data[data_key])))\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "\n",
    "writer = SummaryWriter(log_dir)\n",
    "\n",
    "model = JointBERT(config['model'], DEVICE, dataloader.tag_dim, dataloader.intent_dim, dataloader.intent_weight)\n",
    "model.to(DEVICE)\n",
    "\n",
    "if config['model']['finetune']:\n",
    "    no_decay = ['bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in model.named_parameters() if\n",
    "                    not any(nd in n for nd in no_decay) and p.requires_grad],\n",
    "            'weight_decay': config['model']['weight_decay']},\n",
    "        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay) and p.requires_grad],\n",
    "            'weight_decay': 0.0}\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=config['model']['learning_rate'],\n",
    "                        eps=config['model']['adam_epsilon'])\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=config['model']['warmup_steps'],\n",
    "                                                num_training_steps=config['model']['max_step'])\n",
    "else:\n",
    "    for n, p in model.named_parameters():\n",
    "        if 'bert' in n:\n",
    "            p.requires_grad = False\n",
    "    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()),\n",
    "                                    lr=config['model']['learning_rate'])\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.shape, param.device, param.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.3 불러온 BERT 모델을 Fine-tuning 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "10%|▉         | 999/10000 [01:50<16:54,  8.88it/s][1000|10000] step\n\t slot loss: 0.04791409895531251\n\t intent loss: 0.04932960451469989\n1076 samples val\n\t slot loss: 0.04834849305332337\n\t intent loss: 0.021551814817020246\n--------------------intent--------------------\n\t Precision: 91.75\n\t Recall: 94.26\n\t F1: 92.99\n--------------------slot--------------------\n\t Precision: 90.61\n\t Recall: 92.60\n\t F1: 91.60\n--------------------overall--------------------\n\t Precision: 90.88\n\t Recall: 92.99\n\t F1: 91.92\n 10%|█         | 1001/10000 [01:54<2:04:56,  1.20it/s]best val F1 0.9192\nsave on /home/com10/0813_dialogue_system/data/all_data/output/all\n 20%|█▉        | 1999/10000 [03:45<15:37,  8.53it/s][2000|10000] step\n\t slot loss: 0.003414770948329533\n\t intent loss: 0.010031466174346861\n1076 samples val\n\t slot loss: 0.06444309662150921\n\t intent loss: 0.021941074551443212\n--------------------intent--------------------\n\t Precision: 92.58\n\t Recall: 96.51\n\t F1: 94.51\n--------------------slot--------------------\n\t Precision: 91.17\n\t Recall: 92.82\n\t F1: 91.99\n--------------------overall--------------------\n\t Precision: 91.50\n\t Recall: 93.68\n\t F1: 92.58\n 20%|██        | 2001/10000 [03:49<1:59:52,  1.11it/s]best val F1 0.9258\nsave on /home/com10/0813_dialogue_system/data/all_data/output/all\n 30%|██▉       | 2999/10000 [05:41<13:25,  8.70it/s][3000|10000] step\n\t slot loss: 0.0020251222493388923\n\t intent loss: 0.00728265888873284\n 30%|███       | 3001/10000 [05:44<1:31:19,  1.28it/s]1076 samples val\n\t slot loss: 0.06771692945666673\n\t intent loss: 0.022797923051920067\n--------------------intent--------------------\n\t Precision: 91.45\n\t Recall: 96.01\n\t F1: 93.67\n--------------------slot--------------------\n\t Precision: 91.03\n\t Recall: 92.82\n\t F1: 91.92\n--------------------overall--------------------\n\t Precision: 91.13\n\t Recall: 93.56\n\t F1: 92.33\n 40%|███▉      | 3999/10000 [07:35<11:33,  8.65it/s][4000|10000] step\n\t slot loss: 0.0014933362681313156\n\t intent loss: 0.005311172196154075\n 40%|████      | 4002/10000 [07:39<1:16:23,  1.31it/s]1076 samples val\n\t slot loss: 0.07414940258857544\n\t intent loss: 0.024761991956252623\n--------------------intent--------------------\n\t Precision: 91.81\n\t Recall: 95.01\n\t F1: 93.38\n--------------------slot--------------------\n\t Precision: 91.09\n\t Recall: 92.74\n\t F1: 91.91\n--------------------overall--------------------\n\t Precision: 91.26\n\t Recall: 93.27\n\t F1: 92.25\n 50%|████▉     | 4999/10000 [09:29<09:34,  8.70it/s][5000|10000] step\n\t slot loss: 0.0008925163714393421\n\t intent loss: 0.004043081132167572\n 50%|█████     | 5001/10000 [09:32<1:05:19,  1.28it/s]1076 samples val\n\t slot loss: 0.07026376888970118\n\t intent loss: 0.025365357386089166\n--------------------intent--------------------\n\t Precision: 92.58\n\t Recall: 96.51\n\t F1: 94.51\n--------------------slot--------------------\n\t Precision: 90.67\n\t Recall: 92.73\n\t F1: 91.69\n--------------------overall--------------------\n\t Precision: 91.12\n\t Recall: 93.61\n\t F1: 92.35\n 60%|█████▉    | 5999/10000 [11:24<07:36,  8.76it/s][6000|10000] step\n\t slot loss: 0.0007981558090232283\n\t intent loss: 0.004253985002298577\n1076 samples val\n\t slot loss: 0.07679690390686031\n\t intent loss: 0.024628774900395414\n--------------------intent--------------------\n\t Precision: 92.82\n\t Recall: 96.76\n\t F1: 94.75\n--------------------slot--------------------\n\t Precision: 91.52\n\t Recall: 93.05\n\t F1: 92.28\n--------------------overall--------------------\n\t Precision: 91.83\n\t Recall: 93.91\n\t F1: 92.86\n 60%|██████    | 6001/10000 [11:27<1:00:04,  1.11it/s]best val F1 0.9286\nsave on /home/com10/0813_dialogue_system/data/all_data/output/all\n 70%|██████▉   | 6999/10000 [13:18<05:41,  8.78it/s][7000|10000] step\n\t slot loss: 0.0006684199993424045\n\t intent loss: 0.003784530494023784\n 70%|███████   | 7001/10000 [13:22<37:52,  1.32it/s]1076 samples val\n\t slot loss: 0.08252004901872974\n\t intent loss: 0.026910661030281596\n--------------------intent--------------------\n\t Precision: 90.91\n\t Recall: 97.26\n\t F1: 93.98\n--------------------slot--------------------\n\t Precision: 91.33\n\t Recall: 93.20\n\t F1: 92.26\n--------------------overall--------------------\n\t Precision: 91.23\n\t Recall: 94.14\n\t F1: 92.66\n 80%|███████▉  | 7999/10000 [15:13<03:47,  8.81it/s][8000|10000] step\n\t slot loss: 0.00048301427388787487\n\t intent loss: 0.0033518705239866903\n 80%|████████  | 8001/10000 [15:16<25:38,  1.30it/s]1076 samples val\n\t slot loss: 0.08329955605449994\n\t intent loss: 0.027660371990201774\n--------------------intent--------------------\n\t Precision: 91.75\n\t Recall: 97.01\n\t F1: 94.30\n--------------------slot--------------------\n\t Precision: 91.52\n\t Recall: 93.05\n\t F1: 92.28\n--------------------overall--------------------\n\t Precision: 91.58\n\t Recall: 93.97\n\t F1: 92.76\n 90%|████████▉ | 8999/10000 [17:08<01:52,  8.90it/s][9000|10000] step\n\t slot loss: 0.0002761406870661176\n\t intent loss: 0.0035593677275010125\n 90%|█████████ | 9001/10000 [17:11<12:41,  1.31it/s]1076 samples val\n\t slot loss: 0.08412377255874016\n\t intent loss: 0.02727012328663051\n--------------------intent--------------------\n\t Precision: 92.16\n\t Recall: 96.76\n\t F1: 94.40\n--------------------slot--------------------\n\t Precision: 91.46\n\t Recall: 93.05\n\t F1: 92.24\n--------------------overall--------------------\n\t Precision: 91.62\n\t Recall: 93.91\n\t F1: 92.75\n100%|█████████▉| 9999/10000 [19:03<00:00,  8.41it/s][10000|10000] step\n\t slot loss: 0.0003269579779459946\n\t intent loss: 0.003167884860709364\n100%|██████████| 10000/10000 [19:06<00:00,  8.72it/s]1076 samples val\n\t slot loss: 0.08495130463258223\n\t intent loss: 0.027813962938359058\n--------------------intent--------------------\n\t Precision: 91.75\n\t Recall: 97.01\n\t F1: 94.30\n--------------------slot--------------------\n\t Precision: 91.20\n\t Recall: 93.20\n\t F1: 92.19\n--------------------overall--------------------\n\t Precision: 91.33\n\t Recall: 94.08\n\t F1: 92.69\nzip model to /home/com10/0813_dialogue_system/data/all_data/output/all/bert_camrest_all.zip\n\n"
    }
   ],
   "source": [
    "max_step = config['model']['max_step']\n",
    "check_step = config['model']['check_step']\n",
    "batch_size = config['model']['batch_size']\n",
    "model.zero_grad()\n",
    "train_slot_loss, train_intent_loss = 0, 0\n",
    "best_val_f1 = 0.\n",
    "\n",
    "writer.add_text('config', json.dumps(config))\n",
    "\n",
    "for step in tqdm(range(1, max_step + 1)):\n",
    "    model.train()\n",
    "    batched_data = dataloader.get_train_batch(batch_size)\n",
    "    batched_data = tuple(t.to(DEVICE) for t in batched_data)\n",
    "    word_seq_tensor, tag_seq_tensor, intent_tensor, word_mask_tensor, tag_mask_tensor, context_seq_tensor, context_mask_tensor = batched_data\n",
    "    if not config['model']['context']:\n",
    "        context_seq_tensor, context_mask_tensor = None, None\n",
    "    _, _, slot_loss, intent_loss = model.forward(word_seq_tensor, word_mask_tensor, tag_seq_tensor, tag_mask_tensor,\n",
    "                                                    intent_tensor, context_seq_tensor, context_mask_tensor)\n",
    "    train_slot_loss += slot_loss.item()\n",
    "    train_intent_loss += intent_loss.item()\n",
    "    loss = slot_loss + intent_loss\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    if config['model']['finetune']:\n",
    "        scheduler.step()  # Update learning rate schedule\n",
    "    model.zero_grad()\n",
    "    if step % check_step == 0:\n",
    "        train_slot_loss = train_slot_loss / check_step\n",
    "        train_intent_loss = train_intent_loss / check_step\n",
    "        print('[%d|%d] step' % (step, max_step))\n",
    "        print('\\t slot loss:', train_slot_loss)\n",
    "        print('\\t intent loss:', train_intent_loss)\n",
    "\n",
    "        predict_golden = {'intent': [], 'slot': [], 'overall': []}\n",
    "\n",
    "        val_slot_loss, val_intent_loss = 0, 0\n",
    "        model.eval()\n",
    "        for pad_batch, ori_batch, real_batch_size in dataloader.yield_batches(batch_size, data_key='val'):\n",
    "            pad_batch = tuple(t.to(DEVICE) for t in pad_batch)\n",
    "            word_seq_tensor, tag_seq_tensor, intent_tensor, word_mask_tensor, tag_mask_tensor, context_seq_tensor, context_mask_tensor = pad_batch\n",
    "            if not config['model']['context']:\n",
    "                context_seq_tensor, context_mask_tensor = None, None\n",
    "\n",
    "            with torch.no_grad():\n",
    "                slot_logits, intent_logits, slot_loss, intent_loss = model.forward(word_seq_tensor,\n",
    "                                                                                    word_mask_tensor,\n",
    "                                                                                    tag_seq_tensor,\n",
    "                                                                                    tag_mask_tensor,\n",
    "                                                                                    intent_tensor,\n",
    "                                                                                    context_seq_tensor,\n",
    "                                                                                    context_mask_tensor)\n",
    "            val_slot_loss += slot_loss.item() * real_batch_size\n",
    "            val_intent_loss += intent_loss.item() * real_batch_size\n",
    "            for j in range(real_batch_size):\n",
    "                predicts = recover_intent(dataloader, intent_logits[j], slot_logits[j], tag_mask_tensor[j],\n",
    "                                            ori_batch[j][0], ori_batch[j][-4])\n",
    "                labels = ori_batch[j][3]\n",
    "\n",
    "                predict_golden['overall'].append({\n",
    "                    'predict': predicts,\n",
    "                    'golden': labels\n",
    "                })\n",
    "                predict_golden['slot'].append({\n",
    "                    'predict': [x for x in predicts if is_slot_da(x)],\n",
    "                    'golden': [x for x in labels if is_slot_da(x)]\n",
    "                })\n",
    "                predict_golden['intent'].append({\n",
    "                    'predict': [x for x in predicts if not is_slot_da(x)],\n",
    "                    'golden': [x for x in labels if not is_slot_da(x)]\n",
    "                })\n",
    "\n",
    "        for j in range(10):\n",
    "            writer.add_text('val_sample_{}'.format(j),\n",
    "                            json.dumps(predict_golden['overall'][j], indent=2, ensure_ascii=False),\n",
    "                            global_step=step)\n",
    "\n",
    "        total = len(dataloader.data['val'])\n",
    "        val_slot_loss /= total\n",
    "        val_intent_loss /= total\n",
    "        print('%d samples val' % total)\n",
    "        print('\\t slot loss:', val_slot_loss)\n",
    "        print('\\t intent loss:', val_intent_loss)\n",
    "\n",
    "        writer.add_scalar('intent_loss/train', train_intent_loss, global_step=step)\n",
    "        writer.add_scalar('intent_loss/val', val_intent_loss, global_step=step)\n",
    "\n",
    "        writer.add_scalar('slot_loss/train', train_slot_loss, global_step=step)\n",
    "        writer.add_scalar('slot_loss/val', val_slot_loss, global_step=step)\n",
    "\n",
    "        for x in ['intent', 'slot', 'overall']:\n",
    "            precision, recall, F1 = calculateF1(predict_golden[x])\n",
    "            print('-' * 20 + x + '-' * 20)\n",
    "            print('\\t Precision: %.2f' % (100 * precision))\n",
    "            print('\\t Recall: %.2f' % (100 * recall))\n",
    "            print('\\t F1: %.2f' % (100 * F1))\n",
    "\n",
    "            writer.add_scalar('val_{}/precision'.format(x), precision, global_step=step)\n",
    "            writer.add_scalar('val_{}/recall'.format(x), recall, global_step=step)\n",
    "            writer.add_scalar('val_{}/F1'.format(x), F1, global_step=step)\n",
    "\n",
    "        if F1 > best_val_f1:\n",
    "            best_val_f1 = F1\n",
    "            torch.save(model.state_dict(), os.path.join(output_dir, 'pytorch_model.bin'))\n",
    "            print('best val F1 %.4f' % best_val_f1)\n",
    "            print('save on', output_dir)\n",
    "\n",
    "        train_slot_loss, train_intent_loss = 0, 0\n",
    "\n",
    "writer.add_text('val overall F1', '%.2f' % (100 * best_val_f1))\n",
    "writer.close()\n",
    "\n",
    "model_path = os.path.join(output_dir, 'pytorch_model.bin')\n",
    "zip_path = config['zipped_model_path']\n",
    "print('zip model to', zip_path)\n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zf:\n",
    "    zf.write(model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3. 학습한 모델 평가하기\n",
    "학습한 모델을 양적 (quantitatively), 질적(qualitatively)으로 각각 평가해봅시다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.1 양적 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "intent num: 22\ntag num: 13\nmax sen bert len 50\n[(1, 4), (2, 19), (3, 17), (4, 68), (5, 125), (6, 43), (7, 45), (8, 62), (9, 73), (10, 41), (11, 53), (12, 44), (13, 54), (14, 42), (15, 43), (16, 37), (17, 38), (18, 29), (19, 27), (20, 26), (21, 18), (22, 26), (23, 26), (24, 21), (25, 21), (26, 13), (27, 6), (28, 8), (29, 7), (30, 6), (31, 13), (32, 4), (33, 1), (34, 4), (35, 2), (36, 1), (39, 4), (40, 2), (43, 2), (50, 1)]\nmax context bert len 108\n[(3, 135), (9, 3), (10, 3), (11, 3), (12, 7), (13, 11), (14, 8), (15, 12), (16, 10), (17, 12), (18, 15), (19, 11), (20, 16), (21, 4), (22, 10), (23, 9), (24, 10), (25, 14), (26, 13), (27, 15), (28, 8), (29, 10), (30, 15), (31, 22), (32, 18), (33, 17), (34, 12), (35, 23), (36, 17), (37, 21), (38, 17), (39, 29), (40, 19), (41, 18), (42, 25), (43, 28), (44, 25), (45, 23), (46, 18), (47, 11), (48, 19), (49, 27), (50, 20), (51, 21), (52, 26), (53, 22), (54, 12), (55, 16), (56, 15), (57, 13), (58, 15), (59, 9), (60, 6), (61, 11), (62, 15), (63, 14), (64, 11), (65, 7), (66, 8), (67, 12), (68, 5), (69, 5), (70, 5), (71, 7), (72, 3), (73, 4), (74, 3), (75, 6), (76, 3), (77, 2), (78, 2), (79, 4), (80, 6), (81, 2), (82, 2), (83, 3), (84, 2), (85, 3), (86, 2), (88, 2), (89, 1), (90, 2), (91, 1), (92, 1), (95, 1), (96, 1), (99, 1), (108, 1)]\nval set size: 1076\nmax sen bert len 47\n[(1, 6), (2, 34), (3, 28), (4, 57), (5, 114), (6, 58), (7, 59), (8, 62), (9, 44), (10, 43), (11, 48), (12, 41), (13, 42), (14, 45), (15, 40), (16, 31), (17, 53), (18, 42), (19, 29), (20, 27), (21, 25), (22, 20), (23, 14), (24, 19), (25, 15), (26, 17), (27, 12), (28, 6), (29, 4), (30, 4), (31, 5), (32, 7), (33, 1), (34, 4), (35, 2), (36, 6), (38, 1), (39, 2), (40, 1), (41, 1), (47, 1)]\nmax context bert len 110\n[(3, 135), (7, 1), (9, 2), (10, 1), (11, 1), (12, 10), (13, 10), (14, 8), (15, 11), (16, 12), (17, 12), (18, 15), (19, 10), (20, 7), (21, 18), (22, 13), (23, 13), (24, 8), (25, 10), (26, 9), (27, 9), (28, 7), (29, 13), (30, 12), (31, 15), (32, 11), (33, 16), (34, 18), (35, 13), (36, 22), (37, 10), (38, 22), (39, 27), (40, 25), (41, 27), (42, 37), (43, 19), (44, 25), (45, 23), (46, 19), (47, 23), (48, 20), (49, 23), (50, 14), (51, 28), (52, 11), (53, 20), (54, 31), (55, 10), (56, 21), (57, 16), (58, 12), (59, 20), (60, 18), (61, 12), (62, 11), (63, 8), (64, 7), (65, 4), (66, 4), (67, 7), (68, 10), (69, 3), (70, 6), (71, 9), (72, 4), (73, 6), (74, 4), (75, 3), (76, 2), (77, 5), (78, 3), (79, 2), (80, 2), (81, 5), (83, 1), (85, 1), (86, 1), (91, 2), (93, 1), (94, 1), (103, 2), (110, 1)]\ntest set size: 1070\nbert-base-uncased\n[20|1070] samples\n[40|1070] samples\n[60|1070] samples\n[80|1070] samples\n[100|1070] samples\n[120|1070] samples\n[140|1070] samples\n[160|1070] samples\n[180|1070] samples\n[200|1070] samples\n[220|1070] samples\n[240|1070] samples\n[260|1070] samples\n[280|1070] samples\n[300|1070] samples\n[320|1070] samples\n[340|1070] samples\n[360|1070] samples\n[380|1070] samples\n[400|1070] samples\n[420|1070] samples\n[440|1070] samples\n[460|1070] samples\n[480|1070] samples\n[500|1070] samples\n[520|1070] samples\n[540|1070] samples\n[560|1070] samples\n[580|1070] samples\n[600|1070] samples\n[620|1070] samples\n[640|1070] samples\n[660|1070] samples\n[680|1070] samples\n[700|1070] samples\n[720|1070] samples\n[740|1070] samples\n[760|1070] samples\n[780|1070] samples\n[800|1070] samples\n[820|1070] samples\n[840|1070] samples\n[860|1070] samples\n[880|1070] samples\n[900|1070] samples\n[920|1070] samples\n[940|1070] samples\n[960|1070] samples\n[980|1070] samples\n[1000|1070] samples\n[1020|1070] samples\n[1040|1070] samples\n[1060|1070] samples\n[1070|1070] samples\n1070 samples test\n\t slot loss: 0.06380217826146317\n\t intent loss: 0.028432078651374502\n--------------------intent--------------------\n\t Precision: 92.26\n\t Recall: 95.64\n\t F1: 93.92\n--------------------slot--------------------\n\t Precision: 90.67\n\t Recall: 91.04\n\t F1: 90.85\n--------------------overall--------------------\n\t Precision: 91.09\n\t Recall: 92.24\n\t F1: 91.66\n"
    }
   ],
   "source": [
    "intent_vocab = json.load(open(os.path.join(data_dir, 'intent_vocab.json')))\n",
    "tag_vocab = json.load(open(os.path.join(data_dir, 'tag_vocab.json')))\n",
    "dataloader = Dataloader(intent_vocab=intent_vocab, tag_vocab=tag_vocab,\n",
    "                        pretrained_weights=config['model']['pretrained_weights'])\n",
    "print('intent num:', len(intent_vocab))\n",
    "print('tag num:', len(tag_vocab))\n",
    "for data_key in ['val', 'test']:\n",
    "    dataloader.load_data(json.load(open(os.path.join(data_dir, '{}_data.json'.format(data_key)))), data_key,\n",
    "                            cut_sen_len=0, use_bert_tokenizer=config['use_bert_tokenizer'])\n",
    "    print('{} set size: {}'.format(data_key, len(dataloader.data[data_key])))\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "\n",
    "model = JointBERT(config['model'], DEVICE, dataloader.tag_dim, dataloader.intent_dim)\n",
    "model.load_state_dict(torch.load(os.path.join(output_dir, 'pytorch_model.bin'), DEVICE))\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "batch_size = config['model']['batch_size']\n",
    "GT_sent = []\n",
    "GT_slot = []\n",
    "data_key = 'test'\n",
    "predict_golden = {'intent': [], 'slot': [], 'overall': []}\n",
    "slot_loss, intent_loss = 0, 0\n",
    "for pad_batch, ori_batch, real_batch_size in dataloader.yield_batches(batch_size, data_key=data_key):\n",
    "    pad_batch = tuple(t.to(DEVICE) for t in pad_batch)\n",
    "    word_seq_tensor, tag_seq_tensor, intent_tensor, word_mask_tensor, tag_mask_tensor, context_seq_tensor, context_mask_tensor = pad_batch\n",
    "    if not config['model']['context']:\n",
    "        context_seq_tensor, context_mask_tensor = None, None\n",
    "\n",
    "    with torch.no_grad():\n",
    "        slot_logits, intent_logits, batch_slot_loss, batch_intent_loss = model.forward(word_seq_tensor,\n",
    "                                                                                        word_mask_tensor,\n",
    "                                                                                        tag_seq_tensor,\n",
    "                                                                                        tag_mask_tensor,\n",
    "                                                                                        intent_tensor,\n",
    "                                                                                        context_seq_tensor,\n",
    "                                                                                        context_mask_tensor)\n",
    "    slot_loss += batch_slot_loss.item() * real_batch_size\n",
    "    intent_loss += batch_intent_loss.item() * real_batch_size\n",
    "    for j in range(real_batch_size):\n",
    "        predicts = recover_intent(dataloader, intent_logits[j], slot_logits[j], tag_mask_tensor[j],\n",
    "                                    ori_batch[j][0], ori_batch[j][-4])\n",
    "        labels = ori_batch[j][3]\n",
    "        GT_sent.append(ori_batch[j][0])\n",
    "        GT_slot.append(ori_batch[j][1])\n",
    "        predict_golden['overall'].append({\n",
    "            'predict': predicts,\n",
    "            'golden': labels\n",
    "        })\n",
    "        predict_golden['slot'].append({\n",
    "            'predict': [x for x in predicts if is_slot_da(x)],\n",
    "            'golden': [x for x in labels if is_slot_da(x)]\n",
    "        })\n",
    "        predict_golden['intent'].append({\n",
    "            'predict': [x for x in predicts if not is_slot_da(x)],\n",
    "            'golden': [x for x in labels if not is_slot_da(x)]\n",
    "        })\n",
    "    print('[%d|%d] samples' % (len(predict_golden['overall']), len(dataloader.data[data_key])))\n",
    "\n",
    "total = len(dataloader.data[data_key])\n",
    "slot_loss /= total\n",
    "intent_loss /= total\n",
    "print('%d samples %s' % (total, data_key))\n",
    "print('\\t slot loss:', slot_loss)\n",
    "print('\\t intent loss:', intent_loss)\n",
    "\n",
    "for x in ['intent', 'slot', 'overall']:\n",
    "    precision, recall, F1 = calculateF1(predict_golden[x])\n",
    "    print('-' * 20 + x + '-' * 20)\n",
    "    print('\\t Precision: %.2f' % (100 * precision))\n",
    "    print('\\t Recall: %.2f' % (100 * recall))\n",
    "    print('\\t F1: %.2f' % (100 * F1))\n",
    "\n",
    "output_file = os.path.join(output_dir, 'output.json')\n",
    "json.dump(predict_golden['overall'], open(output_file, 'w', encoding='utf-8'), indent=2, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.2 질적 평가 (Exercise)\n",
    "\n",
    "위 양적 평가에서 BERT가 slot과 intention 예측에서 뛰어난 정확도를 보이고 있는 것을 확인하셨을 것입니다.\n",
    "\n",
    "이번 파트에서는 문장 별 예측 결과를 실제로 출력해보고, 얼마나 모델이 정확하게 예측하고 있는지 실제로 확인해 봅시다.\n",
    "\n",
    "실습과제 구현을 시작하시기 전에, 다음 List와 Dictionary들을 각각 출력해보시기를 권장합니다.\n",
    "\n",
    "- `GT_slot`, `GT_sent` : 각각 모든 Ground-truth 슬롯 값들과 문장들의 정보가 저장되어 있는 List\n",
    "- `predict_golden` : slot/intent 예측값이 구조화되어 저장되어 있는 Dictionary\n",
    "\n",
    "최종적으로 다음과 같은 형식으로 예측 결과를 출력하세요.\n",
    "\n",
    "### Example #1\n",
    "\n",
    "Query(slot_loc): Yes(O) .(O) what(O) type(O) of(O) food(O) do(O) you(O) want(O) ?(O) \n",
    "\n",
    "SLOT   predict : []    / label: []\n",
    "\n",
    "INTENT predict : [['request', 'food', '?']]    / label: [['request', 'food', '?']]\n",
    "\n",
    "### Example #2\n",
    "\n",
    "Query(slot_loc): How(O) about(O) Italian(B-inform+food) ?(O) \n",
    "\n",
    "SLOT   predict : [['inform', 'food', 'Italian']]    / label: [['inform', 'food', 'italian']]\n",
    "\n",
    "INTENT predict : []    / label: []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\nQualitative results:\nQuery(slot_loc): Can(O)  you(O)  help(O)  me(O)  find(O)  a(O)  Russian(B-inform+food)  restaurant(O)  ?(O)  \nSLOT predict: [['inform', 'food', 'Russian']] / label: [['inform', 'food', 'russian']]\nINTENT predict: [] / label: []\n\nQuery(slot_loc): I(O)  'm(O)  sorry(O)  ,(O)  there(O)  are(O)  Russian(O)  restaurants(O)  .(O)  Do(O)  you(O)  have(O)  a(O)  secondary(O)  choice(O)  ?(O)  \nSLOT predict: [['inform', 'food', 'Russian']] / label: []\nINTENT predict: [['request', 'food', '?']] / label: [['request', 'food', '?']]\n\nQuery(slot_loc): Yes(O)  ,(O)  what(O)  about(O)  European(B-inform+food)  type(O)  food(O)  ?(O)  \nSLOT predict: [['inform', 'food', 'European']] / label: [['inform', 'food', 'european']]\nINTENT predict: [] / label: []\n\nQuery(slot_loc): Please(O)  wait(O)  a(O)  moment(O)  while(O)  I(O)  check(O)  for(O)  European(B-inform+food)  restaurants(O)  for(O)  you(O)  \nSLOT predict: [['inform', 'food', 'European']] / label: [['inform', 'food', 'European']]\nINTENT predict: [] / label: []\n\nQuery(slot_loc): If(O)  you(O)  find(O)  a(O)  European(O)  restaurant(O)  ,(O)  I(O)  would(O)  also(O)  like(O)  the(O)  address(O)  and(O)  phone(O)  number(O)  .(O)  \nSLOT predict: [['inform', 'food', 'European']] / label: []\nINTENT predict: [['request', 'address', '?'], ['request', 'phone', '?']] / label: [['request', 'phone', '?'], ['request', 'address', '?']]\n\n"
    }
   ],
   "source": [
    "print()\n",
    "print('Qualitative results:')\n",
    "for i in range(len(GT_sent[:5])):\n",
    "    print(\"Query(slot_loc): \", end=\"\")\n",
    "    for query, slot in zip(GT_sent[i], GT_slot[i]):\n",
    "        print(\"{}({}) \".format(query, slot), end=\" \")\n",
    "    print()\n",
    "    print(\"SLOT predict: {} / label: {}\".format(predict_golden['slot'][i]['predict'], predict_golden['slot'][i]['golden']))\n",
    "    print(\"INTENT predict: {} / label: {}\".format(predict_golden['intent'][i]['predict'], predict_golden['intent'][i]['golden']))\n",
    "    print()"
   ]
  }
 ]
}