{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic setting\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization Section\n",
    "## [Problem 1] Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "with open('./data/linear_regression.pickle', 'rb') as f:\n",
    "    X, y = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the dataset distribution    \n",
    "plt.scatter(X, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [P1.1] You should find an optimal linear regression model using Gradient Descent <font color=red>without Tensorflow.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The best fit model should have the smallest difference between the predicted and real values for all data.\n",
    "### Therefore, you should train the model to <font color=red>minimize the MSE(Mean Squared Error) loss.</font>\n",
    "### You should fill in the blank which is about a loss function and gradients of model parameters.\n",
    "\n",
    "**MSE loss**\n",
    "\\begin{equation*}\n",
    "\\left( \\frac{1}{n} \\sum_{i=1}^n (y_i - (wX_i + b))^2 \\right)\n",
    "\\end{equation*}\n",
    "\n",
    "**Gradients calculation**\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial L}{\\partial w} = -2 * \\frac{1}{n} \\sum_{i=1}^n (y_i - (wX_i + b)) * X_i\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial L}{\\partial b} = -2 * \\frac{1}{n} \\sum_{i=1}^n (y_i - (wX_i + b))\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setting\n",
    "epochs = 1000\n",
    "learning_rate = 1e-7\n",
    "\n",
    "# Model weights and bias parameters\n",
    "# f(x) = w * X + b\n",
    "w = 0.0\n",
    "b = 0.0\n",
    "\n",
    "# Store model parameters and loss for visualization\n",
    "w_list, b_list, loss_list = [], [], []\n",
    "\n",
    "# Perform Gradient Descent\n",
    "for i in range(epochs):\n",
    "    n = float(len(X))   # number of elements in X\n",
    "    \n",
    "    \n",
    "#################################################\n",
    "######## Hint: use +, -, *, /, sum() ########\n",
    "    # MSE loss\n",
    "    # a**2 = a * a = square of a, a**3 = a * a * a\n",
    "    loss =  (1 ? n) ? ???(y ? (w ? X ? b)**2)\n",
    "    \n",
    "    # derivative w.r.t to w\n",
    "    dw = -2 ? (1 ? n) ? ???((y ? (w ? X ? b)) ? X)\n",
    "    # derivative w.r.t to b\n",
    "    db = -2 ? (1 ? n) ? ???(y ? (w ? X ? b))\n",
    "#################################################\n",
    "    \n",
    "    # update w and b\n",
    "    w = w - learning_rate * dw\n",
    "    b = b - learning_rate * db\n",
    "    \n",
    "    w_list.append(w)\n",
    "    b_list.append(b)\n",
    "    loss_list.append(loss)\n",
    "\n",
    "print('Trained model weights : %.4f' % w)\n",
    "print('Trained model bias : %.4f' % b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's visualize the trained results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the trained linear regression model\n",
    "plt.scatter(X, y)   # scatter the original data\n",
    "y_pred = w * X + b\n",
    "plt.plot(X, y_pred, color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualize the intermediate trained model\n",
    "epochs_list = [1, 200, 400, 600, 800, 1000]\n",
    "\n",
    "for i in range(len(epochs_list)):\n",
    "    plt.scatter(X, y)   # scatter the original data\n",
    "    \n",
    "    # Load trained weights in specific epoch\n",
    "    epoch = epochs_list[i] - 1   # In python, all indexes start from 0\n",
    "    w = w_list[epoch]\n",
    "    b = b_list[epoch]\n",
    "    \n",
    "    y_pred = w * X + b\n",
    "    plt.plot(X, y_pred, color='red')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the change of loss\n",
    "plt.plot(loss_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [P1.2] You should find an optimal linear regression model using Gradient Descent <font color=red>with Tensorflow.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setting\n",
    "epochs = 1000\n",
    "learning_rate = 1e-7\n",
    "\n",
    "# Model weights and bias parameters\n",
    "w = tf.Variable(0.0)\n",
    "b = tf.Variable(0.0)\n",
    "\n",
    "# Perform Gradient Descent\n",
    "for i in range(epochs):\n",
    "    \n",
    "    \n",
    "#################################################\n",
    "\n",
    "    # Define MSE loss function (Hint: tf.GradientTape(), tf.reduce_mean())\n",
    "    with ??? as tape:\n",
    "        loss = ???((y ? (w ? X ? b))**2)\n",
    "        \n",
    "    # Get gradients of parameters (Hint: tape.gradient())\n",
    "    dw, db = ???(?, [?, ?])   # dloss_dw, dloss_db\n",
    "    \n",
    "    # Update model weights and bias (Hint : assign_sub())\n",
    "    w.???(? * ?)\n",
    "    b.???(? * ?)\n",
    "    \n",
    "#################################################\n",
    "\n",
    "\n",
    "# Convert parameters type from tensor to numpy\n",
    "w = w.numpy()\n",
    "b = b.numpy()\n",
    "\n",
    "print('Trained model weights : %.4f' % w)\n",
    "print('Trained model bias : %.4f' % b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's visualize the trained results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the trained linear regression model\n",
    "plt.scatter(X, y)   # scatter the original data\n",
    "y_pred = w * X + b\n",
    "plt.plot(X, y_pred, color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trained model parameters are exactly same.\n",
    "### If you use Tensorflow, you don't have to calculate gradients by hand!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Problem 2] Polynomial Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In real world, there are a lot of complex data which are difficult to express linearly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "with open('./data/polynomial_regression.pickle', 'rb') as f:\n",
    "    X, y = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the dataset distribution    \n",
    "plt.scatter(X, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [P2.1] First, you need to find an optimal linear regression model using Gradient Descent <font color=red>with Tensorflow.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setting\n",
    "epochs = 1000\n",
    "learning_rate = 1e-7\n",
    "\n",
    "# Model weights and bias parameters\n",
    "w = tf.Variable(0.0)\n",
    "b = tf.Variable(0.0)\n",
    "\n",
    "# Perform Gradient Descent\n",
    "for i in range(epochs):\n",
    "    \n",
    "    \n",
    "#################################################\n",
    "############# Hint : Same as before #############\n",
    "\n",
    "    # Define MSE loss function (Hint: tf.GradientTape(), tf.reduce_mean())\n",
    "    with ??? as tape:\n",
    "        loss = ???((y ? (w ? X ? b))**2)\n",
    "    \n",
    "    # Get gradients of parameters (Hint: tape.gradient())\n",
    "    dw, db = ???(?, [?, ?])   # dloss_dw, dloss_db\n",
    "    \n",
    "    # Update model weights and bias (Hint : assign_sub())\n",
    "    w.???(? * ?)\n",
    "    b.???(? * ?)\n",
    "    \n",
    "#################################################\n",
    "\n",
    "\n",
    "# Convert parameters type from tensor to numpy\n",
    "w = w.numpy()\n",
    "b = b.numpy()\n",
    "\n",
    "print('Trained model weights : %.4f' % w)\n",
    "print('Trained model bias : %.4f' % b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's visualize the trained results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualize the trained linear regression model\n",
    "plt.scatter(X, y)   # scatter the original data\n",
    "y_pred = w * X + b\n",
    "plt.plot(X, y_pred, color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The linear regression model does not seem to fit well.\n",
    "### Maybe, a more complex model can better represent the given dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [P2.2] Next, you need to find an optimal <font color=red>polynomial regression model</font> using Gradient Descent <font color=red>with Tensorflow.</font>\n",
    "\n",
    "### Learn a cubic regression model. (3차 함수)\n",
    "\\begin{equation*}\n",
    "f(x) = w_1X^3 + w_2X^2 + w_3X + b\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setting\n",
    "epochs = 1000\n",
    "learning_rate = 1e-14\n",
    "\n",
    "\n",
    "#################################################\n",
    "### Define model weights and bias parameters ###\n",
    "w1 = \n",
    "w2 = \n",
    "w3 = \n",
    "b = \n",
    "\n",
    "# Perform Gradient Descent\n",
    "for i in range(epochs):\n",
    "    \n",
    "####### Hint : Consider multiple weights  #######\n",
    "\n",
    "    # Define MSE loss function (Hint: tf.GradientTape(), tf.reduce_mean())\n",
    "    with ??? as tape:\n",
    "        loss = ???((y ? (w1 ? X**3 ? w2 ? X**2 ? w3 ? X ? b))**2)\n",
    "    \n",
    "    # Get gradients of parameters (Hint: tape.gradient())\n",
    "    # dloss_dw1, dloss_dw2, dloss_dw3, dloss_db\n",
    "    dw1, dw2, dw3, db = ???(?, [?, ?, ?, ?])   \n",
    "    \n",
    "    # Update model weights and bias (Hint : assign_sub())\n",
    "    w1.???(? * ?)\n",
    "    w2.???(? * ?)\n",
    "    w3.???(? * ?)\n",
    "    b.???(? * ?)\n",
    "    \n",
    "#################################################\n",
    "\n",
    "# Convert parameters type from tensor to numpy\n",
    "w1 = w1.numpy()\n",
    "w2 = w2.numpy()\n",
    "w3 = w3.numpy()\n",
    "b = b.numpy()\n",
    "\n",
    "# Print the trained parameters value\n",
    "print('Trained model weights 1 : %.4f' % w1)\n",
    "print('Trained model weights 2 : %.4f' % w2)\n",
    "print('Trained model weights 3 : %.4f' % w3)\n",
    "print('Trained model bias : %.4f' % b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's visualize the trained results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the trained linear regression model\n",
    "plt.scatter(X, y)   # scatter the original data\n",
    "y_pred = w1*X**3 + w2*X**2 + w3*X + b\n",
    "plt.plot(X, y_pred, color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Problem 3] Logistic Regression (Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "with open('./data/logistic_regression.pickle', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    \n",
    "data.head()   # show the 5 elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, we are going to train logistic regression model to classify whether a user purchased or not, based on the 'Age' information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract 'Age' and 'Purchased' data\n",
    "X = data['Age'].to_numpy(dtype=np.float32)\n",
    "y = data['Purchased'].to_numpy()\n",
    "\n",
    "# Normalize 'Age' value\n",
    "# X의 평균값이 0이 될 수 있도록 만들어, 학습의 안정성을 높이는 방법.\n",
    "def normalize(X):\n",
    "    return X - X.mean()\n",
    "X = normalize(X)\n",
    "\n",
    "# Visualizing the dataset\n",
    "plt.scatter(X, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You can see the tendency <font color=red> not to purchase(y=0)</font> for the younger and <font color=red>to purchase(y=1)</font> for the older."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [P3.1] You should find an optimal logistic regression model using Gradient Descent <font color=red>without Tensorflow.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The shape of Logistic function is S-shaped curve and it maps to a value between 0 and 1.\n",
    "### Logistic regression model can classify the data based on 0.5 value from the perspective of probability.\n",
    "### You can train a model to <font color=red>minimize -likelihood function(=maximize likelihood function).</font>\n",
    "\n",
    "**Logistic regression**\n",
    "\\begin{equation*}\n",
    "P(y_i=1|X_i) = \\frac{1}{1 + e^{-(wX_i + b)}}\n",
    "\\end{equation*}\n",
    "\n",
    "**Likelihood loss**\n",
    "\\begin{equation*}\n",
    "-\\frac{1}{n} \\sum_{i=1}^n (y_i * log(P(y_i=1|X_i)) + (1 - y_i) * log(1 - P(y_i=1|X_i)))\n",
    "\\end{equation*}\n",
    "\n",
    "**Gradients calculation**\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial L}{\\partial w} = \\frac{1}{n} \\sum_{i=1}^n ((P(y_i=1|X_i) - y_i) * X_i)\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial L}{\\partial w} = \\frac{1}{n} \\sum_{i=1}^n (P(y_i=1|X_i) - y_i)\n",
    "\\end{equation*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training setting\n",
    "epochs = 1000\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# Model weights and bias parameters\n",
    "w = 0.0\n",
    "b = 0.0\n",
    "\n",
    "# Perform Gradient Descent\n",
    "for i in range(epochs):\n",
    "    n = float(len(X))   # number of elements in X\n",
    "    \n",
    "    \n",
    "#################################################\n",
    "######## Hint: use +, -, *, /, **, sum(), #######\n",
    "########           np.exp(), np.log() ###########\n",
    "    y_pred = 1 ? (1 ? ???(-(w ? X ? b)))  # Logistic regression\n",
    "    loss = -(1 ? n) ? ???(y ? ???(y_pred) ? (1 - y) ? ???(1 - y_pred))  # Likelihood loss\n",
    "    \n",
    "    dw = (1 ? n) ? ???((y_pred ? y) ? X)  # gradients w.r.t to w\n",
    "    db = (1 ? n) ? ???(y_pred ? y)  # gradients w.r.t to b\n",
    "    \n",
    "    # Update model weights and bias\n",
    "    w = w - ??? * ?\n",
    "    b = b - ??? * ?\n",
    "    \n",
    "#################################################\n",
    "\n",
    "\n",
    "print('Trained model weights : %.4f' % w)\n",
    "print('Trained model bias : %.4f' % b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's visualize the trained results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualize the trained linear regression model\n",
    "plt.scatter(X, y)   # scatter the original data\n",
    "y_pred = 1 / (1 + np.exp(-(w*X + b)))\n",
    "plt.scatter(X, y_pred, color='red')\n",
    "plt.axhline(y=0.5, color='orange', linestyle='--')   # show 0.5 threshold line\n",
    "\n",
    "# Accurate results are colored as orange\n",
    "index1 = (y == 0) * (y_pred < 0.5)\n",
    "index2 = (y == 1) * (y_pred > 0.5)\n",
    "index = index1 + index2\n",
    "plt.scatter(X[index], y[index], color='orange')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [P3.2] You should find an optimal logistic regression model using Gradient Descent <font color=red>with Tensorflow.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setting\n",
    "epochs = 1000\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# Model weights and bias parameters\n",
    "w = tf.Variable(0.0)\n",
    "b = tf.Variable(0.0)\n",
    "\n",
    "# Perform Gradient Descent\n",
    "for i in range(epochs):\n",
    "    \n",
    "    \n",
    "#################################################\n",
    "\n",
    "    # Define Likelihood loss function (Hint: tf.GradientTape(), tf.reduce_mean())\n",
    "    with ??? as tape:\n",
    "        # Hint: use tf.exp()\n",
    "        y_pred = 1 ? (1 ? ???(-(w ? X ? b)))\n",
    "        # Hint: use tf.math.log\n",
    "        loss = -???(y ? ???(y_pred) + (1 - y) ? ???(1 - y_pred))\n",
    "    \n",
    "    \n",
    "    # Get gradients of parameters (Hint: tape.gradient())\n",
    "    dw, db = ???(?, [?, ?])   # dloss_dw, dloss_db\n",
    "    \n",
    "    \n",
    "    # Update model weights and bias (Hint : assign_sub())\n",
    "    w.???(? * ?)\n",
    "    b.???(? * ?)\n",
    "    \n",
    "    \n",
    "#################################################\n",
    "\n",
    "\n",
    "# Convert parameters type from tensor to numpy\n",
    "w = w.numpy()\n",
    "b = b.numpy()\n",
    "\n",
    "print('Trained model weights : %.4f' % w)\n",
    "print('Trained model bias : %.4f' % b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's visualize the trained results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the trained linear regression model\n",
    "plt.scatter(X, y)   # scatter the original data\n",
    "y_pred = 1 / (1 + tf.exp(-(w*X + b)))\n",
    "plt.scatter(X, y_pred, color='red')\n",
    "plt.axhline(y=0.5, color='orange', linestyle='--')   # show 0.5 threshold line\n",
    "\n",
    "# Accurate results are colored as orange\n",
    "index1 = (y == 0) * (y_pred.numpy() < 0.5)\n",
    "index2 = (y == 1) * (y_pred.numpy() > 0.5)\n",
    "index = index1 + index2\n",
    "plt.scatter(X[index], y[index], color='orange')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
