{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "이번 실습에서는 좀 더 복잡한 데이터와 모델을 최적화해보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from time import time\n",
    "\n",
    "seed = 2020\n",
    "np.random.seed(seed)\n",
    "tf.random.set_random_seed(seed) # for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset info\n",
    "cifar-10은 10가지 카테고리의 32x32 컬러(RGB) 이미지들로 구성되어 있으며 50,000장의 training data, 10,000장의 test data로 이루어져 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "x_train = x_train.astype(np.float32) / 255.\n",
    "y_train = y_train.astype(np.int64)\n",
    "x_test = x_test.astype(np.float32) / 255.\n",
    "y_test = y_test.astype(np.int64)\n",
    "\n",
    "print(f\"input shape: {x_train.shape}\")\n",
    "print(f\"label shape: {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(optimizer='adam', init='glorot_uniform'):\n",
    "    model = keras.Sequential([\n",
    "        # input layer\n",
    "        keras.layers.Flatten(input_shape=(32, 32, 3)),\n",
    "        keras.layers.Dropout(0.5),\n",
    "        # hidden layers\n",
    "        keras.layers.Dense(1024,kernel_initializer=init),\n",
    "        keras.layers.Activation('relu'),\n",
    "        keras.layers.Dropout(0.2),\n",
    "        keras.layers.Dense(1024,kernel_initializer=init),\n",
    "        keras.layers.Activation('relu'),\n",
    "        keras.layers.Dropout(0.2),\n",
    "        # output layer\n",
    "        keras.layers.Dense(10,kernel_initializer=init),\n",
    "        keras.layers.Activation('softmax')\n",
    "    ])\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy']) \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = create_model()\n",
    "model.summary()\n",
    "hist = model.fit(x_train, y_train, \n",
    "                 batch_size=1024, epochs=5,\n",
    "                 validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "def plot_fn(labels, train_loss_histories, train_acc_histories, test_acc_histories):\n",
    "    fig = plt.figure(figsize=(18,5))\n",
    "    ax1 = fig.add_subplot(1, 3, 1)\n",
    "    ax2 = fig.add_subplot(1, 3, 2)\n",
    "    ax3 = fig.add_subplot(1, 3, 3)\n",
    "    for label, train_loss_history, train_acc_history, test_acc_history in zip(labels, train_loss_histories, train_acc_histories, test_acc_histories):\n",
    "        ax1.plot(train_loss_history, label=str(label))\n",
    "        ax2.plot(train_acc_history, label=str(label))\n",
    "        ax3.plot(test_acc_history, label=str(label))\n",
    "        \n",
    "    ax1.set_xlabel('Batch #')\n",
    "    ax1.set_ylabel('Training Loss [entropy]')\n",
    "    ax2.set_xlabel('Batch #')\n",
    "    ax2.set_ylabel('Training Accuracy')\n",
    "    ax3.set_xlabel('Batch #')\n",
    "    ax3.set_ylabel('Test Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show\n",
    "plot_fn(['CIFAR-10'], [hist.history['loss']], [hist.history['acc']], [hist.history['val_acc']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Optimization\n",
    "scikit learn의 model_selection 모듈을 사용하면 간단하게 하이퍼파라미터 서치를 수행할 수 있습니다.\n",
    "\n",
    "GridSearchCV는 K-fold cross validation을 기반으로 주어진 parameter grid를 모두 탐색하면서 최적의 하이퍼 파라미터를 찾아냅니다.\n",
    "\n",
    "RandomizedSearchCV는 parameter grid를 모두 탐색하지 않고 랜덤하게 뽑은 몇 가지 하이퍼 파라미터 조합에 대해서만 탐색을 수행합니다.\n",
    "\n",
    "자세한 내용은 [Sklearn Docs](https://scikit-learn.org/stable/modules/grid_search.html)에서 찾아볼 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "model = KerasClassifier(build_fn=create_model) # wrapper\n",
    "\n",
    "# opt parameter candidates\n",
    "optimizers = ['sgd', 'adam']\n",
    "init = ['glorot_uniform', 'normal']\n",
    "epochs = np.array([5, 10])\n",
    "batches = np.array([128, 256, 1024])\n",
    "# model parameters\n",
    "params={}\n",
    "\n",
    "# parameter grid generation\n",
    "param_grid = dict(optimizer=optimizers, init=init, nb_epoch=epochs, batch_size=batches)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=5) # 5-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 각 hyperparameter에 따른 모델들을 학습합니다\n",
    "start=time()\n",
    "grid_result = grid.fit(x_train, y_train, **params)\n",
    "end=time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 탐색된 모델들의 성능을 비교합니다\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "print(\"total time:\",end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가장 좋은 validation accuracy를 보인 하이퍼파라미터로 model을 create 하고 training data를 다시 학습합니다\n",
    "# 위에서 보인 가장 좋은 Hyperparameter를 직접 입력해주어도 되고, grid_result.best_params_['**'] 이라는 함수를 사용해도 됩니다.\n",
    "\n",
    "best_model = ##TODO : Your Code ##\n",
    "best_hist = ##TODO : Your Code ##\n",
    "\n",
    "plot_fn(['CIFAR-10'], [best_hist.history['loss']], [best_hist.history['acc']], [best_hist.history['val_acc']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
