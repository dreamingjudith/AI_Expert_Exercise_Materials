{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Dropout\n",
    "\n",
    "이번 실습에서는 Dropout에 대해서 알아보고자 합니다. Dropout은 대부분의 모델에서 사용할 수 있는 정규화 방법입니다. \n",
    "\n",
    "![image.png](images/04_dropout.png)\n",
    "\n",
    "여러 개의 subnetworks를 앙상블하는 방법으로 해석할 수 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random \n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "seed = 2020\n",
    "random.seed(seed)\n",
    "np.random.seed(seed=seed)\n",
    "tf.random.set_random_seed(seed)\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape([-1, 28 * 28]) \n",
    "x_test = x_test.reshape([-1, 28 * 28])\n",
    "\n",
    "m = np.random.randint(0, high=60000, size=1100, dtype=np.int64)\n",
    "x_train = x_train[m]\n",
    "y_train = y_train[m]\n",
    "\n",
    "i = np.arange(1100)\n",
    "np.random.shuffle(i)\n",
    "x_train = x_train[i]\n",
    "y_train = y_train[i]\n",
    "\n",
    "x_valid = x_train[:100]\n",
    "y_valid = y_train[:100]\n",
    "\n",
    "x_train = x_train[100:]\n",
    "y_train = y_train[100:]\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 28 * 28])\n",
    "y = tf.placeholder(tf.int32, [None])\n",
    "training = tf.placeholder(tf.bool)\n",
    "\n",
    "n_units = [28 * 28, 512, 512, 10]\n",
    "\n",
    "weights, biases = [], []\n",
    "for i, (n_in, n_out) in enumerate(zip(n_units[:-1], n_units[1:])):\n",
    "    stddev = math.sqrt(2 / n_in) # Kaiming He Initialization\n",
    "    weight = tf.Variable(tf.random.truncated_normal([n_in, n_out], mean=0, stddev=stddev))\n",
    "    bias = tf.Variable(tf.zeros([n_out]))\n",
    "    weights.append(weight)\n",
    "    biases.append(bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout을 사용하기 위해서는 activation 이후에 dropout layer를 추가하면 됩니다. activation 전에하는 것보다 후에하는 것이 더 좋다고 알려져 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "WARNING:tensorflow:From <ipython-input-2-970e964a7a3b>:6: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse keras.layers.dropout instead.\nWARNING:tensorflow:Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7f481c184dd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7f481c184dd0>>: AssertionError: Bad argument number for Name: 3, expecting 4\nWARNING: Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7f481c184dd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7f481c184dd0>>: AssertionError: Bad argument number for Name: 3, expecting 4\nWARNING:tensorflow:Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7f481c1c63d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7f481c1c63d0>>: AssertionError: Bad argument number for Name: 3, expecting 4\nWARNING: Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7f481c1c63d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7f481c1c63d0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
    }
   ],
   "source": [
    "layer = x \n",
    "for i, (weight, bias) in enumerate(zip(weights, biases)):\n",
    "    layer = tf.matmul(layer, weight) + bias\n",
    "    if i < len(weights) - 1:\n",
    "        layer = tf.nn.tanh(layer)  \n",
    "        layer = tf.layers.dropout(layer, rate=0.5, training=training)\n",
    "y_hat = layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다시 다른 부분들은 이전 실습과 동일하게 진행해 줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "10 0.4879 0.5438 0.6256 0.8530 0.8700 0.8016\n20 0.2819 0.4866 0.4834 0.9050 0.8500 0.8492\n30 0.1736 0.4110 0.4387 0.9400 0.8500 0.8655\n40 0.1052 0.3949 0.4048 0.9700 0.8600 0.8801\n50 0.0630 0.4046 0.4002 0.9840 0.8700 0.8837\n60 0.0436 0.3893 0.4023 0.9890 0.8800 0.8856\n70 0.0311 0.4462 0.4046 0.9930 0.8600 0.8884\n80 0.0218 0.4085 0.4027 0.9950 0.8600 0.8916\n90 0.0146 0.3706 0.4045 0.9980 0.8900 0.8886\n100 0.0115 0.3774 0.4055 0.9990 0.8900 0.8929\n110 0.0077 0.3819 0.4061 1.0000 0.9000 0.8933\n120 0.0070 0.4328 0.4165 0.9990 0.8800 0.8918\n130 0.0073 0.3764 0.4221 0.9990 0.8900 0.8934\n140 0.0053 0.4037 0.4293 0.9990 0.9000 0.8954\n150 0.0047 0.4069 0.4248 0.9980 0.8800 0.8923\n160 0.0034 0.4199 0.4328 1.0000 0.8900 0.8924\n170 0.0043 0.3288 0.4259 0.9990 0.9100 0.8937\n180 0.0037 0.3821 0.4253 1.0000 0.8900 0.8969\n190 0.0030 0.4297 0.4213 1.0000 0.8900 0.8970\n200 0.0036 0.4800 0.4266 1.0000 0.8800 0.8982\n210 0.0028 0.4422 0.4272 1.0000 0.8800 0.8981\n220 0.0022 0.3734 0.4243 1.0000 0.8900 0.8996\n230 0.0021 0.4082 0.4310 1.0000 0.8900 0.8968\n240 0.0026 0.4532 0.4404 1.0000 0.8800 0.8973\n250 0.0029 0.4592 0.4459 1.0000 0.8700 0.8987\n260 0.0025 0.4889 0.4335 1.0000 0.8700 0.9011\n270 0.0048 0.4743 0.4420 0.9980 0.9000 0.8983\n0.8937\n"
    }
   ],
   "source": [
    "\n",
    "y_hot = tf.one_hot(y, 10)\n",
    "costs = tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "        labels=y_hot, logits=y_hat)\n",
    "cross_entropy_loss = tf.reduce_mean(costs)\n",
    "loss = cross_entropy_loss\n",
    "\n",
    "accuracy = tf.count_nonzero(\n",
    "        tf.cast(tf.equal(tf.argmax(y_hot, 1), tf.argmax(y_hat, 1)),\n",
    "                tf.int64)) / tf.cast(tf.shape(y_hot)[0], tf.int64)\n",
    "\n",
    "extra_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(extra_ops):\n",
    "    optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "    train_op = optimizer.minimize(loss)\n",
    "    \n",
    "gpu_options = tf.GPUOptions()\n",
    "gpu_options.allow_growth = True\n",
    "session = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "session.run(tf.global_variables_initializer())\n",
    "\n",
    "max_valid_epoch_idx = 0\n",
    "max_valid_accuracy = 0.0\n",
    "final_test_accuracy = 0.0\n",
    "\n",
    "import time \n",
    "times = []\n",
    "for epoch_idx in range(1, 1000 + 1):\n",
    "    start_time = time.time()\n",
    "    session.run(\n",
    "            train_op,\n",
    "            feed_dict={\n",
    "                x: x_train,\n",
    "                y: y_train,\n",
    "                training: True\n",
    "            })\n",
    "    times.append(time.time() - start_time)\n",
    "    \n",
    "    if epoch_idx % 10 == 0:\n",
    "        train_loss_value, train_accuracy_value = session.run(\n",
    "            [loss, accuracy],\n",
    "            feed_dict={\n",
    "                x: x_train,\n",
    "                y: y_train,\n",
    "                training: False\n",
    "            })\n",
    "        \n",
    "        valid_loss_value, valid_accuracy_value = session.run(\n",
    "            [loss, accuracy],\n",
    "            feed_dict={\n",
    "                x: x_valid,\n",
    "                y: y_valid,\n",
    "                training: False\n",
    "            })\n",
    "            \n",
    "        test_loss_value, test_accuracy_value = session.run(\n",
    "            [loss, accuracy],\n",
    "            feed_dict={\n",
    "                x: x_test,\n",
    "                y: y_test,\n",
    "                training: False\n",
    "            })\n",
    "\n",
    "        print(epoch_idx, '%.4f' % train_loss_value, '%.4f' % valid_loss_value, '%.4f' % test_loss_value, '%.4f' % train_accuracy_value, '%.4f' % valid_accuracy_value, '%.4f' % test_accuracy_value)\n",
    "        \n",
    "        if max_valid_accuracy < valid_accuracy_value:\n",
    "            max_valid_accuracy = valid_accuracy_value \n",
    "            max_valid_epoch_idx = epoch_idx\n",
    "            final_test_accuracy = test_accuracy_value\n",
    "            \n",
    "    # Early Stop\n",
    "    if max_valid_epoch_idx + 100 < epoch_idx:\n",
    "        break\n",
    "        \n",
    "print(final_test_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "87.10% -> 89.87% 성능이 향상됨을 확인할 수 있습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 연습문제\n",
    "\n",
    "Q1. Dropout Rate을 바꿔가면서 성능을 확인해봅시다. \n",
    "\n",
    "Q2. tf.nn.dropout를 이용해서 dropout을 다시 구현해봅시다. tf.layers.dropout을 사용할 때와 비슷한 성능이 나오나요?\n",
    "만약 비슷한 성능이 나오지 않는다면 이유가 무엇일까요?\n",
    "(참고: [API 문서](https://www.tensorflow.org/api_docs/python/tf/nn/dropout))\n",
    "\n",
    "**A2.** tf.layers.dropout은 training 아규먼트를 이용해 training 중일 때만 dropout을 하고 inference 중엔 dropout을 끈다\n",
    "\n",
    "Q3. tf.nn.dropout을 이용해서 tf.layers.dropout과 비슷한 성능이 나오기 위해서 어떻게 코드를 작성해야할까요?  \n",
    "(정답은 [여기](01_06_dropout_Q2_answer.txt)를 참고하세요.)\n",
    "\n",
    "Q4. (도전과제) [tf.layers.dropout API 문서](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/layers/dropout)에 보면 다음과 같은 서술이 있습니다.\n",
    "```\n",
    "The units that are kept are scaled by 1 / (1 - rate)\n",
    "```\n",
    "이 구현이 왜 필요한지 생각해봅시다.\n",
    "\n",
    "**A4.** training 중에는 일부 weight를 drop해주는데 test 시에는 전체 weight로부터 값을 받기 때문에 training 때와 scale이 달라지기 때문에 이를 보정해야 한다\n",
    "\n",
    "Q5. tf.keras.layers.GaussianDropout를 이용해 Gaussian Dropout을 구현해봅시다. 더 빠른 수렴이 가능해 지나요? Gaussian Dropout에 대해서도 dropout rate을 변경해보면서 성능을 확인해봅시다.\n",
    "(정답은 [여기](01_06_dropout_Q4_answer.txt)에서 확인할 수 있습니다.)\n",
    "\n",
    "\n",
    "주의사항! 코드를 수정한 이후에는 **Kernel > Restart & Run All** 을 통해 네트워크를 처음부터 다시 학습시켜 주세요. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 다음 실습 \n",
    "\n",
    "이제 다음 [실습](01_07_augmentation.ipynb)에서는 Augmentation을 적용하는 방법에 대해 알아보도록 하겠습니다. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('kaist_mli': conda)",
   "language": "python",
   "name": "python_defaultSpec_1595338644844"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}