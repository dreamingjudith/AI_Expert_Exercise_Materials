{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Input Layer Stabilization\n",
    "\n",
    "Input Layer Stabilization은 가장 간단하지만 파워풀한 정규화 방법입니다. 이전 실습과 동일한 부분들은 일단 동일하게 적용해줍니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random \n",
    "import os \n",
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "seed = 2020\n",
    "random.seed(seed)\n",
    "np.random.seed(seed=seed)\n",
    "tf.random.set_random_seed(seed)\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape([-1, 28 * 28])\n",
    "x_test = x_test.reshape([-1, 28 * 28])\n",
    "\n",
    "m = np.random.randint(0, high=60000, size=1100, dtype=np.int64)\n",
    "x_train = x_train[m]\n",
    "y_train = y_train[m]\n",
    "\n",
    "i = np.arange(1100)\n",
    "np.random.shuffle(i)\n",
    "x_train = x_train[i]\n",
    "y_train = y_train[i]\n",
    "\n",
    "x_valid = x_train[:100]\n",
    "y_valid = y_train[:100]\n",
    "\n",
    "x_train = x_train[100:]\n",
    "y_train = y_train[100:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 여러 Input Layer Stabilization 방식 중 Standardization을 적용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardization\n",
    "mean, std = np.mean(x_train), np.std(x_train)\n",
    "x_train = (x_train.astype(np.float64)- mean) / std\n",
    "x_valid = (x_valid.astype(np.float64)- mean) / std\n",
    "x_test = (x_test.astype(np.float64) - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0-1 normalizaion\n",
    "max, min = 255., 0.\n",
    "x_train = (x_train.astype(np.float64) - min) / (max - min)\n",
    "x_valid = (x_valid.astype(np.float64) - min) / (max - min)\n",
    "x_test = (x_test.astype(np.float64) - min) / (max - min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation set과 Test set은 트레이닝할 때 주어지지 않는다고 가정해야 하므로, Training 데이터만을 이용해 평균과 표준편차를 계산하고 validation, test 셋도 동일하게 표준화 해줍니다. (RGB 채널이 있는 데이터의 경우에는 RGB 채널별로 표준화 하는 것이 일반적입니다.)\n",
    "\n",
    "이제 네트워크를 이전 실습과 동일하게 초기화 합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, 28 * 28])\n",
    "y = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "n_units = [28 * 28, 512, 512, 10]\n",
    "\n",
    "weights, biases = [], []\n",
    "for i, (n_in, n_out) in enumerate(zip(n_units[:-1], n_units[1:])):\n",
    "    stddev = math.sqrt(2 / n_in) # Kaiming He Initialization\n",
    "    weight = tf.Variable(tf.random.truncated_normal([n_in, n_out], mean=0, stddev=stddev))\n",
    "    bias = tf.Variable(tf.zeros([n_out]))\n",
    "    weights.append(weight)\n",
    "    biases.append(bias)\n",
    "    \n",
    "layer = x \n",
    "for i, (weight, bias) in enumerate(zip(weights, biases)):\n",
    "    layer = tf.matmul(layer, weight) + bias\n",
    "    if i < len(weights) - 1:\n",
    "        layer = tf.nn.tanh(layer)        \n",
    "y_hat = layer\n",
    "\n",
    "y_hot = tf.one_hot(y, 10)\n",
    "costs = tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "        labels=y_hot, logits=y_hat)\n",
    "cross_entropy_loss = tf.reduce_mean(costs)\n",
    "loss = cross_entropy_loss \n",
    "\n",
    "accuracy = tf.count_nonzero(\n",
    "        tf.cast(tf.equal(tf.argmax(y_hot, 1), tf.argmax(y_hat, 1)),\n",
    "                tf.int64)) / tf.cast(tf.shape(y_hot)[0], tf.int64)\n",
    "\n",
    "extra_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(extra_ops):\n",
    "    optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "    train_op = optimizer.minimize(loss)\n",
    "    \n",
    "gpu_options = tf.GPUOptions()\n",
    "gpu_options.allow_growth = True\n",
    "session = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "session.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습을 진행합니다. 총 1000번의 Epoch을 수행합니다. 총 100 Epoch 동안 성능 개선이 이뤄지지 않으면 Early Stop 합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0 1.6278 1.7560 1.6327 0.4680 0.4600 0.4691\n10 0.2452 0.5162 0.4144 0.9310 0.8300 0.8754\n20 0.0719 0.4033 0.3618 0.9920 0.8800 0.8972\n30 0.0247 0.4117 0.3563 1.0000 0.8800 0.8982\n40 0.0122 0.4177 0.3600 1.0000 0.8800 0.9023\n50 0.0074 0.4359 0.3647 1.0000 0.8800 0.9026\n60 0.0053 0.4481 0.3694 1.0000 0.8700 0.9034\n70 0.0042 0.4532 0.3733 1.0000 0.8800 0.9032\n80 0.0035 0.4565 0.3760 1.0000 0.8800 0.9029\n90 0.0030 0.4582 0.3794 1.0000 0.8800 0.9020\n100 0.0026 0.4610 0.3819 1.0000 0.8800 0.9022\n110 0.0023 0.4632 0.3844 1.0000 0.8800 0.9023\n120 0.0021 0.4660 0.3869 1.0000 0.8800 0.9017\nEarly stopping Accuracy: 0.8972\n"
    }
   ],
   "source": [
    "max_valid_epoch_idx = 0\n",
    "max_valid_accuracy = 0.0\n",
    "final_test_accuracy = 0.0\n",
    "for epoch_idx in range(0, 1000):\n",
    "    session.run(\n",
    "            train_op,\n",
    "            feed_dict={\n",
    "                x: x_train,\n",
    "                y: y_train\n",
    "            })\n",
    "    \n",
    "    if epoch_idx % 10 == 0:\n",
    "        train_loss_value, train_accuracy_value = session.run(\n",
    "            [loss, accuracy],\n",
    "            feed_dict={\n",
    "                x: x_train,\n",
    "                y: y_train\n",
    "            })\n",
    "        \n",
    "        valid_loss_value, valid_accuracy_value = session.run(\n",
    "            [loss, accuracy],\n",
    "            feed_dict={\n",
    "                x: x_valid,\n",
    "                y: y_valid\n",
    "            })\n",
    "            \n",
    "        test_loss_value, test_accuracy_value = session.run(\n",
    "            [loss, accuracy],\n",
    "            feed_dict={\n",
    "                x: x_test,\n",
    "                y: y_test\n",
    "            })\n",
    "\n",
    "        print(epoch_idx, '%.4f' % train_loss_value, '%.4f' % valid_loss_value, '%.4f' % test_loss_value, '%.4f' % train_accuracy_value, '%.4f' % valid_accuracy_value, '%.4f' % test_accuracy_value)\n",
    "        \n",
    "        if max_valid_accuracy < valid_accuracy_value:\n",
    "            max_valid_accuracy = valid_accuracy_value \n",
    "            max_valid_epoch_idx = epoch_idx\n",
    "            final_test_accuracy = test_accuracy_value\n",
    "            \n",
    "    # Early Stop\n",
    "    if max_valid_epoch_idx + 100 < epoch_idx:\n",
    "        break\n",
    "        \n",
    "print(\"Early stopping Accuracy:\", final_test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리는 이번 실습에서 정말 간단한 표준화 만으로도 87.10% -> 89.72% 로 성능이 2.62% 향상한 것을 확인할 수 있었습니다.\n",
    "\n",
    "### 연습 문제\n",
    "\n",
    "Q1. 표준화를 zero-one Normalization으로 바꿔보고 성능을 측정해보세요. \n",
    "(힌트는 [여기](01_03_input_layer_stabilization_Q1_hint.txt)에서 확인하실 수 있습니다.)\n",
    "\n",
    "\n",
    "주의사항! 코드를 수정한 이후에는 **Kernel > Restart & Run All** 을 통해 네트워크를 처음부터 다시 학습시켜 주세요. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 다음 실습\n",
    "\n",
    "다음 [실습](01_04_loss_penalty.ipynb)에서는 Loss Penalty를 통해 딥 모델을 정규화하는 방법을 알아보고자 합니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('kaist_mli': conda)",
   "language": "python",
   "name": "python_defaultSpec_1595336044184"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}