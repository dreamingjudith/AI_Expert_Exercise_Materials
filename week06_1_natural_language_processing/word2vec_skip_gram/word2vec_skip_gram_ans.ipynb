{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC\n",
    "from typing import List, Dict, Tuple, Set\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "from dataset import SkipgramDataset\n",
    "\n",
    "#############################################\n",
    "# Helper functions below. DO NOT MODIFY!    #\n",
    "#############################################\n",
    "\n",
    "\n",
    "class Word2Vec(torch.nn.Module, ABC):\n",
    "    \"\"\"\n",
    "    A helper class that wraps your word2vec losses.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_tokens: int, word_dimension: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.center_vectors = torch.nn.Parameter(torch.empty([n_tokens, word_dimension]))\n",
    "        self.outside_vectors = torch.nn.Parameter(torch.empty([n_tokens, word_dimension]))\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        torch.nn.init.normal_(self.center_vectors.data)\n",
    "        torch.nn.init.normal_(self.outside_vectors.data)\n",
    "\n",
    "\n",
    "class NaiveWord2Vec(Word2Vec):\n",
    "    def forward(self, center_word_index: torch.Tensor, outside_word_indices: torch.Tensor):\n",
    "        return naive_softmax_loss(self.center_vectors, self.outside_vectors, center_word_index, outside_word_indices)\n",
    "\n",
    "\n",
    "class NegSamplingWord2Vec(Word2Vec):\n",
    "    def __init__(self, n_tokens: int, word_dimension: int, negative_sampler, K: int=10):\n",
    "        super().__init__(n_tokens, word_dimension)\n",
    "\n",
    "        self._negative_sampler = negative_sampler\n",
    "        self._K = K\n",
    "\n",
    "    def forward(self, center_word_index: torch.Tensor, outside_word_indices: torch.Tensor):\n",
    "        return neg_sampling_loss(self.center_vectors, self.outside_vectors, center_word_index, outside_word_indices, self._negative_sampler, self._K)\n",
    "\n",
    "#############################################\n",
    "# Testing functions below.                  #\n",
    "#############################################\n",
    "\n",
    "\n",
    "def test_naive_softmax_loss():\n",
    "    print (\"======Naive Softmax Loss Test Case======\")\n",
    "    center_word_index = torch.randint(1, 100, [10])\n",
    "    outside_word_indices = []\n",
    "    for _ in range(10):\n",
    "        random_window_size = random.randint(3, 6)\n",
    "        outside_word_indices.append([random.randint(1, 99) for _ in range(random_window_size)] + [0] * (6 - random_window_size))\n",
    "    outside_word_indices = torch.Tensor(outside_word_indices).to(torch.long)\n",
    "\n",
    "    model = NaiveWord2Vec(n_tokens=100, word_dimension=3)\n",
    "\n",
    "    loss = model(center_word_index, outside_word_indices).mean()\n",
    "    loss.backward()\n",
    "\n",
    "    # first test\n",
    "    assert (model.center_vectors.grad[0, :] == 0).all() and (model.outside_vectors.grad[0, :] == 0).all(), \\\n",
    "        \"<PAD> token should not affect the result.\"\n",
    "    print(\"The first test passed! Howerver, this test doesn't guarantee you that <PAD> tokens really don't affects result.\")    \n",
    "\n",
    "    # Second test\n",
    "    temp = model.center_vectors.grad.clone().detach()\n",
    "    temp[center_word_index] = 0.\n",
    "    assert (temp == 0.).all() and (model.center_vectors.grad[center_word_index] != 0.).all(), \\\n",
    "        \"Only batched center words can affect the center_word embedding.\"\n",
    "    print(\"The second test passed!\")\n",
    "\n",
    "    # third test\n",
    "    assert loss.detach().allclose(torch.tensor(26.86926651)), \\\n",
    "        \"Loss of naive softmax do not match expected result.\"\n",
    "    print(\"The third test passed!\")\n",
    "\n",
    "    # forth test\n",
    "    expected_grad = torch.Tensor([[-0.07390384, -0.14989397,  0.03736909],\n",
    "                                  [-0.00191219,  0.00386495, -0.00311787],\n",
    "                                  [-0.00470913,  0.00072215,  0.00303244]])\n",
    "    assert model.outside_vectors.grad[1:4, :].allclose(expected_grad), \\\n",
    "        \"Gradients of naive softmax do not match expected result.\"\n",
    "    print(\"The forth test passed!\")\n",
    "\n",
    "    print(\"All 4 tests passed!\")\n",
    "\n",
    "\n",
    "def test_neg_sampling_loss():\n",
    "    print (\"======Negative Sampling Loss Test Case======\")\n",
    "    center_word_index = torch.randint(1, 100, [5])\n",
    "    outside_word_indices = []\n",
    "    for _ in range(5):\n",
    "        random_window_size = random.randint(3, 6)\n",
    "        outside_word_indices.append([random.randint(1, 99) for _ in range(random_window_size)] + [0] * (6 - random_window_size))\n",
    "    outside_word_indices = torch.Tensor(outside_word_indices).to(torch.long)\n",
    "\n",
    "    neg_sampling_prob = torch.ones([100])\n",
    "    neg_sampling_prob[0] = 0.\n",
    "\n",
    "    dummy_database = type('dummy', (), {'_neg_sample_prob': neg_sampling_prob})\n",
    "\n",
    "    sampled_negatives = list()\n",
    "\n",
    "    def negative_sampler_wrapper(outside_word_indices, K):\n",
    "        result = SkipgramDataset.negative_sampler(dummy_database, outside_word_indices, K)\n",
    "        sampled_negatives.clear()\n",
    "        sampled_negatives.append(result)\n",
    "        return result\n",
    "\n",
    "    model = NegSamplingWord2Vec(n_tokens=100, word_dimension=3, negative_sampler=negative_sampler_wrapper, K=5)\n",
    "\n",
    "    loss = model(center_word_index, outside_word_indices).mean()\n",
    "    loss.backward()\n",
    "\n",
    "    # first test\n",
    "    assert (model.center_vectors.grad[0, :] == 0).all() and (model.outside_vectors.grad[0, :] == 0).all(), \\\n",
    "        \"<PAD> token should not affect the result.\"\n",
    "    print(\"The first test passed! Howerver, this test dosen't guarantee you that <PAD> tokens really don't affects result.\")    \n",
    "\n",
    "    # Second test\n",
    "    temp = model.center_vectors.grad.clone().detach()\n",
    "    temp[center_word_index] = 0.\n",
    "    assert (temp == 0.).all() and (model.center_vectors.grad[center_word_index] != 0.).all(), \\\n",
    "        \"Only batched center words can affect the centerword embedding.\"\n",
    "    print(\"The second test passed!\")\n",
    "\n",
    "    # Third test\n",
    "    sampled_negatives = sampled_negatives[0]\n",
    "    sampled_negatives[outside_word_indices.unsqueeze(-1).expand(-1, -1, 5) == 0] = 0\n",
    "    affected_indices = list((set(sampled_negatives.flatten().tolist()) | set(outside_word_indices.flatten().tolist())) - {0})\n",
    "    temp = model.outside_vectors.grad.clone().detach()\n",
    "    temp[affected_indices] = 0.\n",
    "    assert (temp == 0.).all() and (model.outside_vectors.grad[affected_indices] != 0.).all(), \\\n",
    "        \"Only batched outside words and sampled negatives can affect the outside word embedding.\"\n",
    "    print(\"The third test passed!\")\n",
    "\n",
    "    # forth test\n",
    "    assert loss.detach().allclose(torch.tensor(35.82903290)), \\\n",
    "        \"Loss of negative sampling do not match expected result.\"\n",
    "    print(\"The forth test passed!\")\n",
    "\n",
    "    # fifth test\n",
    "    expected_grad = torch.Tensor([[ 0.08583137, -0.40312022, -0.05952500],\n",
    "                                  [ 0.14896543, -0.53478962, -0.18037169],\n",
    "                                  [ 0.03650964,  0.24137473, -0.21831468]])\n",
    "    assert model.outside_vectors.grad[affected_indices[:3], :].allclose(expected_grad), \\\n",
    "        \"Gradient of negative sampling do not match expected result.\"\n",
    "    print(\"The fifth test passed!\")\n",
    "\n",
    "    print(\"All 5 tests passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_softmax_loss(\n",
    "    center_vectors: torch.Tensor, outside_vectors: torch.Tensor, \n",
    "    center_word_index: torch.Tensor, outside_word_indices: torch.Tensor\n",
    ") -> torch.Tensor:\n",
    "    \"\"\" Naive softmax loss function for word2vec models\n",
    "\n",
    "    Implement the naive softmax losses between a center word's embedding and an outside word's embedding.\n",
    "    When using GPU, it is efficient to perform a large calculation at once, so batching is used generally.\n",
    "    In addition, using a large batch size reduces the variance of samples in SGD, making training process more effective and accurate.\n",
    "    To practice this, let's calculate batch-sized losses of skipgram at once.\n",
    "    <PAD> tokens are appended for batching if the number of outside words is less than 2 * window_size. \n",
    "    However, these arbitrarily inserted <PAD> tokens have no meaning so should NOT be included in the loss calculation.\n",
    "\n",
    "    !!!IMPORTANT: Do NOT forget eliminating the effect of <PAD> tokens!!!\n",
    "\n",
    "    Arguments:\n",
    "    center_vectors -- center vectors is\n",
    "                        in shape (num words in vocab, word vector length)\n",
    "                        for all words in vocab\n",
    "    outside_vectors -- outside vector is\n",
    "                        in shape (num words in vocab, word vector length)\n",
    "                        for all words in vocab\n",
    "    center_word_index -- the index of the center word\n",
    "                        in shape (batch size,)\n",
    "    outside_word_indices -- the indices of the outside words\n",
    "                        in shape (batch size, window size * 2)\n",
    "\n",
    "    Return:\n",
    "    losses -- naive softmax loss for each (center_word_index, outsied_word_indices) pair in a batch\n",
    "                        in shape (batch size,)\n",
    "    \"\"\"\n",
    "    assert center_word_index.shape[0] == outside_word_indices.shape[0]\n",
    "\n",
    "    n_tokens, word_dim = center_vectors.shape\n",
    "    batch_size, outside_word_size = outside_word_indices.shape\n",
    "    PAD = SkipgramDataset.PAD_TOKEN_IDX\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    batched_center_vectors = center_vectors[center_word_index]\n",
    "    preds = torch.matmul(batched_center_vectors, torch.t(outside_vectors))\n",
    "    preds[:, 0] = float('-inf')\n",
    "\n",
    "    log_preds = -F.log_softmax(preds, dim=1)\n",
    "    batched_indices = torch.repeat_interleave(torch.arange(batch_size), outside_word_size)\n",
    "    pad_mask = (outside_word_indices != 0)\n",
    "\n",
    "    losses = log_preds[batched_indices, outside_word_indices.flatten()].view(batch_size, -1) * pad_mask\n",
    "    losses[losses != losses] = 0\n",
    "    losses = torch.sum(losses, 1)\n",
    "\n",
    "    ### END YOUR CODE\n",
    "    assert losses.shape == torch.Size([batch_size])\n",
    "    return losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg_sampling_loss(\n",
    "    center_vectors: torch.Tensor, outside_vectors: torch.Tensor,\n",
    "    center_word_index: torch.Tensor, outside_word_indices: torch.Tensor,\n",
    "    negative_sampler, K: int=10\n",
    ") -> torch.Tensor:\n",
    "    \"\"\" Negative sampling loss function for word2vec models\n",
    "\n",
    "    Implement the negative sampling loss for each pair of (center_word_index, outside_word_indices) in a batch.\n",
    "    As same with naive_softmax_loss, all inputs are batched with batch_size.\n",
    "\n",
    "    Note: Implementing negative sampler is a quite tricky job so we pre-implemented this part. See below comments to check how to use it.\n",
    "    If you want to know how the sampler works, check SkipgramDataset.negative_sampler code in dataset.py file\n",
    "\n",
    "    Arguments/Return Specifications: same as naiveSoftmaxLoss\n",
    "\n",
    "    Additional arguments:\n",
    "    negative_sampler -- the negative sampler\n",
    "    K -- the number of negative samples to take\n",
    "    \"\"\"\n",
    "    assert center_word_index.shape[0] == outside_word_indices.shape[0]\n",
    "\n",
    "    n_tokens, word_dim = center_vectors.shape\n",
    "    batch_size, outside_word_size = outside_word_indices.shape\n",
    "    PAD = SkipgramDataset.PAD_TOKEN_IDX\n",
    "\n",
    "    ##### Sampling negtive indices #####\n",
    "    # Because each outside word needs K negatives samples,\n",
    "    # negative_sampler takes a tensor in shape [batch_size, outside_word_size] and gives a tensor in shape [batch_size, outside_word_size, K]\n",
    "    # where values in last dimension are the indices of sampled negatives for each outside_word.\n",
    "    negative_samples: torch.Tensor = negative_sampler(outside_word_indices, K)\n",
    "    assert negative_samples.shape == torch.Size([batch_size, outside_word_size, K])\n",
    "\n",
    "    ###  YOUR CODE HERE\n",
    "    batch_center_vectors = center_vectors[center_word_index]\n",
    "    batch_dot_product = torch.einsum('bj,kj->bk', [batch_center_vectors, outside_vectors])\n",
    "\n",
    "    batch_true_loss = torch.log(torch.sigmoid(torch.gather(batch_dot_product, 1, outside_word_indices)))\n",
    "\n",
    "    batch_neg_dots = batch_dot_product.gather(1, negative_samples.reshape(batch_size, outside_word_size * K))\n",
    "    batch_neg_dots = batch_neg_dots.view(batch_size, outside_word_size, K)\n",
    "    batch_neg_loss = torch.sum(torch.log(torch.sigmoid(-batch_neg_dots)), dim=-1)\n",
    "\n",
    "    loss_matrix = -(batch_true_loss + batch_neg_loss)\n",
    "    losses = torch.sum(loss_matrix * (outside_word_indices != 0).int().float(), dim=-1)\n",
    "    ### END YOUR CODE\n",
    "    assert losses.shape == torch.Size([batch_size])\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(precision=8)\n",
    "torch.manual_seed(4321)\n",
    "random.seed(4321)\n",
    "\n",
    "test_naive_softmax_loss()\n",
    "test_neg_sampling_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
