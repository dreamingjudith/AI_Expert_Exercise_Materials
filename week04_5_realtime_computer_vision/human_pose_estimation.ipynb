{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping tensorflow as it is not installed.\u001b[0m\n",
      "Collecting tensorflow-gpu==1.12\n",
      "  Downloading tensorflow_gpu-1.12.0-cp36-cp36m-manylinux1_x86_64.whl (281.7 MB)\n",
      "\u001b[K     |████                            | 35.1 MB 32 kB/s eta 2:06:370"
     ]
    }
   ],
   "source": [
    "!pip uninstall tensorflow -y\n",
    "!pip install tensorflow-gpu==1.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-6159b4e68a84>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# set tf backend to allow memory to grow, instead of claiming everything\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslim\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mslim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "import time\n",
    "\n",
    "# set tf backend to allow memory to grow, instead of claiming everything\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "\n",
    "# custom functions\n",
    "from dataset import Dataset\n",
    "from tools import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arguments for training model\n",
    "parser = argparse.ArgumentParser(description='Hyperparameter for training a Mobilenet V2')\n",
    "parser.add_argument('--input_width',      help='Rescale the image in x-axis.', type=int, default=192)\n",
    "parser.add_argument('--input_height',     help='Rescale the image in y-axis.', type=int, default=192)\n",
    "parser.add_argument('--batchsize',        help='Size of the batches.',         type=int, default=16)\n",
    "parser.add_argument('--gpu',              help='Id of the GPU to use (as reported by nvidia-smi).', type=str, default=\"0\")\n",
    "parser.add_argument('--epoch',            help='Number of epochs to train.',   type=int, default=30)\n",
    "parser.add_argument('--save_feq',         help='Frequency of saving checkpoints in epochs.', type=int, default=10)\n",
    "parser.add_argument('--checkpoint_path',  help='Path to sotre snapshots of models during training.', type=str, default='./checkpoints')\n",
    "parser.add_argument('--lr',               help='Learning rate.',                   type=float, default=1e-3)\n",
    "parser.add_argument('--lr_decay_rate',    help='Decay rate of the learning rate.', type=float, default=0.95)\n",
    "parser.add_argument('--random_transform', help='Able/Disable data augmentation.',  type=bool, default=False)\n",
    "args = parser.parse_known_args()[0]\n",
    "\n",
    "# use this environment flag to change which GPU to use\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the Image Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_and_reshape(img, coord, target_shape=(args.input_width, args.input_height)):\n",
    "    # compute how much padding need.\n",
    "    h,w,_ = img.shape\n",
    "    max_dim = max(h, w)\n",
    "    delta_w = max_dim - w\n",
    "    delta_h = max_dim - h\n",
    "    top, bottom = delta_h // 2, delta_h - (delta_h // 2)\n",
    "    left, right = delta_w // 2, delta_w - (delta_w // 2)\n",
    "\n",
    "    # pad the image to match ratio of target shape\n",
    "    resized_img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=[0,0,0])\n",
    "    draw = resized_img.copy()\n",
    "    scale = max_dim / target_shape[0]\n",
    "\n",
    "    # resize the image with target shape\n",
    "    resized_img = cv2.resize(resized_img, target_shape)\n",
    "    \n",
    "    # pad the coordinate to match ratio of target shape\n",
    "    for i in range(coord.shape[0]):\n",
    "        coord[i, 0] += left \n",
    "        coord[i, 1] += top\n",
    "    \n",
    "    # resize the coordinate with the computed scale\n",
    "    resized_coord = coord.copy()\n",
    "    for i in range(resized_coord.shape[0]):\n",
    "        resized_coord[i, 0] = int(resized_coord[i, 0] / scale)\n",
    "        resized_coord[i, 1] = int(resized_coord[i, 1] / scale)\n",
    "    \n",
    "    return resized_img, resized_coord\n",
    "\n",
    "\n",
    "def render_gaussian_heatmap(coord):\n",
    "    sigmas = 3\n",
    "    \n",
    "    input_shape  = [args.input_height, args.input_width]\n",
    "    output_shape = [args.input_height//2, args.input_width//2]\n",
    "    num_kps = coord.shape[1]\n",
    "    \n",
    "    x = [i for i in range(output_shape[1])]\n",
    "    y = [i for i in range(output_shape[0])]\n",
    "    xx, yy = tf.meshgrid(x, y)\n",
    "    xx = tf.reshape(tf.to_float(xx), (1, output_shape[0], output_shape[1], 1))\n",
    "    yy = tf.reshape(tf.to_float(yy), (1, output_shape[0], output_shape[1], 1))\n",
    "    \n",
    "    x = tf.floor(tf.reshape(coord[:, :, 0], [-1, 1, 1, num_kps]) / input_shape[1] * output_shape[1] + 0.5)\n",
    "    y = tf.floor(tf.reshape(coord[:, :, 1], [-1, 1, 1, num_kps]) / input_shape[0] * output_shape[0] + 0.5)\n",
    "    \n",
    "    heatmap = tf.exp(-(((xx-x)/tf.to_float(sigmas))**2)/tf.to_float(2)\n",
    "                     -(((yy-y)/tf.to_float(sigmas))**2)/tf.to_float(2))\n",
    "    \n",
    "    return heatmap\n",
    "    \n",
    "    \n",
    "def generate_batch(train_data, train_indices, step, batchsize=args.batchsize):\n",
    "    image_lst = []\n",
    "    coord_lst = []\n",
    "    flags_lst = []\n",
    "    for j in train_indices[step * batchsize:(step+1) * batchsize]:\n",
    "        im2read = cv2.imread(train_data[j]['image_path'])\n",
    "        joints = np.array(train_data[j]['joints']).reshape(14, 3)\n",
    "        keypoints = joints[:, :2].astype(np.float32)\n",
    "        flags = joints[:, 2].astype(np.float32)\n",
    "        reshaped_image, reshaped_keypoint = pad_and_reshape(im2read, keypoints)            \n",
    "                \n",
    "        image_lst.append(reshaped_image)\n",
    "        coord_lst.append(reshaped_keypoint)\n",
    "        flags_lst.append(flags)\n",
    "                \n",
    "    np_image = np.array(image_lst, dtype=np.float32)\n",
    "    np_coord = np.array(coord_lst, dtype=np.float32)\n",
    "    np_flags = np.array(flags_lst, dtype=np.float32)\n",
    "            \n",
    "    feed_dict = dict()\n",
    "    feed_dict[input_image] = np_image\n",
    "    feed_dict[input_coord] = np_coord\n",
    "    feed_dict[input_flags] = np_flags\n",
    "    \n",
    "    return feed_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = Dataset()\n",
    "train_data = d.load_frame_data(task='train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Function of Mobilenet V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "_init_xavier = tf.contrib.layers.xavier_initializer()\n",
    "_init_norm = tf.truncated_normal_initializer(stddev=0.01)\n",
    "_init_zero = slim.init_ops.zeros_initializer()\n",
    "_l2_regularizer_00004 = tf.contrib.layers.l2_regularizer(0.00004)\n",
    "_trainable = True\n",
    "\n",
    "\n",
    "def is_trainable(trainable=True):\n",
    "    global _trainable\n",
    "    _trainable = trainable\n",
    "    \n",
    "\n",
    "def max_pool(inputs, k_h, k_w, s_h, s_w, name, padding=\"SAME\"):\n",
    "    return tf.nn.max_pool(inputs,\n",
    "                          ksize=[1, k_h, k_w, 1],\n",
    "                          strides=[1, s_h, s_w, 1],\n",
    "                          padding=padding,\n",
    "                          name=name)\n",
    "\n",
    "\n",
    "def upsample(inputs, factor, name):\n",
    "    return tf.image.resize_bilinear(inputs, [int(inputs.get_shape()[1]) * factor, int(inputs.get_shape()[2]) * factor],\n",
    "                                    name=name)\n",
    "\n",
    "\n",
    "def convb(input, k_h, k_w, c_o, stride, name, relu=True):\n",
    "    with slim.arg_scope([slim.batch_norm], decay=0.999, fused=True, is_training=_trainable):\n",
    "        output = slim.convolution2d(\n",
    "            inputs=input,\n",
    "            num_outputs=c_o,\n",
    "            kernel_size=[k_h, k_w],\n",
    "            stride=stride,\n",
    "            normalizer_fn=slim.batch_norm,\n",
    "            weights_regularizer=_l2_regularizer_00004,\n",
    "            weights_initializer=_init_xavier,\n",
    "            biases_initializer=_init_zero,\n",
    "            activation_fn=tf.nn.relu if relu else None,\n",
    "            scope=name,\n",
    "            trainable=_trainable)\n",
    "    return output\n",
    "\n",
    "\n",
    "def separable_conv(input, c_o, k_s, stride, scope):\n",
    "    with slim.arg_scope([slim.batch_norm],\n",
    "                        decay=0.999,\n",
    "                        fused=True,\n",
    "                        is_training=_trainable,\n",
    "                        activation_fn=tf.nn.relu6):\n",
    "        output = slim.separable_convolution2d(input,\n",
    "                                              num_outputs=None,\n",
    "                                              stride=stride,\n",
    "                                              trainable=_trainable,\n",
    "                                              depth_multiplier=1.0,\n",
    "                                              kernel_size=[k_s, k_s],\n",
    "                                              weights_initializer=_init_xavier,\n",
    "                                              weights_regularizer=_l2_regularizer_00004,\n",
    "                                              biases_initializer=None,\n",
    "                                              scope=scope + '_depthwise')\n",
    "\n",
    "        output = slim.convolution2d(output,\n",
    "                                    c_o,\n",
    "                                    stride=1,\n",
    "                                    kernel_size=[1, 1],\n",
    "                                    weights_initializer=_init_xavier,\n",
    "                                    biases_initializer=_init_zero,\n",
    "                                    normalizer_fn=slim.batch_norm,\n",
    "                                    trainable=_trainable,\n",
    "                                    weights_regularizer=None,\n",
    "                                    scope=scope + '_pointwise')\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def inverted_bottleneck(inputs, up_channel_rate, channels, subsample, k_s=3, scope=\"\"):\n",
    "    with tf.variable_scope(\"inverted_bottleneck_%s\" % scope):\n",
    "        with slim.arg_scope([slim.batch_norm],\n",
    "                            decay=0.999,\n",
    "                            fused=True,\n",
    "                            is_training=_trainable,\n",
    "                            activation_fn=tf.nn.relu6):\n",
    "            stride = 2 if subsample else 1\n",
    "\n",
    "            output = slim.convolution2d(inputs,\n",
    "                                        up_channel_rate * inputs.get_shape().as_list()[-1],\n",
    "                                        stride=1,\n",
    "                                        kernel_size=[1, 1],\n",
    "                                        weights_initializer=_init_xavier,\n",
    "                                        biases_initializer=_init_zero,\n",
    "                                        normalizer_fn=slim.batch_norm,\n",
    "                                        weights_regularizer=None,\n",
    "                                        scope=scope + '_up_pointwise',\n",
    "                                        trainable=_trainable)\n",
    "\n",
    "            output = slim.separable_convolution2d(output,\n",
    "                                                  num_outputs=None,\n",
    "                                                  stride=stride,\n",
    "                                                  depth_multiplier=1.0,\n",
    "                                                  kernel_size=k_s,\n",
    "                                                  weights_initializer=_init_xavier,\n",
    "                                                  weights_regularizer=_l2_regularizer_00004,\n",
    "                                                  biases_initializer=None,\n",
    "                                                  padding=\"SAME\",\n",
    "                                                  scope=scope + '_depthwise',\n",
    "                                                  trainable=_trainable)\n",
    "\n",
    "            output = slim.convolution2d(output,\n",
    "                                        channels,\n",
    "                                        stride=1,\n",
    "                                        kernel_size=[1, 1],\n",
    "                                        activation_fn=None,\n",
    "                                        weights_initializer=_init_xavier,\n",
    "                                        biases_initializer=_init_zero,\n",
    "                                        normalizer_fn=slim.batch_norm,\n",
    "                                        weights_regularizer=None,\n",
    "                                        scope=scope + '_pointwise',\n",
    "                                        trainable=_trainable)\n",
    "            if inputs.get_shape().as_list()[-1] == channels:\n",
    "                output = tf.add(inputs, output)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MobileNet V2 Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_KPOINTS = 14\n",
    "STAGE_NUM = 6\n",
    "out_channel_ratio = lambda d: max(int(d * 0.75), 8)\n",
    "up_channel_ratio = lambda d: int(d * 1.)\n",
    "out_channel_cpm = lambda d: max(int(d * 0.75), 8)\n",
    "\n",
    "\n",
    "def cpm_mobilenet_v2(input, trainable):\n",
    "    is_trainable(trainable)\n",
    "    \n",
    "    net = convb(input, 3, 3, out_channel_ratio(32), 2, name=\"Conv2d_0\")\n",
    "    with tf.variable_scope('MobilenetV2'):\n",
    "\n",
    "        # 128, 112\n",
    "        mv2_branch_0 = slim.stack(net, inverted_bottleneck,\n",
    "                                  [\n",
    "                                      (1, out_channel_ratio(16), 0, 3),\n",
    "                                      (1, out_channel_ratio(16), 0, 3)\n",
    "                                  ], scope=\"MobilenetV2_part_0\")\n",
    "\n",
    "        # 64, 56\n",
    "        mv2_branch_1 = slim.stack(mv2_branch_0, inverted_bottleneck,\n",
    "                                  [\n",
    "                                      (up_channel_ratio(6), out_channel_ratio(24), 1, 3),\n",
    "                                      (up_channel_ratio(6), out_channel_ratio(24), 0, 3),\n",
    "                                      (up_channel_ratio(6), out_channel_ratio(24), 0, 3),\n",
    "                                      (up_channel_ratio(6), out_channel_ratio(24), 0, 3),\n",
    "                                      (up_channel_ratio(6), out_channel_ratio(24), 0, 3),\n",
    "                                  ], scope=\"MobilenetV2_part_1\")\n",
    "\n",
    "        # 32, 28\n",
    "        mv2_branch_2 = slim.stack(mv2_branch_1, inverted_bottleneck,\n",
    "                                  [\n",
    "                                      (up_channel_ratio(6), out_channel_ratio(32), 1, 3),\n",
    "                                      (up_channel_ratio(6), out_channel_ratio(32), 0, 3),\n",
    "                                      (up_channel_ratio(6), out_channel_ratio(32), 0, 3),\n",
    "                                      (up_channel_ratio(6), out_channel_ratio(32), 0, 3),\n",
    "                                      (up_channel_ratio(6), out_channel_ratio(32), 0, 3),\n",
    "                                  ], scope=\"MobilenetV2_part_2\")\n",
    "\n",
    "        # 16, 14\n",
    "        mv2_branch_3 = slim.stack(mv2_branch_2, inverted_bottleneck,\n",
    "                                  [\n",
    "                                      (up_channel_ratio(6), out_channel_ratio(64), 1, 3),\n",
    "                                      (up_channel_ratio(6), out_channel_ratio(64), 0, 3),\n",
    "                                      (up_channel_ratio(6), out_channel_ratio(64), 0, 3),\n",
    "                                      (up_channel_ratio(6), out_channel_ratio(64), 0, 3),\n",
    "                                      (up_channel_ratio(6), out_channel_ratio(64), 0, 3),\n",
    "                                  ], scope=\"MobilenetV2_part_3\")\n",
    "\n",
    "        # 8, 7\n",
    "        mv2_branch_4 = slim.stack(mv2_branch_3, inverted_bottleneck,\n",
    "                                  [\n",
    "                                      (up_channel_ratio(6), out_channel_ratio(96), 1, 3),\n",
    "                                      (up_channel_ratio(6), out_channel_ratio(96), 0, 3),\n",
    "                                      (up_channel_ratio(6), out_channel_ratio(96), 0, 3),\n",
    "                                      (up_channel_ratio(6), out_channel_ratio(96), 0, 3),\n",
    "                                      (up_channel_ratio(6), out_channel_ratio(96), 0, 3)\n",
    "                                  ], scope=\"MobilenetV2_part_4\")\n",
    "\n",
    "        cancat_mv2 = tf.concat(\n",
    "            [\n",
    "                max_pool(mv2_branch_0, 4, 4, 4, 4, name=\"mv2_0_max_pool\"),\n",
    "                max_pool(mv2_branch_1, 2, 2, 2, 2, name=\"mv2_1_max_pool\"),\n",
    "                mv2_branch_2,\n",
    "                upsample(mv2_branch_3, 2, name=\"mv2_3_upsample\"),\n",
    "                upsample(mv2_branch_4, 4, name=\"mv2_4_upsample\")\n",
    "            ]\n",
    "            , axis=3)\n",
    "\n",
    "    with tf.variable_scope(\"Convolutional_Pose_Machine\"):\n",
    "        l2s = []\n",
    "        prev = None\n",
    "        for stage_number in range(STAGE_NUM):\n",
    "            if prev is not None:\n",
    "                inputs = tf.concat([cancat_mv2, prev], axis=3)\n",
    "            else:\n",
    "                inputs = cancat_mv2\n",
    "\n",
    "            kernel_size = 7\n",
    "            lastest_channel_size = 128\n",
    "            if stage_number == 0:\n",
    "                kernel_size = 3\n",
    "                lastest_channel_size = 512\n",
    "\n",
    "            _ = slim.stack(inputs, inverted_bottleneck,\n",
    "                           [\n",
    "                               (2, out_channel_cpm(32), 0, kernel_size),\n",
    "                               (up_channel_ratio(4), out_channel_cpm(32), 0, kernel_size),\n",
    "                               (up_channel_ratio(4), out_channel_cpm(32), 0, kernel_size),\n",
    "                           ], scope=\"stage_%d_mv2\" % stage_number)\n",
    "\n",
    "            _ = slim.stack(_, separable_conv,\n",
    "                           [\n",
    "                               (out_channel_ratio(lastest_channel_size), 1, 1),\n",
    "                               (N_KPOINTS, 1, 1)\n",
    "                           ], scope=\"stage_%d_mv1\" % stage_number)\n",
    "\n",
    "            prev = _\n",
    "            cpm_out = upsample(_, 4, \"stage_%d_out\" % stage_number)\n",
    "            l2s.append(cpm_out)\n",
    "\n",
    "    return cpm_out, l2s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image = tf.placeholder(tf.float32, [None, args.input_height, args.input_width, 3])    # [batchsize, H, W, C]\n",
    "input_coord = tf.placeholder(tf.float32, [None, 14, 2])         # [batchsize, num_kps, 2]\n",
    "input_flags = tf.placeholder(tf.float32, [None, 14])            # [batchsize, num_kps]\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "learning_rate = tf.train.exponential_decay(float(args.lr), global_step,\n",
    "        decay_steps=10000, decay_rate=float(args.lr_decay_rate), staircase=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_heatmaps = render_gaussian_heatmap(input_coord)\n",
    "with tf.variable_scope(tf.get_variable_scope(), reuse=False):\n",
    "    _, pred_heatmaps_all = cpm_mobilenet_v2(input_image, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "for idx, pred_heat in enumerate(pred_heatmaps_all):\n",
    "    reshaped_flags = tf.reshape(input_flags, [-1, 1, 1, 14])\n",
    "    loss_l2 = tf.nn.l2_loss((tf.concat(pred_heat, axis=0) - input_heatmaps) * reshaped_flags, name='loss_heatmap_stage%d' % idx)\n",
    "    losses.append(loss_l2)\n",
    "\n",
    "total_loss = tf.reduce_sum(losses) / args.batchsize\n",
    "total_loss_ll_heat = tf.reduce_sum(loss_l2) / args.batchsize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = tf.train.AdamOptimizer(learning_rate, epsilon = 1e-8)\n",
    "grads = optim.compute_gradients(total_loss)\n",
    "apply_gradients_op = optim.apply_gradients(grads, global_step=global_step)\n",
    "\n",
    "MOVING_AVERAGE_DECAY = 0.99\n",
    "variable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)\n",
    "variable_to_average = (tf.trainable_variables() + tf.moving_average_variables())\n",
    "variables_averages_op = variable_averages.apply(variable_to_average)\n",
    "\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(update_ops):\n",
    "    train_op = tf.group(apply_gradients_op, variables_averages_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create session before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = get_session()\n",
    "\n",
    "# initialize variables\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# prepare to train\n",
    "train_indices = np.arange(len(train_data))\n",
    "num_steps = len(train_indices) // args.batchsize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss = 10044.46, last_heat_loss = 2610.38, duration = 0.1174\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(args.epoch):\n",
    "    # shuffle all elements per epoch\n",
    "    shuffled_indices = train_indices.copy()\n",
    "    get_rng().shuffle(shuffled_indices)\n",
    "        \n",
    "    for i in range(num_steps):\n",
    "        feed_dict = generate_batch(train_data, shuffled_indices, i)\n",
    "        start = time.time()\n",
    "        _, loss, loss_lastlayer_heat = sess.run([train_op, total_loss, total_loss_ll_heat], \n",
    "                                            feed_dict = feed_dict)\n",
    "        duration = time.time() - start\n",
    "    \n",
    "    # print train info per epoch\n",
    "    print('epoch: %d, loss = %.2f, last_heat_loss = %.2f, duration = %.4f' % (epoch, loss, loss_lastlayer_heat, duration))      \n",
    "\n",
    "    # visualize ground truth heatmap and output heatmap\n",
    "    background = feed_dict[input_image][0]\n",
    "    flags_info = feed_dict[input_flags][0]\n",
    "    heatmaps, outputs = sess.run([input_heatmaps, pred_heat], feed_dict = feed_dict)  \n",
    "    visualize_gt_and_output(background, heatmaps[0], outputs[0], flags_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restore pretrained model\n",
    "saver = tf.train.Saver()\n",
    "pretrain_dir = './pretrained_ckpts/model-450000'\n",
    "saver.restore(sess, pretrain_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Test with Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set trainable=False and get output heatmap\n",
    "with tf.variable_scope(tf.get_variable_scope(), reuse=True):\n",
    "    test_heatmaps, test_heatmaps_all = cpm_mobilenet_v2(input_image, False)\n",
    "\n",
    "num_steps = len(train_indices) // 1\n",
    "\n",
    "for epoch in range(1):\n",
    "    for i in range(num_steps):\n",
    "        feed_dict = generate_batch(train_data, train_indices, i, batchsize=1)    \n",
    "        background = feed_dict[input_image][0]\n",
    "        flags_info = feed_dict[input_flags][0]\n",
    "        \n",
    "        start = time.time()\n",
    "        heatmaps, outputs = sess.run([input_heatmaps, test_heatmaps], feed_dict = feed_dict)\n",
    "        duration = time.time() - start\n",
    "        print('processing time = %.4f' % duration)\n",
    "        \n",
    "        visualize_gt_and_output(background, heatmaps[0], outputs[0], flags_info, delay=10, draw_keypoint=True)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
